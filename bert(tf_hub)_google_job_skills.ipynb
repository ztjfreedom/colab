{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bert(tf hub)_google_job_skills.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ztjfreedom/colab/blob/master/bert(tf_hub)_google_job_skills.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCpvgG0vwXAZ",
        "colab_type": "text"
      },
      "source": [
        "#Classification on Google Job Skills dataset with BERT on TF Hub"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsZvic2YxnTz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from datetime import datetime"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cp5wfXDx5SPH",
        "colab_type": "text"
      },
      "source": [
        "In addition to the standard libraries we imported above, we'll need to install BERT's python package."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jviywGyWyKsA",
        "colab_type": "code",
        "outputId": "84c8d2d5-d42d-49b5-ff04-ada1f8a3cbbc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "!pip install bert-tensorflow"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting bert-tensorflow\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/66/7eb4e8b6ea35b7cc54c322c816f976167a43019750279a8473d355800a93/bert_tensorflow-1.0.1-py2.py3-none-any.whl (67kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 3.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from bert-tensorflow) (1.12.0)\n",
            "Installing collected packages: bert-tensorflow\n",
            "Successfully installed bert-tensorflow-1.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhbGEfwgdEtw",
        "colab_type": "code",
        "outputId": "cd6b3257-83ed-48f9-f7b3-0ca31ac53878",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import bert\n",
        "from bert import run_classifier\n",
        "from bert import optimization\n",
        "from bert import tokenization"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0712 04:32:37.548106 140341499602816 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/bert/optimization.py:87: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVB3eOcjxxm1",
        "colab_type": "text"
      },
      "source": [
        "Below, we'll set an output directory location to store our model output and checkpoints. This can be a local directory, in which case you'd set OUTPUT_DIR to the name of the directory you'd like to create. If you're running this code in Google's hosted Colab, the directory won't persist after the Colab session ends.\n",
        "\n",
        "Alternatively, if you're a GCP user, you can store output in a GCP bucket. To do that, set a directory name in OUTPUT_DIR and the name of the GCP bucket in the BUCKET field.\n",
        "\n",
        "Set DO_DELETE to rewrite the OUTPUT_DIR if it exists. Otherwise, Tensorflow will load existing model checkpoints from that directory (if they exist)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "US_EAnICvP7f",
        "colab_type": "code",
        "outputId": "4ee295f8-5068-4179-f4e6-cf2c2ead30f6",
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Set the output directory for saving model file\n",
        "# Optionally, set a GCP bucket location\n",
        "\n",
        "OUTPUT_DIR = 'bert_output'#@param {type:\"string\"}\n",
        "#@markdown Whether or not to clear/delete the directory and create a new one\n",
        "DO_DELETE = True #@param {type:\"boolean\"}\n",
        "#@markdown Set USE_BUCKET and BUCKET if you want to (optionally) store model output on GCP bucket.\n",
        "USE_BUCKET = False #@param {type:\"boolean\"}\n",
        "BUCKET = '' #@param {type:\"string\"}\n",
        "\n",
        "if USE_BUCKET:\n",
        "    OUTPUT_DIR = 'gs://{}/{}'.format(BUCKET, OUTPUT_DIR)\n",
        "    from google.colab import auth\n",
        "    auth.authenticate_user()\n",
        "\n",
        "if DO_DELETE:\n",
        "    try:\n",
        "        tf.gfile.DeleteRecursively(OUTPUT_DIR)\n",
        "    except:\n",
        "        # Doesn't matter if the directory didn't exist\n",
        "        pass\n",
        "tf.gfile.MakeDirs(OUTPUT_DIR)\n",
        "print('***** Model output directory: {} *****'.format(OUTPUT_DIR))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "***** Model output directory: bert_output *****\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmFYvkylMwXn",
        "colab_type": "text"
      },
      "source": [
        "#Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1n5JM4kK1OM",
        "colab_type": "code",
        "outputId": "d636235c-6b1c-4972-ec6e-af9b1f4975bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "# mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "%cd gdrive/My Drive/Colab Notebooks/dataset/"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n",
            "/content/gdrive/My Drive/Colab Notebooks/dataset\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsdPHW51Lrew",
        "colab_type": "code",
        "outputId": "34b347a4-4de2-4288-df82-fb015c8da134",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import collections\n",
        "from IPython.display import display\n",
        "\n",
        "dataset_file = 'google_job_skills/job_skills.csv'\n",
        "df = pd.read_csv(dataset_file)\n",
        "print(len(df))\n",
        "display(df.head())"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1250\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Company</th>\n",
              "      <th>Title</th>\n",
              "      <th>Category</th>\n",
              "      <th>Location</th>\n",
              "      <th>Responsibilities</th>\n",
              "      <th>Minimum Qualifications</th>\n",
              "      <th>Preferred Qualifications</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Google</td>\n",
              "      <td>Google Cloud Program Manager</td>\n",
              "      <td>Program Management</td>\n",
              "      <td>Singapore</td>\n",
              "      <td>Shape, shepherd, ship, and show technical prog...</td>\n",
              "      <td>BA/BS degree or equivalent practical experienc...</td>\n",
              "      <td>Experience in the business technology market a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Google</td>\n",
              "      <td>Supplier Development Engineer (SDE), Cable/Con...</td>\n",
              "      <td>Manufacturing &amp; Supply Chain</td>\n",
              "      <td>Shanghai, China</td>\n",
              "      <td>Drive cross-functional activities in the suppl...</td>\n",
              "      <td>BS degree in an Engineering discipline or equi...</td>\n",
              "      <td>BSEE, BSME or BSIE degree.\\nExperience of usin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Google</td>\n",
              "      <td>Data Analyst, Product and Tools Operations, Go...</td>\n",
              "      <td>Technical Solutions</td>\n",
              "      <td>New York, NY, United States</td>\n",
              "      <td>Collect and analyze data to draw insight and i...</td>\n",
              "      <td>Bachelor’s degree in Business, Economics, Stat...</td>\n",
              "      <td>Experience partnering or consulting cross-func...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Google</td>\n",
              "      <td>Developer Advocate, Partner Engineering</td>\n",
              "      <td>Developer Relations</td>\n",
              "      <td>Mountain View, CA, United States</td>\n",
              "      <td>Work one-on-one with the top Android, iOS, and...</td>\n",
              "      <td>BA/BS degree in Computer Science or equivalent...</td>\n",
              "      <td>Experience as a software developer, architect,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Google</td>\n",
              "      <td>Program Manager, Audio Visual (AV) Deployments</td>\n",
              "      <td>Program Management</td>\n",
              "      <td>Sunnyvale, CA, United States</td>\n",
              "      <td>Plan requirements with internal customers.\\nPr...</td>\n",
              "      <td>BA/BS degree or equivalent practical experienc...</td>\n",
              "      <td>CTS Certification.\\nExperience in the construc...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Company  ...                           Preferred Qualifications\n",
              "0  Google  ...  Experience in the business technology market a...\n",
              "1  Google  ...  BSEE, BSME or BSIE degree.\\nExperience of usin...\n",
              "2  Google  ...  Experience partnering or consulting cross-func...\n",
              "3  Google  ...  Experience as a software developer, architect,...\n",
              "4  Google  ...  CTS Certification.\\nExperience in the construc...\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6RNY5EKeLrcY",
        "colab_type": "code",
        "outputId": "182eab14-a2fb-490e-92f4-c530b2cbb986",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "df = df[['Title', 'Responsibilities', 'Category']]\n",
        "mask = (df['Category'].notnull()) & (df['Title'].notnull()) & (df['Responsibilities'].notnull())\n",
        "df = df.loc[mask]\n",
        "print(len(df))\n",
        "display(df.head())"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1235\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Title</th>\n",
              "      <th>Responsibilities</th>\n",
              "      <th>Category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Google Cloud Program Manager</td>\n",
              "      <td>Shape, shepherd, ship, and show technical prog...</td>\n",
              "      <td>Program Management</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Supplier Development Engineer (SDE), Cable/Con...</td>\n",
              "      <td>Drive cross-functional activities in the suppl...</td>\n",
              "      <td>Manufacturing &amp; Supply Chain</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Data Analyst, Product and Tools Operations, Go...</td>\n",
              "      <td>Collect and analyze data to draw insight and i...</td>\n",
              "      <td>Technical Solutions</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Developer Advocate, Partner Engineering</td>\n",
              "      <td>Work one-on-one with the top Android, iOS, and...</td>\n",
              "      <td>Developer Relations</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Program Manager, Audio Visual (AV) Deployments</td>\n",
              "      <td>Plan requirements with internal customers.\\nPr...</td>\n",
              "      <td>Program Management</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               Title  ...                      Category\n",
              "0                       Google Cloud Program Manager  ...            Program Management\n",
              "1  Supplier Development Engineer (SDE), Cable/Con...  ...  Manufacturing & Supply Chain\n",
              "2  Data Analyst, Product and Tools Operations, Go...  ...           Technical Solutions\n",
              "3            Developer Advocate, Partner Engineering  ...           Developer Relations\n",
              "4     Program Manager, Audio Visual (AV) Deployments  ...            Program Management\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LdA2BhAiLrZv",
        "colab_type": "code",
        "outputId": "bb9df47a-8a32-4db3-c31f-ff86a11fe430",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        }
      },
      "source": [
        "print(df.groupby(['Category']).size().count())\n",
        "print(df.groupby(['Category']).size())"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "23\n",
            "Category\n",
            "Administrative                       40\n",
            "Business Strategy                    98\n",
            "Data Center & Network                 2\n",
            "Developer Relations                   5\n",
            "Finance                             115\n",
            "Hardware Engineering                 22\n",
            "IT & Data Management                  5\n",
            "Legal & Government Relations         46\n",
            "Manufacturing & Supply Chain         16\n",
            "Marketing & Communications          165\n",
            "Network Engineering                   6\n",
            "Partnerships                         59\n",
            "People Operations                    86\n",
            "Product & Customer Support           50\n",
            "Program Management                   72\n",
            "Real Estate & Workplace Services     25\n",
            "Sales & Account Management          168\n",
            "Sales Operations                     31\n",
            "Software Engineering                 24\n",
            "Technical Infrastructure             11\n",
            "Technical Solutions                 100\n",
            "Technical Writing                     5\n",
            "User Experience & Design             84\n",
            "dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9pwBp6aDLrW6",
        "colab_type": "code",
        "outputId": "ea541c14-4fc6-4be2-fbe2-2c31129e836e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(df['Title'].apply(len).mean())\n",
        "print(df['Responsibilities'].apply(len).mean())"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "41.83400809716599\n",
            "639.9352226720648\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePgR7VQ3NK6j",
        "colab_type": "code",
        "outputId": "7347550e-74ef-4a4b-864c-edee14796a0f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "label_list = list(set(df.get('Category').tolist()))\n",
        "print(len(label_list))\n",
        "print(label_list)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "23\n",
            "['Program Management', 'Software Engineering', 'User Experience & Design', 'Real Estate & Workplace Services', 'Sales & Account Management', 'Technical Writing', 'Developer Relations', 'IT & Data Management', 'Technical Solutions', 'People Operations', 'Marketing & Communications', 'Manufacturing & Supply Chain', 'Data Center & Network', 'Technical Infrastructure', 'Business Strategy', 'Hardware Engineering', 'Network Engineering', 'Finance', 'Legal & Government Relations', 'Partnerships', 'Administrative', 'Sales Operations', 'Product & Customer Support']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZFhcBiRNK3J",
        "colab_type": "code",
        "outputId": "c5333a07-3c4a-4537-83f4-427eb7bdc8f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Train test set split, satisfying that test set contains each category\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "train, test = train_test_split(df, test_size=0.2, random_state=0, stratify=df.get('Category'))\n",
        "print(len(train), type(train), len(test), type(test))\n",
        "print(test.groupby(['Category']).size().count())"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "988 <class 'pandas.core.frame.DataFrame'> 247 <class 'pandas.core.frame.DataFrame'>\n",
            "23\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V399W0rqNJ-Z",
        "colab_type": "text"
      },
      "source": [
        "#Data Preprocessing\n",
        "We'll need to transform our data into a format BERT understands. This involves two steps. First, we create  `InputExample`'s using the constructor provided in the BERT library.\n",
        "\n",
        "- `text_a` is the text we want to classify, which in this case, is the `Request` field in our Dataframe. \n",
        "- `text_b` is used if we're training a model to understand the relationship between sentences (i.e. is `text_b` a translation of `text_a`? Is `text_b` an answer to the question asked by `text_a`?). This doesn't apply to our task, so we can leave `text_b` blank.\n",
        "- `label` is the label for our example, i.e. True, False"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9gEt5SmM6i6",
        "colab_type": "code",
        "outputId": "b0b2f80a-d5fe-4bab-eec8-9096231ed6c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Use the InputExample class from BERT's run_classifier code to create examples from the data\n",
        "train_InputExamples_t = train.apply(lambda x: bert.run_classifier.InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this example\n",
        "                                                                     text_a = x['Title'], \n",
        "                                                                     text_b = None, \n",
        "                                                                     label = x['Category']), axis = 1)\n",
        "\n",
        "test_InputExamples_t = test.apply(lambda x: bert.run_classifier.InputExample(guid=None, \n",
        "                                                                     text_a = x['Title'], \n",
        "                                                                     text_b = None, \n",
        "                                                                     label = x['Category']), axis = 1)\n",
        "\n",
        "train_InputExamples_r = train.apply(lambda x: bert.run_classifier.InputExample(guid=None,\n",
        "                                                                     text_a = x['Responsibilities'], \n",
        "                                                                     text_b = None, \n",
        "                                                                     label = x['Category']), axis = 1)\n",
        "\n",
        "test_InputExamples_r = test.apply(lambda x: bert.run_classifier.InputExample(guid=None, \n",
        "                                                                     text_a = x['Responsibilities'], \n",
        "                                                                     text_b = None, \n",
        "                                                                     label = x['Category']), axis = 1)\n",
        "\n",
        "print(type(train_InputExamples_t))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.series.Series'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCZWZtKxObjh",
        "colab_type": "text"
      },
      "source": [
        "Next, we need to preprocess our data so that it matches the data BERT was trained on. For this, we'll need to do a couple of things (but don't worry--this is also included in the Python library):\n",
        "\n",
        "\n",
        "1. Lowercase our text (if we're using a BERT lowercase model)\n",
        "2. Tokenize it (i.e. \"sally says hi\" -> [\"sally\", \"says\", \"hi\"])\n",
        "3. Break words into WordPieces (i.e. \"calling\" -> [\"call\", \"##ing\"])\n",
        "4. Map our words to indexes using a vocab file that BERT provides\n",
        "5. Add special \"CLS\" and \"SEP\" tokens (see the [readme](https://github.com/google-research/bert))\n",
        "6. Append \"index\" and \"segment\" tokens to each input (see the [BERT paper](https://arxiv.org/pdf/1810.04805.pdf))\n",
        "\n",
        "Happily, we don't have to worry about most of these details.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMWiDtpyQSoU",
        "colab_type": "text"
      },
      "source": [
        "To start, we'll need to load a vocabulary file and lowercasing information directly from the BERT tf hub module:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IhJSe0QHNG7U",
        "colab_type": "code",
        "outputId": "c5c12698-77a7-493a-844f-a8257d2989b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# This is a path to an uncased (all lowercase) version of BERT\n",
        "BERT_MODEL_HUB = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n",
        "\n",
        "def create_tokenizer_from_hub_module():\n",
        "    \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n",
        "    with tf.Graph().as_default():\n",
        "        bert_module = hub.Module(BERT_MODEL_HUB)\n",
        "        tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
        "        with tf.Session() as sess:\n",
        "            vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\n",
        "                                                  tokenization_info[\"do_lower_case\"]])\n",
        "      \n",
        "    return bert.tokenization.FullTokenizer(vocab_file=vocab_file, do_lower_case=do_lower_case)\n",
        "\n",
        "tokenizer = create_tokenizer_from_hub_module()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0712 04:33:32.260858 140341499602816 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/bert/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4oFkhpZBDKm",
        "colab_type": "text"
      },
      "source": [
        "Great--we just learned that the BERT model we're using expects lowercase data (that's what stored in tokenization_info[\"do_lower_case\"]) and we also loaded BERT's vocab file. We also created a tokenizer, which breaks words into word pieces:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsBo6RCtQmwx",
        "colab_type": "code",
        "outputId": "a713209b-7b40-4871-910e-fe54ed8fdd3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "tokenizer.tokenize(\"This here's an example of using the BERT tokenizer\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['this',\n",
              " 'here',\n",
              " \"'\",\n",
              " 's',\n",
              " 'an',\n",
              " 'example',\n",
              " 'of',\n",
              " 'using',\n",
              " 'the',\n",
              " 'bert',\n",
              " 'token',\n",
              " '##izer']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0OEzfFIt6GIc",
        "colab_type": "text"
      },
      "source": [
        "Using our tokenizer, we'll call `run_classifier.convert_examples_to_features` on our InputExamples to convert them into features BERT understands."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LL5W8gEGRTAf",
        "colab_type": "code",
        "outputId": "45aaf01a-1426-42f5-df35-60811e04ea20",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import os\n",
        "\n",
        "# Change log level so that we can check the results after tokenization\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '0' \n",
        "tf.logging.set_verbosity(tf.logging.INFO)\n",
        "\n",
        "# We'll set sequences to be at most 128 tokens long.ß\n",
        "MAX_SEQ_LENGTH_t = 32\n",
        "MAX_SEQ_LENGTH_r = 128\n",
        "\n",
        "# Convert our train and test features to InputFeatures that BERT understands.\n",
        "train_features_t = bert.run_classifier.convert_examples_to_features(train_InputExamples_t, label_list, MAX_SEQ_LENGTH_t, tokenizer)\n",
        "test_features_t = bert.run_classifier.convert_examples_to_features(test_InputExamples_t, label_list, MAX_SEQ_LENGTH_t, tokenizer)\n",
        "\n",
        "train_features_r = bert.run_classifier.convert_examples_to_features(train_InputExamples_r, label_list, MAX_SEQ_LENGTH_r, tokenizer)\n",
        "test_features_r = bert.run_classifier.convert_examples_to_features(test_InputExamples_r, label_list, MAX_SEQ_LENGTH_r, tokenizer)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0712 04:33:35.704999 140341499602816 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/bert/run_classifier.py:774: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n",
            "I0712 04:33:35.710829 140341499602816 run_classifier.py:774] Writing example 0 of 988\n",
            "I0712 04:33:35.712619 140341499602816 run_classifier.py:461] *** Example ***\n",
            "I0712 04:33:35.713667 140341499602816 run_classifier.py:462] guid: None\n",
            "I0712 04:33:35.716379 140341499602816 run_classifier.py:464] tokens: [CLS] user experience engineer intern , summer 2018 [SEP]\n",
            "I0712 04:33:35.718050 140341499602816 run_classifier.py:465] input_ids: 101 5310 3325 3992 25204 1010 2621 2760 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0712 04:33:35.719858 140341499602816 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0712 04:33:35.722665 140341499602816 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0712 04:33:35.724123 140341499602816 run_classifier.py:468] label: User Experience & Design (id = 2)\n",
            "I0712 04:33:35.725497 140341499602816 run_classifier.py:461] *** Example ***\n",
            "I0712 04:33:35.726716 140341499602816 run_classifier.py:462] guid: None\n",
            "I0712 04:33:35.728153 140341499602816 run_classifier.py:464] tokens: [CLS] google cloud customer engineer manager , financial services [SEP]\n",
            "I0712 04:33:35.730120 140341499602816 run_classifier.py:465] input_ids: 101 8224 6112 8013 3992 3208 1010 3361 2578 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0712 04:33:35.731530 140341499602816 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0712 04:33:35.733356 140341499602816 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0712 04:33:35.735767 140341499602816 run_classifier.py:468] label: Finance (id = 17)\n",
            "I0712 04:33:35.738646 140341499602816 run_classifier.py:461] *** Example ***\n",
            "I0712 04:33:35.739830 140341499602816 run_classifier.py:462] guid: None\n",
            "I0712 04:33:35.741144 140341499602816 run_classifier.py:464] tokens: [CLS] networking product specialist , google cloud ( english ) [SEP]\n",
            "I0712 04:33:35.742841 140341499602816 run_classifier.py:465] input_ids: 101 14048 4031 8325 1010 8224 6112 1006 2394 1007 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0712 04:33:35.744762 140341499602816 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0712 04:33:35.746081 140341499602816 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0712 04:33:35.747869 140341499602816 run_classifier.py:468] label: Technical Solutions (id = 8)\n",
            "I0712 04:33:35.749177 140341499602816 run_classifier.py:461] *** Example ***\n",
            "I0712 04:33:35.750883 140341499602816 run_classifier.py:462] guid: None\n",
            "I0712 04:33:35.752238 140341499602816 run_classifier.py:464] tokens: [CLS] survey lead , google cloud support [SEP]\n",
            "I0712 04:33:35.753773 140341499602816 run_classifier.py:465] input_ids: 101 5002 2599 1010 8224 6112 2490 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0712 04:33:35.755140 140341499602816 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0712 04:33:35.756211 140341499602816 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0712 04:33:35.757655 140341499602816 run_classifier.py:468] label: Product & Customer Support (id = 22)\n",
            "I0712 04:33:35.759381 140341499602816 run_classifier.py:461] *** Example ***\n",
            "I0712 04:33:35.760790 140341499602816 run_classifier.py:462] guid: None\n",
            "I0712 04:33:35.762126 140341499602816 run_classifier.py:464] tokens: [CLS] business intern 2018 , argentina [SEP]\n",
            "I0712 04:33:35.763434 140341499602816 run_classifier.py:465] input_ids: 101 2449 25204 2760 1010 5619 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0712 04:33:35.764648 140341499602816 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0712 04:33:35.766508 140341499602816 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0712 04:33:35.767770 140341499602816 run_classifier.py:468] label: Finance (id = 17)\n",
            "I0712 04:33:35.965858 140341499602816 run_classifier.py:774] Writing example 0 of 247\n",
            "I0712 04:33:35.966996 140341499602816 run_classifier.py:461] *** Example ***\n",
            "I0712 04:33:35.969388 140341499602816 run_classifier.py:462] guid: None\n",
            "I0712 04:33:35.970953 140341499602816 run_classifier.py:464] tokens: [CLS] product analytics lead , data science [SEP]\n",
            "I0712 04:33:35.972713 140341499602816 run_classifier.py:465] input_ids: 101 4031 25095 2599 1010 2951 2671 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0712 04:33:35.973828 140341499602816 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0712 04:33:35.975334 140341499602816 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0712 04:33:35.978376 140341499602816 run_classifier.py:468] label: Business Strategy (id = 14)\n",
            "I0712 04:33:35.979379 140341499602816 run_classifier.py:461] *** Example ***\n",
            "I0712 04:33:35.981500 140341499602816 run_classifier.py:462] guid: None\n",
            "I0712 04:33:35.983198 140341499602816 run_classifier.py:464] tokens: [CLS] mba intern 2018 [SEP]\n",
            "I0712 04:33:35.984706 140341499602816 run_classifier.py:465] input_ids: 101 15038 25204 2760 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0712 04:33:35.986565 140341499602816 run_classifier.py:466] input_mask: 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0712 04:33:35.988193 140341499602816 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0712 04:33:35.989984 140341499602816 run_classifier.py:468] label: Administrative (id = 20)\n",
            "I0712 04:33:35.991462 140341499602816 run_classifier.py:461] *** Example ***\n",
            "I0712 04:33:35.992576 140341499602816 run_classifier.py:462] guid: None\n",
            "I0712 04:33:35.994055 140341499602816 run_classifier.py:464] tokens: [CLS] mechanical engineer , data center [SEP]\n",
            "I0712 04:33:35.995331 140341499602816 run_classifier.py:465] input_ids: 101 6228 3992 1010 2951 2415 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0712 04:33:35.996333 140341499602816 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0712 04:33:35.997916 140341499602816 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0712 04:33:35.999290 140341499602816 run_classifier.py:468] label: Technical Infrastructure (id = 13)\n",
            "I0712 04:33:36.000644 140341499602816 run_classifier.py:461] *** Example ***\n",
            "I0712 04:33:36.002116 140341499602816 run_classifier.py:462] guid: None\n",
            "I0712 04:33:36.003605 140341499602816 run_classifier.py:464] tokens: [CLS] technical analytics architect , google cloud online experience [SEP]\n",
            "I0712 04:33:36.004559 140341499602816 run_classifier.py:465] input_ids: 101 4087 25095 4944 1010 8224 6112 3784 3325 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0712 04:33:36.006190 140341499602816 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0712 04:33:36.007702 140341499602816 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0712 04:33:36.010580 140341499602816 run_classifier.py:468] label: Marketing & Communications (id = 10)\n",
            "I0712 04:33:36.014194 140341499602816 run_classifier.py:461] *** Example ***\n",
            "I0712 04:33:36.016484 140341499602816 run_classifier.py:462] guid: None\n",
            "I0712 04:33:36.018705 140341499602816 run_classifier.py:464] tokens: [CLS] platform sales manager , media platforms account lead [SEP]\n",
            "I0712 04:33:36.023841 140341499602816 run_classifier.py:465] input_ids: 101 4132 4341 3208 1010 2865 7248 4070 2599 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0712 04:33:36.026353 140341499602816 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0712 04:33:36.028804 140341499602816 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0712 04:33:36.031362 140341499602816 run_classifier.py:468] label: Sales & Account Management (id = 4)\n",
            "I0712 04:33:36.099469 140341499602816 run_classifier.py:774] Writing example 0 of 988\n",
            "I0712 04:33:36.100503 140341499602816 run_classifier.py:461] *** Example ***\n",
            "I0712 04:33:36.101252 140341499602816 run_classifier.py:462] guid: None\n",
            "I0712 04:33:36.102317 140341499602816 run_classifier.py:464] tokens: [CLS] specific responsibilities vary by project area . [SEP]\n",
            "I0712 04:33:36.103368 140341499602816 run_classifier.py:465] input_ids: 101 3563 10198 8137 2011 2622 2181 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0712 04:33:36.104337 140341499602816 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0712 04:33:36.105824 140341499602816 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0712 04:33:36.108746 140341499602816 run_classifier.py:468] label: User Experience & Design (id = 2)\n",
            "I0712 04:33:36.116792 140341499602816 run_classifier.py:461] *** Example ***\n",
            "I0712 04:33:36.118405 140341499602816 run_classifier.py:462] guid: None\n",
            "I0712 04:33:36.119922 140341499602816 run_classifier.py:464] tokens: [CLS] hire , coach and drive a high performance team to deliver against assigned goals while prior ##iti ##zing an outstanding customer experience to customers and prospects . work with the team to ramp up adoption in the late cycle . create activities ( e . g . identifying and handling key customer technical questions and objections ) and develop the strategy to resolve technical imp ##ed ##ime ##nts and sc ##oping out migration work ##load ##s . take responsibility for technical aspects of solutions to include such activities as managing product and solution briefing ##s , proof - of - concept work , and the coordination of supporting technical resources . prepare and deliver product messaging in an effort to highlight google cloud platform value proposition [SEP]\n",
            "I0712 04:33:36.121729 140341499602816 run_classifier.py:465] input_ids: 101 10887 1010 2873 1998 3298 1037 2152 2836 2136 2000 8116 2114 4137 3289 2096 3188 25090 6774 2019 5151 8013 3325 2000 6304 1998 16746 1012 2147 2007 1996 2136 2000 13276 2039 9886 1999 1996 2397 5402 1012 3443 3450 1006 1041 1012 1043 1012 12151 1998 8304 3145 8013 4087 3980 1998 17304 1007 1998 4503 1996 5656 2000 10663 4087 17727 2098 14428 7666 1998 8040 17686 2041 9230 2147 11066 2015 1012 2202 5368 2005 4087 5919 1997 7300 2000 2421 2107 3450 2004 6605 4031 1998 5576 27918 2015 1010 6947 1011 1997 1011 4145 2147 1010 1998 1996 12016 1997 4637 4087 4219 1012 7374 1998 8116 4031 24732 1999 2019 3947 2000 12944 8224 6112 4132 3643 14848 102\n",
            "I0712 04:33:36.123382 140341499602816 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0712 04:33:36.124840 140341499602816 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0712 04:33:36.126322 140341499602816 run_classifier.py:468] label: Finance (id = 17)\n",
            "I0712 04:33:36.135708 140341499602816 run_classifier.py:461] *** Example ***\n",
            "I0712 04:33:36.137410 140341499602816 run_classifier.py:462] guid: None\n",
            "I0712 04:33:36.139488 140341499602816 run_classifier.py:464] tokens: [CLS] work as part of the sales team to identify and qualify platform opportunities and determine if google cloud platform is right for them . identify business and technical requirements , conduct full technical discovery and architect client solutions to meet gathered requirements . take responsibility for leading the technical project including such activities as technology advocacy , supporting bid responses , product and solution briefing ##s , proof - of - concept work , and the coordination of supporting technical resources . work hands - on with google cloud platform products to demonstrate and prototype integration ##s in customer / partner environments . travel frequently around em ##ea for meetings , technical reviews and on ##sit ##e delivery activities . prepare and deliver product messaging in [SEP]\n",
            "I0712 04:33:36.141125 140341499602816 run_classifier.py:465] input_ids: 101 2147 2004 2112 1997 1996 4341 2136 2000 6709 1998 7515 4132 6695 1998 5646 2065 8224 6112 4132 2003 2157 2005 2068 1012 6709 2449 1998 4087 5918 1010 6204 2440 4087 5456 1998 4944 7396 7300 2000 3113 5935 5918 1012 2202 5368 2005 2877 1996 4087 2622 2164 2107 3450 2004 2974 12288 1010 4637 7226 10960 1010 4031 1998 5576 27918 2015 1010 6947 1011 1997 1011 4145 2147 1010 1998 1996 12016 1997 4637 4087 4219 1012 2147 2398 1011 2006 2007 8224 6112 4132 3688 2000 10580 1998 8773 8346 2015 1999 8013 1013 4256 10058 1012 3604 4703 2105 7861 5243 2005 6295 1010 4087 4391 1998 2006 28032 2063 6959 3450 1012 7374 1998 8116 4031 24732 1999 102\n",
            "I0712 04:33:36.142712 140341499602816 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0712 04:33:36.144487 140341499602816 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0712 04:33:36.145903 140341499602816 run_classifier.py:468] label: Technical Solutions (id = 8)\n",
            "I0712 04:33:36.151545 140341499602816 run_classifier.py:461] *** Example ***\n",
            "I0712 04:33:36.153048 140341499602816 run_classifier.py:462] guid: None\n",
            "I0712 04:33:36.154579 140341499602816 run_classifier.py:464] tokens: [CLS] develop and track core metric ##s across all google cloud products , including overall satisfaction , customer effort , net promoter score ( np ##s ) , partner satisfaction , etc . build and maintain reports , dashboard ##s and metric ##s to monitor the performance of the program apply quantitative analysis , data mining and data presentation to provide meaningful business insights to stakeholders ( with support from data scientists ) provide insights that influence and drive actions / improvements across the customer journey demonstrate ability to manage risks at the operational level to ensure smooth execution of survey launches [SEP]\n",
            "I0712 04:33:36.156033 140341499602816 run_classifier.py:465] input_ids: 101 4503 1998 2650 4563 12046 2015 2408 2035 8224 6112 3688 1010 2164 3452 9967 1010 8013 3947 1010 5658 15543 3556 1006 27937 2015 1007 1010 4256 9967 1010 4385 1012 3857 1998 5441 4311 1010 24923 2015 1998 12046 2015 2000 8080 1996 2836 1997 1996 2565 6611 20155 4106 1010 2951 5471 1998 2951 8312 2000 3073 15902 2449 20062 2000 22859 1006 2007 2490 2013 2951 6529 1007 3073 20062 2008 3747 1998 3298 4506 1013 8377 2408 1996 8013 4990 10580 3754 2000 6133 10831 2012 1996 6515 2504 2000 5676 5744 7781 1997 5002 18989 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0712 04:33:36.157511 140341499602816 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0712 04:33:36.158941 140341499602816 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0712 04:33:36.160425 140341499602816 run_classifier.py:468] label: Product & Customer Support (id = 22)\n",
            "I0712 04:33:36.163050 140341499602816 run_classifier.py:461] *** Example ***\n",
            "I0712 04:33:36.164555 140341499602816 run_classifier.py:462] guid: None\n",
            "I0712 04:33:36.166072 140341499602816 run_classifier.py:464] tokens: [CLS] google intern ##s are given a lot of responsibility and the opportunity to make a meaningful contribution to their teams . specific responsibilities are assigned to intern ##s at the start of the program . [SEP]\n",
            "I0712 04:33:36.167967 140341499602816 run_classifier.py:465] input_ids: 101 8224 25204 2015 2024 2445 1037 2843 1997 5368 1998 1996 4495 2000 2191 1037 15902 6691 2000 2037 2780 1012 3563 10198 2024 4137 2000 25204 2015 2012 1996 2707 1997 1996 2565 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0712 04:33:36.169469 140341499602816 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0712 04:33:36.170954 140341499602816 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0712 04:33:36.172983 140341499602816 run_classifier.py:468] label: Finance (id = 17)\n",
            "I0712 04:33:38.456222 140341499602816 run_classifier.py:774] Writing example 0 of 247\n",
            "I0712 04:33:38.460089 140341499602816 run_classifier.py:461] *** Example ***\n",
            "I0712 04:33:38.460945 140341499602816 run_classifier.py:462] guid: None\n",
            "I0712 04:33:38.462210 140341499602816 run_classifier.py:464] tokens: [CLS] conduct data analysis to make business recommendations ( cost - benefit , invest - dive ##st , forecast ##ing , impact analysis ) . deliver effective presentations of findings and recommendations to multiple levels of leadership , creating visual displays of quantitative information . develop and auto ##mate reports , it ##erative ##ly build and prototype dashboard ##s to provide insights at scale , solving for analytical needs . collaborate with cross - functional partners to understand their business needs , formula ##te and complete end - to - end analysis that includes data gathering , analysis , ongoing scaled deliver ##able ##s and presentations . develop a vision and road ##ma ##p for analytics across a product or functional group . own relationships with executive [SEP]\n",
            "I0712 04:33:38.463209 140341499602816 run_classifier.py:465] input_ids: 101 6204 2951 4106 2000 2191 2449 11433 1006 3465 1011 5770 1010 15697 1011 11529 3367 1010 19939 2075 1010 4254 4106 1007 1012 8116 4621 18216 1997 9556 1998 11433 2000 3674 3798 1997 4105 1010 4526 5107 8834 1997 20155 2592 1012 4503 1998 8285 8585 4311 1010 2009 25284 2135 3857 1998 8773 24923 2015 2000 3073 20062 2012 4094 1010 13729 2005 17826 3791 1012 20880 2007 2892 1011 8360 5826 2000 3305 2037 2449 3791 1010 5675 2618 1998 3143 2203 1011 2000 1011 2203 4106 2008 2950 2951 7215 1010 4106 1010 7552 18953 8116 3085 2015 1998 18216 1012 4503 1037 4432 1998 2346 2863 2361 2005 25095 2408 1037 4031 2030 8360 2177 1012 2219 6550 2007 3237 102\n",
            "I0712 04:33:38.464469 140341499602816 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0712 04:33:38.467455 140341499602816 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0712 04:33:38.471050 140341499602816 run_classifier.py:468] label: Business Strategy (id = 14)\n",
            "I0712 04:33:38.474244 140341499602816 run_classifier.py:461] *** Example ***\n",
            "I0712 04:33:38.477602 140341499602816 run_classifier.py:462] guid: None\n",
            "I0712 04:33:38.478537 140341499602816 run_classifier.py:464] tokens: [CLS] intern projects will be determined based on the experience , skills and interests of the student . if you are interested in a particular area , please make sure to include it in your resume . [SEP]\n",
            "I0712 04:33:38.479635 140341499602816 run_classifier.py:465] input_ids: 101 25204 3934 2097 2022 4340 2241 2006 1996 3325 1010 4813 1998 5426 1997 1996 3076 1012 2065 2017 2024 4699 1999 1037 3327 2181 1010 3531 2191 2469 2000 2421 2009 1999 2115 13746 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0712 04:33:38.480588 140341499602816 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0712 04:33:38.481782 140341499602816 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0712 04:33:38.483933 140341499602816 run_classifier.py:468] label: Administrative (id = 20)\n",
            "I0712 04:33:38.489530 140341499602816 run_classifier.py:461] *** Example ***\n",
            "I0712 04:33:38.490442 140341499602816 run_classifier.py:462] guid: None\n",
            "I0712 04:33:38.491709 140341499602816 run_classifier.py:464] tokens: [CLS] collaborate with the operations and design team to develop data center ( dc ) mechanical designs , starting from project conception to issue for construction ( if ##c ) for new dc projects build outs and major infrastructure upgrades . contribute to new technology insertion ##s through collaboration with the core engineering teams during development . create system level concept and sc ##hema ##tics designs , and carry through to detailed designs to ensure requirements are met . manage all site - level mechanical system issues during project design phase . identify and resolve issues with the cross - functional program teams . be responsible for the development of site master plans . [SEP]\n",
            "I0712 04:33:38.492747 140341499602816 run_classifier.py:465] input_ids: 101 20880 2007 1996 3136 1998 2640 2136 2000 4503 2951 2415 1006 5887 1007 6228 5617 1010 3225 2013 2622 13120 2000 3277 2005 2810 1006 2065 2278 1007 2005 2047 5887 3934 3857 21100 1998 2350 6502 18739 1012 9002 2000 2047 2974 23851 2015 2083 5792 2007 1996 4563 3330 2780 2076 2458 1012 3443 2291 2504 4145 1998 8040 28433 14606 5617 1010 1998 4287 2083 2000 6851 5617 2000 5676 5918 2024 2777 1012 6133 2035 2609 1011 2504 6228 2291 3314 2076 2622 2640 4403 1012 6709 1998 10663 3314 2007 1996 2892 1011 8360 2565 2780 1012 2022 3625 2005 1996 2458 1997 2609 3040 3488 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0712 04:33:38.493796 140341499602816 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0712 04:33:38.494801 140341499602816 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0712 04:33:38.495609 140341499602816 run_classifier.py:468] label: Technical Infrastructure (id = 13)\n",
            "I0712 04:33:38.502922 140341499602816 run_classifier.py:461] *** Example ***\n",
            "I0712 04:33:38.504294 140341499602816 run_classifier.py:462] guid: None\n",
            "I0712 04:33:38.507359 140341499602816 run_classifier.py:464] tokens: [CLS] work closely with optimization and web team members to define key performance indicators ( k ##pis ) and critical measurements and then translate these requirements into analytics tracking . work with technical team to ensure accurate and comprehensive web analytics implementation and successful deployment of new solutions . identify opportunities for automation and innovation in the area of web analytics implementation , quality assurance testing to valid ##ate and de ##bu ##g analytics implementations and web experimentation projects . create proper data architecture , data integration , reporting and dashboard ##ing for et ##l projects . work with other teams to integrate online data with other channels , to allow comprehensive business insight across multiple channels . support the internal team in the installation , configuration [SEP]\n",
            "I0712 04:33:38.508505 140341499602816 run_classifier.py:465] input_ids: 101 2147 4876 2007 20600 1998 4773 2136 2372 2000 9375 3145 2836 20390 1006 1047 18136 1007 1998 4187 11702 1998 2059 17637 2122 5918 2046 25095 9651 1012 2147 2007 4087 2136 2000 5676 8321 1998 7721 4773 25095 7375 1998 3144 10813 1997 2047 7300 1012 6709 6695 2005 19309 1998 8144 1999 1996 2181 1997 4773 25095 7375 1010 3737 16375 5604 2000 9398 3686 1998 2139 8569 2290 25095 24977 1998 4773 21470 3934 1012 3443 5372 2951 4294 1010 2951 8346 1010 7316 1998 24923 2075 2005 3802 2140 3934 1012 2147 2007 2060 2780 2000 17409 3784 2951 2007 2060 6833 1010 2000 3499 7721 2449 12369 2408 3674 6833 1012 2490 1996 4722 2136 1999 1996 8272 1010 9563 102\n",
            "I0712 04:33:38.513298 140341499602816 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0712 04:33:38.514743 140341499602816 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0712 04:33:38.515633 140341499602816 run_classifier.py:468] label: Marketing & Communications (id = 10)\n",
            "I0712 04:33:38.525370 140341499602816 run_classifier.py:461] *** Example ***\n",
            "I0712 04:33:38.526611 140341499602816 run_classifier.py:462] guid: None\n",
            "I0712 04:33:38.531329 140341499602816 run_classifier.py:464] tokens: [CLS] drive google ’ s relationship with all levels of ad ##vert ##iser ##s and become the primary point of contact for customers and their dedicated agency partners . serve as a trusted consultant to build a deep understanding of client needs ( both technology and advertising - related ) and to grow lasting partnerships with key clients . drive marketing performance for clients through consultation , implementation of optimal ad technology , and media strategy , thereby growing business and account performance . manage multiple complex opportunities and projects simultaneously ( e . g . coordinate the design of optimal technical set - up , structure proposals , drive optimal implementation of media plans , etc ) . drive cross - functional projects to deliver on [SEP]\n",
            "I0712 04:33:38.532329 140341499602816 run_classifier.py:465] input_ids: 101 3298 8224 1521 1055 3276 2007 2035 3798 1997 4748 16874 17288 2015 1998 2468 1996 3078 2391 1997 3967 2005 6304 1998 2037 4056 4034 5826 1012 3710 2004 1037 9480 8930 2000 3857 1037 2784 4824 1997 7396 3791 1006 2119 2974 1998 6475 1011 3141 1007 1998 2000 4982 9879 13797 2007 3145 7846 1012 3298 5821 2836 2005 7846 2083 16053 1010 7375 1997 15502 4748 2974 1010 1998 2865 5656 1010 8558 3652 2449 1998 4070 2836 1012 6133 3674 3375 6695 1998 3934 7453 1006 1041 1012 1043 1012 13530 1996 2640 1997 15502 4087 2275 1011 2039 1010 3252 10340 1010 3298 15502 7375 1997 2865 3488 1010 4385 1007 1012 3298 2892 1011 8360 3934 2000 8116 2006 102\n",
            "I0712 04:33:38.533728 140341499602816 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0712 04:33:38.535744 140341499602816 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0712 04:33:38.537311 140341499602816 run_classifier.py:468] label: Sales & Account Management (id = 4)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccp5trMwRtmr",
        "colab_type": "text"
      },
      "source": [
        "#Creating a model\n",
        "\n",
        "Now that we've prepared our data, let's focus on building a model. `create_model` does just this below. First, it loads the BERT tf hub module again (this time to extract the computation graph). Next, it creates a single new layer that will be trained to adapt BERT to our sentiment task (i.e. classifying whether a movie review is positive or negative). This strategy of using a mostly trained model is called [fine-tuning](http://wiki.fast.ai/index.php/Fine_tuning)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6o2a5ZIvRcJq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model(is_predicting, input_ids_t, input_mask_t, segment_ids_t,\n",
        "                 input_ids_r, input_mask_r, segment_ids_r,\n",
        "                 labels, num_labels):\n",
        "    \"\"\"Creates a classification model.\"\"\"\n",
        "  \n",
        "    bert_module = hub.Module(BERT_MODEL_HUB, trainable=True)\n",
        "      \n",
        "    bert_inputs_t = dict(input_ids=input_ids_t, input_mask=input_mask_t, segment_ids=segment_ids_t)\n",
        "    bert_inputs_r = dict(input_ids=input_ids_r, input_mask=input_mask_r, segment_ids=segment_ids_r)\n",
        "  \n",
        "    bert_outputs_t = bert_module(inputs=bert_inputs_t, signature=\"tokens\", as_dict=True)\n",
        "    bert_outputs_r = bert_module(inputs=bert_inputs_r, signature=\"tokens\", as_dict=True)\n",
        "  \n",
        "    # Use \"pooled_output\" for classification tasks on an entire sentence.\n",
        "    # Use \"sequence_outputs\" for token-level output.\n",
        "    output_layer_t = bert_outputs_t[\"pooled_output\"]\n",
        "    output_layer_r = bert_outputs_r[\"pooled_output\"]\n",
        "    print(output_layer_t.shape)\n",
        "    print(output_layer_r.shape)\n",
        "    output_layer = tf.concat([output_layer_t, output_layer_r], axis=1)\n",
        "  \n",
        "    hidden_size = output_layer.shape[-1].value\n",
        "  \n",
        "    # Create our own layer to tune for politeness data.\n",
        "    output_weights = tf.get_variable(\n",
        "        \"output_weights\", [num_labels, hidden_size],\n",
        "        initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
        "  \n",
        "    output_bias = tf.get_variable(\n",
        "        \"output_bias\", [num_labels], initializer=tf.zeros_initializer())\n",
        "  \n",
        "    with tf.variable_scope(\"loss\"):\n",
        "  \n",
        "        # Dropout helps prevent overfitting\n",
        "        output_layer = tf.nn.dropout(output_layer, rate=0.1)\n",
        "    \n",
        "        logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n",
        "        logits = tf.nn.bias_add(logits, output_bias)\n",
        "        log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
        "    \n",
        "        # Convert labels into one-hot encoding\n",
        "        one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n",
        "    \n",
        "        predicted_labels = tf.squeeze(tf.argmax(log_probs, axis=-1, output_type=tf.int32))\n",
        "        # If we're predicting, we want predicted labels and the probabiltiies.\n",
        "        if is_predicting:\n",
        "            return (predicted_labels, log_probs)\n",
        "    \n",
        "        # If we're train/eval, compute loss between predicted and actual label\n",
        "        per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n",
        "        loss = tf.reduce_mean(per_example_loss)\n",
        "        return (loss, predicted_labels, log_probs)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpE0ZIDOCQzE",
        "colab_type": "text"
      },
      "source": [
        "Next we'll wrap our model function in a `model_fn_builder` function that adapts our model to work for training, evaluation, and prediction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnH-AnOQ9KKW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model_fn_builder actually creates our model function\n",
        "# using the passed parameters for num_labels, learning_rate, etc.\n",
        "def model_fn_builder(num_labels, learning_rate, num_train_steps, num_warmup_steps):\n",
        "    \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n",
        "    def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n",
        "        \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n",
        "    \n",
        "        input_ids_t = features[\"input_ids_t\"]\n",
        "        input_mask_t = features[\"input_mask_t\"]\n",
        "        segment_ids_t = features[\"segment_ids_t\"]\n",
        "        \n",
        "        input_ids_r = features[\"input_ids_r\"]\n",
        "        input_mask_r = features[\"input_mask_r\"]\n",
        "        segment_ids_r = features[\"segment_ids_r\"]\n",
        "        \n",
        "        label_ids = features[\"label_ids\"]\n",
        "    \n",
        "        is_predicting = (mode == tf.estimator.ModeKeys.PREDICT)\n",
        "        \n",
        "        # TRAIN and EVAL\n",
        "        if not is_predicting:\n",
        "            (loss, predicted_labels, log_probs) = create_model(\n",
        "              is_predicting, input_ids_t, input_mask_t, segment_ids_t,\n",
        "              input_ids_r, input_mask_r, segment_ids_r, label_ids, num_labels)\n",
        "      \n",
        "            train_op = bert.optimization.create_optimizer(\n",
        "                loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu=False)\n",
        "      \n",
        "            # Calculate evaluation metrics. \n",
        "            def metric_fn(label_ids, predicted_labels):\n",
        "                accuracy = tf.metrics.accuracy(label_ids, predicted_labels)\n",
        "                return {\n",
        "                    \"eval_accuracy\": accuracy\n",
        "                }\n",
        "      \n",
        "            eval_metrics = metric_fn(label_ids, predicted_labels)\n",
        "      \n",
        "            if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "              return tf.estimator.EstimatorSpec(mode=mode,\n",
        "                loss=loss,\n",
        "                train_op=train_op)\n",
        "            else:\n",
        "                return tf.estimator.EstimatorSpec(mode=mode,\n",
        "                  loss=loss,\n",
        "                  eval_metric_ops=eval_metrics)\n",
        "        else:\n",
        "            (predicted_labels, log_probs) = create_model(\n",
        "              is_predicting, input_ids_t, input_mask_t, segment_ids_t,\n",
        "              input_ids_r, input_mask_r, segment_ids_r, label_ids, num_labels)\n",
        "      \n",
        "            predictions = {\n",
        "                'probabilities': log_probs,\n",
        "                'labels': predicted_labels\n",
        "            }\n",
        "            return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
        "  \n",
        "    # Return the actual model function in the closure\n",
        "    return model_fn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjwJ4bTeWXD8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compute train and warmup steps from batch size\n",
        "# These hyperparameters are copied from this colab notebook (https://colab.sandbox.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb)\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 2e-5\n",
        "NUM_TRAIN_EPOCHS = 10.0\n",
        "# Warmup is a period of time where hte learning rate \n",
        "# is small and gradually increases--usually helps training.\n",
        "WARMUP_PROPORTION = 0.1\n",
        "# Model configs\n",
        "SAVE_CHECKPOINTS_STEPS = 500\n",
        "SAVE_SUMMARY_STEPS = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emHf9GhfWBZ_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compute # train and warmup steps from batch size\n",
        "num_train_steps = int(len(train_features_t) / BATCH_SIZE * NUM_TRAIN_EPOCHS)\n",
        "num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEJldMr3WYZa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Specify output directory and number of checkpoint steps to save\n",
        "run_config = tf.estimator.RunConfig(\n",
        "    model_dir=OUTPUT_DIR,\n",
        "    save_summary_steps=SAVE_SUMMARY_STEPS,\n",
        "    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_WebpS1X97v",
        "colab_type": "code",
        "outputId": "408ea1aa-559a-4484-c525-cbccae91af65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "model_fn = model_fn_builder(\n",
        "  num_labels=len(label_list),\n",
        "  learning_rate=LEARNING_RATE,\n",
        "  num_train_steps=num_train_steps,\n",
        "  num_warmup_steps=num_warmup_steps)\n",
        "\n",
        "estimator = tf.estimator.Estimator(\n",
        "  model_fn=model_fn,\n",
        "  config=run_config,\n",
        "  params={\"batch_size\": BATCH_SIZE})\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I0712 04:34:27.435110 140341499602816 estimator.py:209] Using config: {'_model_dir': 'bert_output', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 500, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fa37b27da20>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOO3RfG1DYLo",
        "colab_type": "text"
      },
      "source": [
        "Next we create an input builder function that takes our training feature set (`train_features`) and produces a generator. This is a pretty standard design pattern for working with Tensorflow [Estimators](https://www.tensorflow.org/guide/estimators)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RkF7xbxSRdws",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def input_fn_builder(features_t, features_r, seq_length_t, seq_length_r, is_training, drop_remainder):\n",
        "    \"\"\"Creates an `input_fn` closure to be passed to TPUEstimator.\"\"\"\n",
        "  \n",
        "    all_input_ids_t = []\n",
        "    all_input_mask_t = []\n",
        "    all_segment_ids_t = []\n",
        "    \n",
        "    all_input_ids_r = []\n",
        "    all_input_mask_r = []\n",
        "    all_segment_ids_r = []\n",
        "    \n",
        "    all_label_ids = []\n",
        "  \n",
        "    for feature_t in features_t:\n",
        "        all_input_ids_t.append(feature_t.input_ids)\n",
        "        all_input_mask_t.append(feature_t.input_mask)\n",
        "        all_segment_ids_t.append(feature_t.segment_ids)\n",
        "        all_label_ids.append(feature_t.label_id)\n",
        "        \n",
        "    for feature_r in features_r:\n",
        "        all_input_ids_r.append(feature_r.input_ids)\n",
        "        all_input_mask_r.append(feature_r.input_mask)\n",
        "        all_segment_ids_r.append(feature_r.segment_ids)\n",
        "  \n",
        "    def input_fn(params):\n",
        "        \"\"\"The actual input function.\"\"\"\n",
        "        batch_size = params[\"batch_size\"]\n",
        "    \n",
        "        num_examples = len(features_t)\n",
        "    \n",
        "        # This is for demo purposes and does NOT scale to large data sets. We do\n",
        "        # not use Dataset.from_generator() because that uses tf.py_func which is\n",
        "        # not TPU compatible. The right way to load data is with TFRecordReader.\n",
        "        d = tf.data.Dataset.from_tensor_slices({\n",
        "            \"input_ids_t\":\n",
        "                tf.constant(\n",
        "                    all_input_ids_t, shape=[num_examples, seq_length_t],\n",
        "                    dtype=tf.int32),\n",
        "            \"input_mask_t\":\n",
        "                tf.constant(\n",
        "                    all_input_mask_t,\n",
        "                    shape=[num_examples, seq_length_t],\n",
        "                    dtype=tf.int32),\n",
        "            \"segment_ids_t\":\n",
        "                tf.constant(\n",
        "                    all_segment_ids_t,\n",
        "                    shape=[num_examples, seq_length_t],\n",
        "                    dtype=tf.int32),\n",
        "            \"input_ids_r\":\n",
        "                tf.constant(\n",
        "                    all_input_ids_r, shape=[num_examples, seq_length_r],\n",
        "                    dtype=tf.int32),\n",
        "            \"input_mask_r\":\n",
        "                tf.constant(\n",
        "                    all_input_mask_r,\n",
        "                    shape=[num_examples, seq_length_r],\n",
        "                    dtype=tf.int32),\n",
        "            \"segment_ids_r\":\n",
        "                tf.constant(\n",
        "                    all_segment_ids_r,\n",
        "                    shape=[num_examples, seq_length_r],\n",
        "                    dtype=tf.int32),\n",
        "            \"label_ids\":\n",
        "                tf.constant(all_label_ids, shape=[num_examples], dtype=tf.int32),\n",
        "        })\n",
        "    \n",
        "        if is_training:\n",
        "            d = d.repeat()\n",
        "            d = d.shuffle(buffer_size=100)\n",
        "    \n",
        "        d = d.batch(batch_size=batch_size, drop_remainder=drop_remainder)\n",
        "        return d\n",
        "  \n",
        "    return input_fn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Pv2bAlOX_-K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create an input function for training. drop_remainder = True for using TPUs.\n",
        "train_input_fn = input_fn_builder(\n",
        "    features_t=train_features_t,\n",
        "    features_r=train_features_r,\n",
        "    seq_length_t=MAX_SEQ_LENGTH_t,\n",
        "    seq_length_r=MAX_SEQ_LENGTH_r,\n",
        "    is_training=True,\n",
        "    drop_remainder=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6Nukby2EB6-",
        "colab_type": "text"
      },
      "source": [
        "Now we train our model! For me, using a Colab notebook running on Google's GPUs, my training time was about 14 minutes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nucD4gluYJmK",
        "colab_type": "code",
        "outputId": "0299ea9c-16f9-4549-e16e-e0be671c4288",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        }
      },
      "source": [
        "print(f'Beginning Training!')\n",
        "current_time = datetime.now()\n",
        "estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n",
        "print(\"Training took time \", datetime.now() - current_time)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0712 04:34:48.422138 140341499602816 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Beginning Training!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0712 04:34:49.119671 140341499602816 estimator.py:1145] Calling model_fn.\n",
            "I0712 04:34:51.884239 140341499602816 saver.py:1499] Saver not created because there are no variables in the graph to restore\n",
            "I0712 04:34:52.656802 140341499602816 saver.py:1499] Saver not created because there are no variables in the graph to restore\n",
            "W0712 04:34:52.841491 140341499602816 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/bert/optimization.py:27: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.\n",
            "\n",
            "W0712 04:34:52.843405 140341499602816 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/bert/optimization.py:32: The name tf.train.polynomial_decay is deprecated. Please use tf.compat.v1.train.polynomial_decay instead.\n",
            "\n",
            "W0712 04:34:52.849877 140341499602816 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/optimizer_v2/learning_rate_schedule.py:409: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n",
            "W0712 04:34:52.867155 140341499602816 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/bert/optimization.py:70: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(?, 768)\n",
            "(?, 768)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0712 04:34:53.257192 140341499602816 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1205: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0712 04:35:00.017908 140341499602816 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/bert/optimization.py:117: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
            "I0712 04:35:04.428295 140341499602816 estimator.py:1147] Done calling model_fn.\n",
            "I0712 04:35:04.431043 140341499602816 basic_session_run_hooks.py:541] Create CheckpointSaverHook.\n",
            "I0712 04:35:08.647073 140341499602816 monitored_session.py:240] Graph was finalized.\n",
            "I0712 04:35:14.779228 140341499602816 session_manager.py:500] Running local_init_op.\n",
            "I0712 04:35:15.044456 140341499602816 session_manager.py:502] Done running local_init_op.\n",
            "I0712 04:35:26.854594 140341499602816 basic_session_run_hooks.py:606] Saving checkpoints for 0 into bert_output/model.ckpt.\n",
            "I0712 04:35:48.148109 140341499602816 basic_session_run_hooks.py:262] loss = 3.2135198, step = 0\n",
            "I0712 04:38:03.844927 140341499602816 basic_session_run_hooks.py:692] global_step/sec: 0.736916\n",
            "I0712 04:38:03.846440 140341499602816 basic_session_run_hooks.py:260] loss = 0.21199511, step = 100 (135.698 sec)\n",
            "I0712 04:39:50.718113 140341499602816 basic_session_run_hooks.py:692] global_step/sec: 0.935689\n",
            "I0712 04:39:50.719756 140341499602816 basic_session_run_hooks.py:260] loss = 0.020621262, step = 200 (106.873 sec)\n",
            "I0712 04:41:37.418835 140341499602816 basic_session_run_hooks.py:692] global_step/sec: 0.937201\n",
            "I0712 04:41:37.420456 140341499602816 basic_session_run_hooks.py:260] loss = 0.07252242, step = 300 (106.701 sec)\n",
            "I0712 04:41:44.870047 140341499602816 basic_session_run_hooks.py:606] Saving checkpoints for 308 into bert_output/model.ckpt.\n",
            "I0712 04:41:52.952862 140341499602816 estimator.py:368] Loss for final step: 0.01625444.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training took time  0:07:04.551520\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmbLTVniARy3",
        "colab_type": "text"
      },
      "source": [
        "Now let's use our test data to see how well our model did:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JIhejfpyJ8Bx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_input_fn = input_fn_builder(\n",
        "    features_t=test_features_t,\n",
        "    features_r=test_features_r,\n",
        "    seq_length_t=MAX_SEQ_LENGTH_t,\n",
        "    seq_length_r=MAX_SEQ_LENGTH_r,\n",
        "    is_training=False,\n",
        "    drop_remainder=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PPVEXhNjYXC-",
        "colab_type": "code",
        "outputId": "b97d17fb-4be1-45d3-f12b-5988d9d35b18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "estimator.evaluate(input_fn=test_input_fn, steps=None)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I0712 04:44:32.976675 140341499602816 estimator.py:1145] Calling model_fn.\n",
            "I0712 04:44:36.276080 140341499602816 saver.py:1499] Saver not created because there are no variables in the graph to restore\n",
            "I0712 04:44:37.108463 140341499602816 saver.py:1499] Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(?, 768)\n",
            "(?, 768)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
            "I0712 04:44:49.451953 140341499602816 estimator.py:1147] Done calling model_fn.\n",
            "I0712 04:44:49.477385 140341499602816 evaluation.py:255] Starting evaluation at 2019-07-12T04:44:49Z\n",
            "I0712 04:44:50.854363 140341499602816 monitored_session.py:240] Graph was finalized.\n",
            "W0712 04:44:50.860321 140341499602816 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "I0712 04:44:50.868097 140341499602816 saver.py:1280] Restoring parameters from bert_output/model.ckpt-308\n",
            "I0712 04:44:54.765058 140341499602816 session_manager.py:500] Running local_init_op.\n",
            "I0712 04:44:55.041480 140341499602816 session_manager.py:502] Done running local_init_op.\n",
            "I0712 04:45:00.071524 140341499602816 evaluation.py:275] Finished evaluation at 2019-07-12-04:45:00\n",
            "I0712 04:45:00.072712 140341499602816 estimator.py:2039] Saving dict for global step 308: eval_accuracy = 0.87449396, global_step = 308, loss = 0.5120736\n",
            "I0712 04:45:02.945542 140341499602816 estimator.py:2099] Saving 'checkpoint_path' summary for global step 308: bert_output/model.ckpt-308\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'eval_accuracy': 0.87449396, 'global_step': 308, 'loss': 0.5120736}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    }
  ]
}