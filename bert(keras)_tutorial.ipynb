{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bert(keras)_tutorial.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ztjfreedom/colab/blob/master/bert(keras)_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMydRyTNH21n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! pip install tf-nightly sentencepiece spacy ftfy -q # tensorflow version >= 1.13 fixed some problem of keras tpu. After 1.13.0 released, please change tensorflow to 1.13.0 stable"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqCJEO-KFgGt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import sys"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMJlxlG2FnFp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!test -d bert_keras_repo ||  git clone https://github.com/Separius/BERT-keras.git bert_keras_repo --recursive --quiet\n",
        "if not 'bert_keras_repo' in sys.path:\n",
        "    sys.path = [sys.path[0]] + ['bert_keras_repo'] + sys.path[1:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WV1_SO2Ty8-7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# @title Select your keras backend { display-mode: \"form\", run: \"auto\" }\n",
        "backend = 'tensorflow' # @param [\"tensorflow\", \"theano\"]\n",
        "#@markdown Theano does't not support training.\n",
        "os.environ['KERAS_BACKEND'] = backend\n",
        "from transformer import refresh_keras_backend\n",
        "\n",
        "if backend == 'tensorflow':\n",
        "  #@markdown Only tensorflow backend has TPU support.\n",
        "  use_tpu=False # @param {type:\"boolean\"}\n",
        "else:\n",
        "  use_tpu=False\n",
        "\n",
        "if 'keras_first_import' not in globals().keys():\n",
        "  keras_first_import = True\n",
        "else:\n",
        "  raise ValueError('You should restart the kernel if you change the optional, or you may meet some problems.')\n",
        "refresh_keras_backend(use_tpu=use_tpu)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6aNsg2AF4HH",
        "colab_type": "text"
      },
      "source": [
        "# Tutorial"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTNiOM0E3c7l",
        "colab_type": "text"
      },
      "source": [
        "## Simple Example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JzQCNpt6Fy-V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This is a tutorial on using this library\n",
        "# first off we need a text_encoder so we would know our vocab_size (and later on use it to encode sentences)\n",
        "from data.vocab import SentencePieceTextEncoder  # you could also import OpenAITextEncoder\n",
        "\n",
        "sentence_piece_encoder = SentencePieceTextEncoder(text_corpus_address='bert_keras_repo/openai/model/params_shapes.json',\n",
        "                                                  model_name='tutorial', vocab_size=20)\n",
        "\n",
        "with open(os.path.join('bert_keras_repo/openai/model', 'params_shapes.json')) as f:\n",
        "    for line in f:\n",
        "        print(line)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UL8vJ6rEF9rG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# now we need a sequence encoder\n",
        "from transformer.model import create_transformer\n",
        "\n",
        "sequence_encoder_config = {\n",
        "    'embedding_dim': 6,\n",
        "    'vocab_size': sentence_piece_encoder.vocab_size,\n",
        "    'max_len': 8,\n",
        "    'trainable_pos_embedding': False,\n",
        "    'num_heads': 2,\n",
        "    'num_layers': 3,\n",
        "    'd_hid': 12,\n",
        "    'use_attn_mask': True\n",
        "}\n",
        "sequence_encoder = create_transformer(**sequence_encoder_config)\n",
        "\n",
        "import keras as keras\n",
        "\n",
        "assert type(sequence_encoder) == keras.Model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1m_NGzIF_eS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# now look at the inputs:\n",
        "print(sequence_encoder.inputs)  # tokens, segment_ids, pos_ids, attn_mask"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rHhZ5VczGDXL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tokens is a batch_size * seq_len tensor containing token_ids\n",
        "# segment_ids is a batch_size * seq_len tensor containing segment_ids (as in segment_{a, b} of BERT)\n",
        "# pos_ids is a batch_size * seq_len tensor containing position ids (0..max_len)(you will see how can easily generate it)\n",
        "# attn_mask is a batch_size * 1 * max_len * max_len tensor and can encode padding and causality constraints (ignore it for now)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YNcDG1IOGFaS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for outputs we have:\n",
        "print(sequence_encoder.outputs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjoRhUh2GG_C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 'a long name' is a batch_size * max_len * embedding_dim tensor which is our encoded sequence (here with a transformer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmEGR1RBGJO6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# now it's time to train it both on pre-training tasks and fine-tuning tasks\n",
        "# first we need to define our tasks:"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5TPjU7APGLQ5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from data.dataset import TaskMetadata, TaskWeightScheduler\n",
        "\n",
        "tasks = [TaskMetadata('lm', is_token_level=True,\n",
        "                      num_classes=sentence_piece_encoder.vocab_size + sentence_piece_encoder.SPECIAL_COUNT,\n",
        "                      dropout=0,\n",
        "                      weight_scheduler=TaskWeightScheduler(active_in_pretrain=True, active_in_finetune=False,\n",
        "                                                           pretrain_value=1.0))]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D14OHODJGM9s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# well let's pause and see what this task is, 'lm' is the name of the task\n",
        "# and 'lm' is also a special task, because it uses a tied decoder (if you don't know what it means, ignore it)\n",
        "# then num_classes is set to vocab+special_count which is actually incorrect (we are never going to predict mask, pad, )\n",
        "# but it's here for the tied decoder to work; dropout is for the decoder of this task\n",
        "# and finally a weight_scheduler, in this example we are only training on 'lm' task during the pretraing but not after\n",
        "# now let's add a more complex task, a sentence level one with a complex weight_scheduler"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71D19P_3GO4S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ComplexTaskWeightScheduler(TaskWeightScheduler):  # note: this is an example, it is not a clean code\n",
        "    def __init__(self, number_of_pretrain_steps, number_of_finetune_steps):\n",
        "        super().__init__(active_in_pretrain=True, active_in_finetune=True)\n",
        "        self.number_of_pretrain_steps = number_of_pretrain_steps\n",
        "        self.number_of_finetune_steps = number_of_finetune_steps\n",
        "\n",
        "    def get(self, is_pretrain: bool, step: int) -> float:\n",
        "        return step / (self.number_of_pretrain_steps if is_pretrain else self.number_of_finetune_steps)\n",
        "\n",
        "\n",
        "number_of_pretrain_steps = 100\n",
        "number_of_finetune_steps = 100\n",
        "# in this task we are going to count the number of tokens in a sentence and predict if it's odd or not\n",
        "tasks.append(TaskMetadata('odd', is_token_level=False, num_classes=2, dropout=0.3,\n",
        "                          weight_scheduler=ComplexTaskWeightScheduler(number_of_pretrain_steps,\n",
        "                                                                      number_of_finetune_steps)))\n",
        "\n",
        "# and let's add a unsolvable task for fun\n",
        "tasks.append(TaskMetadata('lm_random', is_token_level=True,\n",
        "                          num_classes=sentence_piece_encoder.vocab_size + sentence_piece_encoder.SPECIAL_COUNT,\n",
        "                          dropout=0.3,\n",
        "                          weight_scheduler=TaskWeightScheduler(active_in_pretrain=True, active_in_finetune=True,\n",
        "                                                               pretrain_value=0.5)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPI_SDNeGRXZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# now we need a data generator, for a good reference see data.lm_dataset._get_lm_generator_single or _double\n",
        "# but for now we are going to write a simple one so you understand the Sentence class\n",
        "# again this is a simple generator just showing you the core ideas\n",
        "# so for 'lm' task we are just going to predict the token itself (identity function)\n",
        "# first we are importing things, ignore them for now, I will explain them in a bit"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Th0CnybYGT3v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from data.dataset import Sentence, TokenTaskData, SentenceTaskData, TextEncoder\n",
        "from data.lm_dataset import _create_batch\n",
        "import random\n",
        "\n",
        "\n",
        "def tutorial_batch_generator(vocab_size: int, max_len: int, batch_size: int, steps: int):\n",
        "    def sentence_generator():\n",
        "        for _ in range(steps):\n",
        "            # for each sentence we are going to generate up to max_len tokens\n",
        "            seq_len = random.randint(1, max_len - 1)\n",
        "            # and this is their ids (in reality we have to use our TextEncoder instance here)\n",
        "            tokens = [random.randrange(vocab_size) for _ in range(seq_len)]\n",
        "            # we manually set the last token to EOS (which we will see how it's calculated)\n",
        "            tokens[-1] = eos_id\n",
        "            yield Sentence(\n",
        "                tokens=tokens,\n",
        "                padding_mask=[True] * seq_len,  # it means that non of the original tokens are padding\n",
        "                segments=[0] * seq_len,  # for this simple example we are going to use segment_a(0) for all of them\n",
        "                token_classification={  # we put labels here (for token level tasks)\n",
        "                    # name_of_the_task: TokenTaskData(target(aka label), label_mask)\n",
        "                    # there might be situations that you are only interested in predictions for certain tokens,\n",
        "                    # you can use mask in those situations (see the bert paper to understand this)\n",
        "                    'lm': TokenTaskData(tokens, [True] * seq_len),\n",
        "                    # this task is unsolvable so we will see the loss not decreasing\n",
        "                    'lm_random': TokenTaskData([random.randrange(vocab_size) for i in range(seq_len)],\n",
        "                                               [True] * seq_len)\n",
        "                },\n",
        "                # similar to token_classification, it's also a dictionary of task to label\n",
        "                # SentenceTaskData contains (label, where to extract that label_from)\n",
        "                # in this case we are going to predict whether a sentence has\n",
        "                # odd number of tokens or not whenever we see eos token\n",
        "                sentence_classification={'odd': SentenceTaskData(seq_len % 2, seq_len - 1)}\n",
        "            )\n",
        "\n",
        "    # we need eos_id and it's always at this place\n",
        "    eos_id = vocab_size + TextEncoder.EOS_OFFSET\n",
        "    # likewise for pad_id\n",
        "    pad_id = vocab_size + TextEncoder.PAD_OFFSET\n",
        "    generator = sentence_generator()\n",
        "    batch = []\n",
        "    for item in generator:\n",
        "        batch.append(item)\n",
        "        if len(batch) == batch_size:\n",
        "            batch = _create_batch(batch, pad_id, max_len)  # magic to pad and batch sentences\n",
        "            # at the end it will generate a SentenceBatch which is more than just a list of Sentence\n",
        "            yield batch\n",
        "            batch = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WjqYgas3GWfg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# now we instantiate our generator\n",
        "# we are going to set steps to a large number (it doesn't matter)\n",
        "# we have to set batch_size too"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHtttrcCGYw1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 8\n",
        "generator = tutorial_batch_generator(sentence_piece_encoder.vocab_size, sequence_encoder_config['max_len'],\n",
        "                                     batch_size, steps=10000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKj42qx_GbGl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# now let the fun begin :D"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dYYCx5nGcl_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformer.train import train_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNIp8t7zsxpU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# @title Get TPUStrategy  here\n",
        "if use_tpu:\n",
        "    assert 'COLAB_TPU_ADDR' in os.environ, 'ERROR: Not connected to a TPU runtime; Maybe you should switch hardware accelerator to TPU for TPU support'\n",
        "    import tensorflow as tf\n",
        "    tpu_address = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "    strategy = tf.contrib.tpu.TPUDistributionStrategy(\n",
        "        tf.contrib.cluster_resolver.TPUClusterResolver(tpu=tpu_address)\n",
        "    )\n",
        "else:\n",
        "    strategy = None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YUYvd56CGdy6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# we are going to use the same generator for both pretrain and finetune"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07z6vttVGfyX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "m = train_model(base_model=sequence_encoder, is_causal=False, tasks_meta_data=tasks, pretrain_generator=generator,\n",
        "                finetune_generator=generator, pretrain_epochs=100, pretrain_steps=number_of_pretrain_steps // 100,\n",
        "                finetune_epochs=100, finetune_steps=number_of_finetune_steps // 100, verbose=2, TPUStrategy=strategy)\n",
        "# now m is ready to be used!\n",
        "print(m.inputs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLHiWcd6ichC",
        "colab_type": "text"
      },
      "source": [
        "### Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBdQePIsGhxj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# token, segment, pos, att_mask, odd_mask (where to extract the class from)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dc7KaNV_GjYb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "bs = 8 # must be multiple of 8\n",
        "vs = sentence_piece_encoder.vocab_size\n",
        "sl = sequence_encoder_config['max_len']\n",
        "# generate random tokens\n",
        "token = np.random.randint(0, vs, (bs, sl))\n",
        "# generate random seg_id\n",
        "segment = np.random.randint(0, 2, (bs, sl))\n",
        "# generate pos_id\n",
        "from transformer.train import generate_pos_ids\n",
        "\n",
        "pos = generate_pos_ids(bs, sl)\n",
        "# generate attn_mask\n",
        "from data.dataset import create_attention_mask\n",
        "\n",
        "# first generate a padding_mask(1 means it is not padded)\n",
        "pad_mask = np.random.randint(0, 2, (bs, sl)).astype(np.int8)\n",
        "# create the mask\n",
        "mask = create_attention_mask(pad_mask=pad_mask, is_causal=False)\n",
        "# generate target index\n",
        "target_index = np.random.randint(0, sl, (bs, 1))\n",
        "res = m.predict([token, segment, pos, mask, target_index], verbose=2)\n",
        "assert res[0].shape == (bs, sl, vs + TextEncoder.SPECIAL_COUNT)  # lm\n",
        "assert res[1].shape == (bs, 1, 2)  # odd\n",
        "assert res[2].shape == (bs, sl, vs + TextEncoder.SPECIAL_COUNT)  # random_lm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V16tO2hpTrki",
        "colab_type": "text"
      },
      "source": [
        "## Google BERT model load example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_4yp35FuZib",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Available pretrained model checkpoints:\n",
        "#   uncased_L-12_H-768_A-12: uncased BERT base model\n",
        "#   uncased_L-24_H-1024_A-16: uncased BERT large model\n",
        "#   cased_L-12_H-768_A-12: cased BERT large model\n",
        "BERT_MODEL = 'uncased_L-12_H-768_A-12' #@param {type:\"string\"}\n",
        "download_url = 'https://storage.googleapis.com/bert_models/2018_10_18/{}.zip'.format(BERT_MODEL)\n",
        "zip_path = '{}.zip'.format(BERT_MODEL)\n",
        "! wget $download_url && unzip $zip_path\n",
        "BERT_PRETRAINED_DIR = os.path.realpath(BERT_MODEL)\n",
        "print('***** BERT pretrained directory: {} *****'.format(BERT_PRETRAINED_DIR))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "diq83g6subs9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformer.load import load_google_bert\n",
        "g_bert = load_google_bert(base_location=BERT_PRETRAINED_DIR+'/')\n",
        "g_bert.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjfwtkDgRR1l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(g_bert.inputs,g_bert.outputs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IV6WzY2awqfz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# @title Compile keras  model here\n",
        "from transformer.train import train_model\n",
        "if use_tpu:\n",
        "    assert 'COLAB_TPU_ADDR' in os.environ, 'ERROR: Not connected to a TPU runtime; Maybe you should switch hardware accelerator to TPU for TPU support'\n",
        "    import tensorflow as tf\n",
        "    tpu_address = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "    strategy = tf.contrib.tpu.TPUDistributionStrategy(\n",
        "        tf.contrib.cluster_resolver.TPUClusterResolver(tpu=tpu_address)\n",
        "    )\n",
        "    g_bert = tf.contrib.tpu.keras_to_tpu_model(\n",
        "                      g_bert, strategy=strategy)\n",
        "g_bert.compile('adam', 'mse')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qe48g-XqRkR7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "def random_data_generator():\n",
        "    while True:\n",
        "        x1 = np.random.randint(0,30000,(8,512))\n",
        "        x2 = np.random.randint(0,2,(8,512))\n",
        "        x3 = np.random.randint(0,512,(8,512))\n",
        "        x4 = np.random.rand(8,1,512,512)\n",
        "        y = np.random.rand(8,512,768)\n",
        "        yield [x1,x2,x3,x4],y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUiuchGMSeqC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "d_g = random_data_generator()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qyTV6aEdSiXF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x,y = next(d_g)\n",
        "print(g_bert.train_on_batch(x,y))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}