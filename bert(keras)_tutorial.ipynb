{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bert(keras)_tutorial.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ztjfreedom/colab/blob/master/bert(keras)_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMydRyTNH21n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! pip install tf-nightly sentencepiece spacy ftfy -q # tensorflow version >= 1.13 fixed some problem of keras tpu. After 1.13.0 released, please change tensorflow to 1.13.0 stable"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqCJEO-KFgGt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import sys"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMJlxlG2FnFp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!test -d bert_keras_repo ||  git clone https://github.com/Separius/BERT-keras.git bert_keras_repo --recursive --quiet\n",
        "if not 'bert_keras_repo' in sys.path:\n",
        "    sys.path = [sys.path[0]] + ['bert_keras_repo'] + sys.path[1:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WV1_SO2Ty8-7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "c52cd88d-0cb1-45e6-e086-ce2ecdcf5d6b"
      },
      "source": [
        "# @title Select your keras backend { display-mode: \"form\", run: \"auto\" }\n",
        "backend = 'tensorflow' # @param [\"tensorflow\", \"theano\"]\n",
        "#@markdown Theano does't not support training.\n",
        "os.environ['KERAS_BACKEND'] = backend\n",
        "from transformer import refresh_keras_backend\n",
        "\n",
        "if backend == 'tensorflow':\n",
        "  #@markdown Only tensorflow backend has TPU support.\n",
        "  use_tpu=False # @param {type:\"boolean\"}\n",
        "else:\n",
        "  use_tpu=False\n",
        "\n",
        "if 'keras_first_import' not in globals().keys():\n",
        "  keras_first_import = True\n",
        "else:\n",
        "  raise ValueError('You should restart the kernel if you change the optional, or you may meet some problems.')\n",
        "refresh_keras_backend(use_tpu=use_tpu)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<module 'keras.backend' from '/usr/local/lib/python3.6/dist-packages/keras/backend/__init__.py'>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6aNsg2AF4HH",
        "colab_type": "text"
      },
      "source": [
        "# Tutorial"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTNiOM0E3c7l",
        "colab_type": "text"
      },
      "source": [
        "## Simple Example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JzQCNpt6Fy-V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "85e47f18-ef34-4b0a-fddb-a37007fa3f9d"
      },
      "source": [
        "# This is a tutorial on using this library\n",
        "# first off we need a text_encoder so we would know our vocab_size (and later on use it to encode sentences)\n",
        "from data.vocab import SentencePieceTextEncoder  # you could also import OpenAITextEncoder\n",
        "\n",
        "sentence_piece_encoder = SentencePieceTextEncoder(text_corpus_address='bert_keras_repo/openai/model/params_shapes.json',\n",
        "                                                  model_name='tutorial', vocab_size=20)\n",
        "\n",
        "with open(os.path.join('bert_keras_repo/openai/model', 'params_shapes.json')) as f:\n",
        "    for line in f:\n",
        "        print(line)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[512, 768], [40478, 768], [1, 768, 2304], [2304], [1, 768, 768], [768], [768], [768], [1, 768, 3072], [3072], [1, 3072, 768], [768], [768], [768], [1, 768, 2304], [2304], [1, 768, 768], [768], [768], [768], [1, 768, 3072], [3072], [1, 3072, 768], [768], [768], [768], [1, 768, 2304], [2304], [1, 768, 768], [768], [768], [768], [1, 768, 3072], [3072], [1, 3072, 768], [768], [768], [768], [1, 768, 2304], [2304], [1, 768, 768], [768], [768], [768], [1, 768, 3072], [3072], [1, 3072, 768], [768], [768], [768], [1, 768, 2304], [2304], [1, 768, 768], [768], [768], [768], [1, 768, 3072], [3072], [1, 3072, 768], [768], [768], [768], [1, 768, 2304], [2304], [1, 768, 768], [768], [768], [768], [1, 768, 3072], [3072], [1, 3072, 768], [768], [768], [768], [1, 768, 2304], [2304], [1, 768, 768], [768], [768], [768], [1, 768, 3072], [3072], [1, 3072, 768], [768], [768], [768], [1, 768, 2304], [2304], [1, 768, 768], [768], [768], [768], [1, 768, 3072], [3072], [1, 3072, 768], [768], [768], [768], [1, 768, 2304], [2304], [1, 768, 768], [768], [768], [768], [1, 768, 3072], [3072], [1, 3072, 768], [768], [768], [768], [1, 768, 2304], [2304], [1, 768, 768], [768], [768], [768], [1, 768, 3072], [3072], [1, 3072, 768], [768], [768], [768], [1, 768, 2304], [2304], [1, 768, 768], [768], [768], [768], [1, 768, 3072], [3072], [1, 3072, 768], [768], [768], [768], [1, 768, 2304], [2304], [1, 768, 768], [768], [768], [768], [1, 768, 3072], [3072], [1, 3072, 768], [768], [768], [768]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UL8vJ6rEF9rG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "61331446-01fc-40a0-e7fa-3a5de32f9f3a"
      },
      "source": [
        "# now we need a sequence encoder\n",
        "from transformer.model import create_transformer\n",
        "\n",
        "sequence_encoder_config = {\n",
        "    'embedding_dim': 6,\n",
        "    'vocab_size': sentence_piece_encoder.vocab_size,\n",
        "    'max_len': 8,\n",
        "    'trainable_pos_embedding': False,\n",
        "    'num_heads': 2,\n",
        "    'num_layers': 3,\n",
        "    'd_hid': 12,\n",
        "    'use_attn_mask': True\n",
        "}\n",
        "sequence_encoder = create_transformer(**sequence_encoder_config)\n",
        "\n",
        "import keras as keras\n",
        "\n",
        "assert type(sequence_encoder) == keras.Model"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0704 11:22:49.480923 140107069044608 deprecation_wrapper.py:118] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0704 11:22:49.719778 140107069044608 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1m_NGzIF_eS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "03c9a940-a51b-42e4-eb51-745d2b55699a"
      },
      "source": [
        "# now look at the inputs:\n",
        "print(sequence_encoder.inputs)  # tokens, segment_ids, pos_ids, attn_mask"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[<tf.Tensor 'token_input:0' shape=(?, 8) dtype=int32>, <tf.Tensor 'segment_input:0' shape=(?, 8) dtype=int32>, <tf.Tensor 'position_input:0' shape=(?, 8) dtype=int32>, <tf.Tensor 'attention_mask_input:0' shape=(?, 1, 8, 8) dtype=float32>]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rHhZ5VczGDXL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tokens is a batch_size * seq_len tensor containing token_ids\n",
        "# segment_ids is a batch_size * seq_len tensor containing segment_ids (as in segment_{a, b} of BERT)\n",
        "# pos_ids is a batch_size * seq_len tensor containing position ids (0..max_len)(you will see how can easily generate it)\n",
        "# attn_mask is a batch_size * 1 * max_len * max_len tensor and can encode padding and causality constraints (ignore it for now)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YNcDG1IOGFaS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ef1b2aad-8ae8-471d-a18f-cd302a6b8bb2"
      },
      "source": [
        "# for outputs we have:\n",
        "print(sequence_encoder.outputs)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[<tf.Tensor 'layer_2/ln_2/add_1:0' shape=(?, 8, 6) dtype=float32>]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjoRhUh2GG_C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 'a long name' is a batch_size * max_len * embedding_dim tensor which is our encoded sequence (here with a transformer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmEGR1RBGJO6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# now it's time to train it both on pre-training tasks and fine-tuning tasks\n",
        "# first we need to define our tasks:"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5TPjU7APGLQ5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from data.dataset import TaskMetadata, TaskWeightScheduler\n",
        "\n",
        "tasks = [TaskMetadata('lm', is_token_level=True,\n",
        "                      num_classes=sentence_piece_encoder.vocab_size + sentence_piece_encoder.SPECIAL_COUNT,\n",
        "                      dropout=0,\n",
        "                      weight_scheduler=TaskWeightScheduler(active_in_pretrain=True, active_in_finetune=False,\n",
        "                                                           pretrain_value=1.0))]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D14OHODJGM9s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# well let's pause and see what this task is, 'lm' is the name of the task\n",
        "# and 'lm' is also a special task, because it uses a tied decoder (if you don't know what it means, ignore it)\n",
        "# then num_classes is set to vocab+special_count which is actually incorrect (we are never going to predict mask, pad, )\n",
        "# but it's here for the tied decoder to work; dropout is for the decoder of this task\n",
        "# and finally a weight_scheduler, in this example we are only training on 'lm' task during the pretraing but not after\n",
        "# now let's add a more complex task, a sentence level one with a complex weight_scheduler"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71D19P_3GO4S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ComplexTaskWeightScheduler(TaskWeightScheduler):  # note: this is an example, it is not a clean code\n",
        "    def __init__(self, number_of_pretrain_steps, number_of_finetune_steps):\n",
        "        super().__init__(active_in_pretrain=True, active_in_finetune=True)\n",
        "        self.number_of_pretrain_steps = number_of_pretrain_steps\n",
        "        self.number_of_finetune_steps = number_of_finetune_steps\n",
        "\n",
        "    def get(self, is_pretrain: bool, step: int) -> float:\n",
        "        return step / (self.number_of_pretrain_steps if is_pretrain else self.number_of_finetune_steps)\n",
        "\n",
        "\n",
        "number_of_pretrain_steps = 100\n",
        "number_of_finetune_steps = 100\n",
        "# in this task we are going to count the number of tokens in a sentence and predict if it's odd or not\n",
        "tasks.append(TaskMetadata('odd', is_token_level=False, num_classes=2, dropout=0.3,\n",
        "                          weight_scheduler=ComplexTaskWeightScheduler(number_of_pretrain_steps,\n",
        "                                                                      number_of_finetune_steps)))\n",
        "\n",
        "# and let's add a unsolvable task for fun\n",
        "tasks.append(TaskMetadata('lm_random', is_token_level=True,\n",
        "                          num_classes=sentence_piece_encoder.vocab_size + sentence_piece_encoder.SPECIAL_COUNT,\n",
        "                          dropout=0.3,\n",
        "                          weight_scheduler=TaskWeightScheduler(active_in_pretrain=True, active_in_finetune=True,\n",
        "                                                               pretrain_value=0.5)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPI_SDNeGRXZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# now we need a data generator, for a good reference see data.lm_dataset._get_lm_generator_single or _double\n",
        "# but for now we are going to write a simple one so you understand the Sentence class\n",
        "# again this is a simple generator just showing you the core ideas\n",
        "# so for 'lm' task we are just going to predict the token itself (identity function)\n",
        "# first we are importing things, ignore them for now, I will explain them in a bit"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Th0CnybYGT3v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from data.dataset import Sentence, TokenTaskData, SentenceTaskData, TextEncoder\n",
        "from data.lm_dataset import _create_batch\n",
        "import random\n",
        "\n",
        "\n",
        "def tutorial_batch_generator(vocab_size: int, max_len: int, batch_size: int, steps: int):\n",
        "    def sentence_generator():\n",
        "        for _ in range(steps):\n",
        "            # for each sentence we are going to generate up to max_len tokens\n",
        "            seq_len = random.randint(1, max_len - 1)\n",
        "            # and this is their ids (in reality we have to use our TextEncoder instance here)\n",
        "            tokens = [random.randrange(vocab_size) for _ in range(seq_len)]\n",
        "            # we manually set the last token to EOS (which we will see how it's calculated)\n",
        "            tokens[-1] = eos_id\n",
        "            yield Sentence(\n",
        "                tokens=tokens,\n",
        "                padding_mask=[True] * seq_len,  # it means that non of the original tokens are padding\n",
        "                segments=[0] * seq_len,  # for this simple example we are going to use segment_a(0) for all of them\n",
        "                token_classification={  # we put labels here (for token level tasks)\n",
        "                    # name_of_the_task: TokenTaskData(target(aka label), label_mask)\n",
        "                    # there might be situations that you are only interested in predictions for certain tokens,\n",
        "                    # you can use mask in those situations (see the bert paper to understand this)\n",
        "                    'lm': TokenTaskData(tokens, [True] * seq_len),\n",
        "                    # this task is unsolvable so we will see the loss not decreasing\n",
        "                    'lm_random': TokenTaskData([random.randrange(vocab_size) for i in range(seq_len)],\n",
        "                                               [True] * seq_len)\n",
        "                },\n",
        "                # similar to token_classification, it's also a dictionary of task to label\n",
        "                # SentenceTaskData contains (label, where to extract that label_from)\n",
        "                # in this case we are going to predict whether a sentence has\n",
        "                # odd number of tokens or not whenever we see eos token\n",
        "                sentence_classification={'odd': SentenceTaskData(seq_len % 2, seq_len - 1)}\n",
        "            )\n",
        "\n",
        "    # we need eos_id and it's always at this place\n",
        "    eos_id = vocab_size + TextEncoder.EOS_OFFSET\n",
        "    # likewise for pad_id\n",
        "    pad_id = vocab_size + TextEncoder.PAD_OFFSET\n",
        "    generator = sentence_generator()\n",
        "    batch = []\n",
        "    for item in generator:\n",
        "        batch.append(item)\n",
        "        if len(batch) == batch_size:\n",
        "            batch = _create_batch(batch, pad_id, max_len)  # magic to pad and batch sentences\n",
        "            # at the end it will generate a SentenceBatch which is more than just a list of Sentence\n",
        "            yield batch\n",
        "            batch = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WjqYgas3GWfg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# now we instantiate our generator\n",
        "# we are going to set steps to a large number (it doesn't matter)\n",
        "# we have to set batch_size too"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHtttrcCGYw1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 8\n",
        "generator = tutorial_batch_generator(sentence_piece_encoder.vocab_size, sequence_encoder_config['max_len'],\n",
        "                                     batch_size, steps=10000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKj42qx_GbGl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# now let the fun begin :D"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dYYCx5nGcl_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformer.train import train_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNIp8t7zsxpU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# @title Get TPUStrategy  here\n",
        "if use_tpu:\n",
        "    assert 'COLAB_TPU_ADDR' in os.environ, 'ERROR: Not connected to a TPU runtime; Maybe you should switch hardware accelerator to TPU for TPU support'\n",
        "    import tensorflow as tf\n",
        "    tpu_address = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "    strategy = tf.contrib.tpu.TPUDistributionStrategy(\n",
        "        tf.contrib.cluster_resolver.TPUClusterResolver(tpu=tpu_address)\n",
        "    )\n",
        "else:\n",
        "    strategy = None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YUYvd56CGdy6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# we are going to use the same generator for both pretrain and finetune"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07z6vttVGfyX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "41bf07e2-7d71-447a-8d4c-7c4b196ac335"
      },
      "source": [
        "m = train_model(base_model=sequence_encoder, is_causal=False, tasks_meta_data=tasks, pretrain_generator=generator,\n",
        "                finetune_generator=generator, pretrain_epochs=100, pretrain_steps=number_of_pretrain_steps // 100,\n",
        "                finetune_epochs=100, finetune_steps=number_of_finetune_steps // 100, verbose=2, TPUStrategy=strategy)\n",
        "# now m is ready to be used!\n",
        "print(m.inputs)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0704 11:22:50.738151 140107069044608 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3080: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0704 11:22:50.935418 140107069044608 deprecation_wrapper.py:118] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            " - 2s - loss: 4.9964 - lm_loss_loss: 3.2446 - odd_loss_loss: 0.0000e+00 - lm_random_loss_loss: 1.7518\n",
            "Epoch 2/100\n",
            " - 0s - loss: 5.0278 - lm_loss_loss: 3.2587 - odd_loss_loss: 0.0107 - lm_random_loss_loss: 1.7584\n",
            "Epoch 3/100\n",
            " - 0s - loss: 5.1187 - lm_loss_loss: 3.2632 - odd_loss_loss: 0.0181 - lm_random_loss_loss: 1.8375\n",
            "Epoch 4/100\n",
            " - 0s - loss: 4.9907 - lm_loss_loss: 3.2347 - odd_loss_loss: 0.0235 - lm_random_loss_loss: 1.7325\n",
            "Epoch 5/100\n",
            " - 0s - loss: 5.0823 - lm_loss_loss: 3.2398 - odd_loss_loss: 0.0442 - lm_random_loss_loss: 1.7984\n",
            "Epoch 6/100\n",
            " - 0s - loss: 5.0073 - lm_loss_loss: 3.2160 - odd_loss_loss: 0.0472 - lm_random_loss_loss: 1.7441\n",
            "Epoch 7/100\n",
            " - 0s - loss: 5.0687 - lm_loss_loss: 3.2144 - odd_loss_loss: 0.0545 - lm_random_loss_loss: 1.7998\n",
            "Epoch 8/100\n",
            " - 0s - loss: 5.0282 - lm_loss_loss: 3.2049 - odd_loss_loss: 0.0512 - lm_random_loss_loss: 1.7721\n",
            "Epoch 9/100\n",
            " - 0s - loss: 5.0535 - lm_loss_loss: 3.2223 - odd_loss_loss: 0.0888 - lm_random_loss_loss: 1.7424\n",
            "Epoch 10/100\n",
            " - 0s - loss: 5.0105 - lm_loss_loss: 3.2232 - odd_loss_loss: 0.0590 - lm_random_loss_loss: 1.7284\n",
            "Epoch 11/100\n",
            " - 0s - loss: 5.0489 - lm_loss_loss: 3.2551 - odd_loss_loss: 0.0586 - lm_random_loss_loss: 1.7352\n",
            "Epoch 12/100\n",
            " - 0s - loss: 5.0586 - lm_loss_loss: 3.2332 - odd_loss_loss: 0.0740 - lm_random_loss_loss: 1.7515\n",
            "Epoch 13/100\n",
            " - 0s - loss: 4.9451 - lm_loss_loss: 3.2128 - odd_loss_loss: 0.0850 - lm_random_loss_loss: 1.6472\n",
            "Epoch 14/100\n",
            " - 0s - loss: 5.0042 - lm_loss_loss: 3.2201 - odd_loss_loss: 0.1198 - lm_random_loss_loss: 1.6642\n",
            "Epoch 15/100\n",
            " - 0s - loss: 5.0889 - lm_loss_loss: 3.2401 - odd_loss_loss: 0.1331 - lm_random_loss_loss: 1.7157\n",
            "Epoch 16/100\n",
            " - 0s - loss: 5.0139 - lm_loss_loss: 3.1970 - odd_loss_loss: 0.1068 - lm_random_loss_loss: 1.7100\n",
            "Epoch 17/100\n",
            " - 0s - loss: 5.0497 - lm_loss_loss: 3.2106 - odd_loss_loss: 0.1074 - lm_random_loss_loss: 1.7317\n",
            "Epoch 18/100\n",
            " - 0s - loss: 4.9955 - lm_loss_loss: 3.2153 - odd_loss_loss: 0.1189 - lm_random_loss_loss: 1.6613\n",
            "Epoch 19/100\n",
            " - 0s - loss: 5.1210 - lm_loss_loss: 3.2567 - odd_loss_loss: 0.1125 - lm_random_loss_loss: 1.7519\n",
            "Epoch 20/100\n",
            " - 0s - loss: 4.9788 - lm_loss_loss: 3.2041 - odd_loss_loss: 0.1258 - lm_random_loss_loss: 1.6489\n",
            "Epoch 21/100\n",
            " - 0s - loss: 5.1800 - lm_loss_loss: 3.2105 - odd_loss_loss: 0.1625 - lm_random_loss_loss: 1.8071\n",
            "Epoch 22/100\n",
            " - 0s - loss: 5.1251 - lm_loss_loss: 3.2302 - odd_loss_loss: 0.1470 - lm_random_loss_loss: 1.7480\n",
            "Epoch 23/100\n",
            " - 0s - loss: 5.0598 - lm_loss_loss: 3.2185 - odd_loss_loss: 0.1758 - lm_random_loss_loss: 1.6655\n",
            "Epoch 24/100\n",
            " - 0s - loss: 5.0971 - lm_loss_loss: 3.2130 - odd_loss_loss: 0.1653 - lm_random_loss_loss: 1.7187\n",
            "Epoch 25/100\n",
            " - 0s - loss: 5.1389 - lm_loss_loss: 3.2236 - odd_loss_loss: 0.2323 - lm_random_loss_loss: 1.6831\n",
            "Epoch 26/100\n",
            " - 0s - loss: 5.0679 - lm_loss_loss: 3.1928 - odd_loss_loss: 0.1634 - lm_random_loss_loss: 1.7117\n",
            "Epoch 27/100\n",
            " - 0s - loss: 5.0499 - lm_loss_loss: 3.1880 - odd_loss_loss: 0.1373 - lm_random_loss_loss: 1.7247\n",
            "Epoch 28/100\n",
            " - 0s - loss: 4.9529 - lm_loss_loss: 3.2162 - odd_loss_loss: 0.1219 - lm_random_loss_loss: 1.6149\n",
            "Epoch 29/100\n",
            " - 0s - loss: 5.0830 - lm_loss_loss: 3.1792 - odd_loss_loss: 0.1986 - lm_random_loss_loss: 1.7052\n",
            "Epoch 30/100\n",
            " - 0s - loss: 5.0697 - lm_loss_loss: 3.2123 - odd_loss_loss: 0.1837 - lm_random_loss_loss: 1.6737\n",
            "Epoch 31/100\n",
            " - 0s - loss: 5.1966 - lm_loss_loss: 3.2059 - odd_loss_loss: 0.2071 - lm_random_loss_loss: 1.7836\n",
            "Epoch 32/100\n",
            " - 0s - loss: 5.0759 - lm_loss_loss: 3.1944 - odd_loss_loss: 0.2242 - lm_random_loss_loss: 1.6573\n",
            "Epoch 33/100\n",
            " - 0s - loss: 5.2502 - lm_loss_loss: 3.1935 - odd_loss_loss: 0.2047 - lm_random_loss_loss: 1.8520\n",
            "Epoch 34/100\n",
            " - 0s - loss: 5.1321 - lm_loss_loss: 3.2203 - odd_loss_loss: 0.2019 - lm_random_loss_loss: 1.7099\n",
            "Epoch 35/100\n",
            " - 0s - loss: 5.2161 - lm_loss_loss: 3.1910 - odd_loss_loss: 0.3076 - lm_random_loss_loss: 1.7175\n",
            "Epoch 36/100\n",
            " - 0s - loss: 5.0615 - lm_loss_loss: 3.1924 - odd_loss_loss: 0.2192 - lm_random_loss_loss: 1.6499\n",
            "Epoch 37/100\n",
            " - 0s - loss: 5.2129 - lm_loss_loss: 3.1945 - odd_loss_loss: 0.2374 - lm_random_loss_loss: 1.7810\n",
            "Epoch 38/100\n",
            " - 0s - loss: 5.0910 - lm_loss_loss: 3.2053 - odd_loss_loss: 0.2464 - lm_random_loss_loss: 1.6393\n",
            "Epoch 39/100\n",
            " - 0s - loss: 5.3073 - lm_loss_loss: 3.1923 - odd_loss_loss: 0.3192 - lm_random_loss_loss: 1.7958\n",
            "Epoch 40/100\n",
            " - 0s - loss: 5.1444 - lm_loss_loss: 3.1997 - odd_loss_loss: 0.2320 - lm_random_loss_loss: 1.7127\n",
            "Epoch 41/100\n",
            " - 0s - loss: 5.1192 - lm_loss_loss: 3.1877 - odd_loss_loss: 0.2400 - lm_random_loss_loss: 1.6915\n",
            "Epoch 42/100\n",
            " - 0s - loss: 5.1763 - lm_loss_loss: 3.1803 - odd_loss_loss: 0.2471 - lm_random_loss_loss: 1.7489\n",
            "Epoch 43/100\n",
            " - 0s - loss: 5.1707 - lm_loss_loss: 3.1718 - odd_loss_loss: 0.2485 - lm_random_loss_loss: 1.7504\n",
            "Epoch 44/100\n",
            " - 0s - loss: 5.1319 - lm_loss_loss: 3.1887 - odd_loss_loss: 0.2428 - lm_random_loss_loss: 1.7005\n",
            "Epoch 45/100\n",
            " - 0s - loss: 5.0579 - lm_loss_loss: 3.1968 - odd_loss_loss: 0.2210 - lm_random_loss_loss: 1.6401\n",
            "Epoch 46/100\n",
            " - 0s - loss: 5.1234 - lm_loss_loss: 3.1857 - odd_loss_loss: 0.3342 - lm_random_loss_loss: 1.6035\n",
            "Epoch 47/100\n",
            " - 0s - loss: 5.2288 - lm_loss_loss: 3.1813 - odd_loss_loss: 0.3319 - lm_random_loss_loss: 1.7156\n",
            "Epoch 48/100\n",
            " - 0s - loss: 5.2161 - lm_loss_loss: 3.1752 - odd_loss_loss: 0.3420 - lm_random_loss_loss: 1.6989\n",
            "Epoch 49/100\n",
            " - 0s - loss: 5.1365 - lm_loss_loss: 3.1840 - odd_loss_loss: 0.3036 - lm_random_loss_loss: 1.6489\n",
            "Epoch 50/100\n",
            " - 0s - loss: 5.2684 - lm_loss_loss: 3.1872 - odd_loss_loss: 0.2948 - lm_random_loss_loss: 1.7864\n",
            "Epoch 51/100\n",
            " - 0s - loss: 5.1665 - lm_loss_loss: 3.1648 - odd_loss_loss: 0.3128 - lm_random_loss_loss: 1.6890\n",
            "Epoch 52/100\n",
            " - 0s - loss: 5.2324 - lm_loss_loss: 3.1664 - odd_loss_loss: 0.3998 - lm_random_loss_loss: 1.6662\n",
            "Epoch 53/100\n",
            " - 0s - loss: 5.1886 - lm_loss_loss: 3.1801 - odd_loss_loss: 0.3882 - lm_random_loss_loss: 1.6203\n",
            "Epoch 54/100\n",
            " - 0s - loss: 5.0868 - lm_loss_loss: 3.1748 - odd_loss_loss: 0.3282 - lm_random_loss_loss: 1.5839\n",
            "Epoch 55/100\n",
            " - 0s - loss: 5.1416 - lm_loss_loss: 3.1721 - odd_loss_loss: 0.3535 - lm_random_loss_loss: 1.6160\n",
            "Epoch 56/100\n",
            " - 0s - loss: 5.2697 - lm_loss_loss: 3.1813 - odd_loss_loss: 0.4075 - lm_random_loss_loss: 1.6809\n",
            "Epoch 57/100\n",
            " - 0s - loss: 5.2871 - lm_loss_loss: 3.1699 - odd_loss_loss: 0.4497 - lm_random_loss_loss: 1.6675\n",
            "Epoch 58/100\n",
            " - 0s - loss: 5.1430 - lm_loss_loss: 3.1763 - odd_loss_loss: 0.3452 - lm_random_loss_loss: 1.6214\n",
            "Epoch 59/100\n",
            " - 0s - loss: 5.1520 - lm_loss_loss: 3.1662 - odd_loss_loss: 0.2834 - lm_random_loss_loss: 1.7024\n",
            "Epoch 60/100\n",
            " - 0s - loss: 5.3205 - lm_loss_loss: 3.2079 - odd_loss_loss: 0.4326 - lm_random_loss_loss: 1.6800\n",
            "Epoch 61/100\n",
            " - 0s - loss: 5.2868 - lm_loss_loss: 3.1453 - odd_loss_loss: 0.3883 - lm_random_loss_loss: 1.7532\n",
            "Epoch 62/100\n",
            " - 0s - loss: 5.3020 - lm_loss_loss: 3.1805 - odd_loss_loss: 0.5192 - lm_random_loss_loss: 1.6023\n",
            "Epoch 63/100\n",
            " - 0s - loss: 5.3176 - lm_loss_loss: 3.1761 - odd_loss_loss: 0.4541 - lm_random_loss_loss: 1.6874\n",
            "Epoch 64/100\n",
            " - 0s - loss: 5.3397 - lm_loss_loss: 3.1516 - odd_loss_loss: 0.4713 - lm_random_loss_loss: 1.7168\n",
            "Epoch 65/100\n",
            " - 0s - loss: 5.3711 - lm_loss_loss: 3.1801 - odd_loss_loss: 0.4181 - lm_random_loss_loss: 1.7729\n",
            "Epoch 66/100\n",
            " - 0s - loss: 5.2709 - lm_loss_loss: 3.1799 - odd_loss_loss: 0.3867 - lm_random_loss_loss: 1.7043\n",
            "Epoch 67/100\n",
            " - 0s - loss: 5.3652 - lm_loss_loss: 3.1528 - odd_loss_loss: 0.5681 - lm_random_loss_loss: 1.6442\n",
            "Epoch 68/100\n",
            " - 0s - loss: 5.2933 - lm_loss_loss: 3.1731 - odd_loss_loss: 0.5498 - lm_random_loss_loss: 1.5705\n",
            "Epoch 69/100\n",
            " - 0s - loss: 5.3170 - lm_loss_loss: 3.1887 - odd_loss_loss: 0.3795 - lm_random_loss_loss: 1.7487\n",
            "Epoch 70/100\n",
            " - 0s - loss: 5.3438 - lm_loss_loss: 3.1515 - odd_loss_loss: 0.4453 - lm_random_loss_loss: 1.7470\n",
            "Epoch 71/100\n",
            " - 0s - loss: 5.2793 - lm_loss_loss: 3.1670 - odd_loss_loss: 0.4301 - lm_random_loss_loss: 1.6822\n",
            "Epoch 72/100\n",
            " - 0s - loss: 5.4211 - lm_loss_loss: 3.1705 - odd_loss_loss: 0.6085 - lm_random_loss_loss: 1.6421\n",
            "Epoch 73/100\n",
            " - 0s - loss: 5.3952 - lm_loss_loss: 3.1833 - odd_loss_loss: 0.5916 - lm_random_loss_loss: 1.6203\n",
            "Epoch 74/100\n",
            " - 0s - loss: 5.4203 - lm_loss_loss: 3.1601 - odd_loss_loss: 0.6114 - lm_random_loss_loss: 1.6488\n",
            "Epoch 75/100\n",
            " - 0s - loss: 5.2410 - lm_loss_loss: 3.1586 - odd_loss_loss: 0.4643 - lm_random_loss_loss: 1.6181\n",
            "Epoch 76/100\n",
            " - 0s - loss: 5.2714 - lm_loss_loss: 3.1694 - odd_loss_loss: 0.4633 - lm_random_loss_loss: 1.6387\n",
            "Epoch 77/100\n",
            " - 0s - loss: 5.2491 - lm_loss_loss: 3.1611 - odd_loss_loss: 0.5122 - lm_random_loss_loss: 1.5758\n",
            "Epoch 78/100\n",
            " - 0s - loss: 5.3681 - lm_loss_loss: 3.1645 - odd_loss_loss: 0.5379 - lm_random_loss_loss: 1.6657\n",
            "Epoch 79/100\n",
            " - 0s - loss: 5.3048 - lm_loss_loss: 3.1441 - odd_loss_loss: 0.5208 - lm_random_loss_loss: 1.6399\n",
            "Epoch 80/100\n",
            " - 0s - loss: 5.3158 - lm_loss_loss: 3.1655 - odd_loss_loss: 0.4952 - lm_random_loss_loss: 1.6550\n",
            "Epoch 81/100\n",
            " - 0s - loss: 5.4257 - lm_loss_loss: 3.1609 - odd_loss_loss: 0.6065 - lm_random_loss_loss: 1.6583\n",
            "Epoch 82/100\n",
            " - 0s - loss: 5.4403 - lm_loss_loss: 3.1887 - odd_loss_loss: 0.4889 - lm_random_loss_loss: 1.7627\n",
            "Epoch 83/100\n",
            " - 0s - loss: 5.2712 - lm_loss_loss: 3.1299 - odd_loss_loss: 0.4584 - lm_random_loss_loss: 1.6830\n",
            "Epoch 84/100\n",
            " - 0s - loss: 5.3532 - lm_loss_loss: 3.1672 - odd_loss_loss: 0.5043 - lm_random_loss_loss: 1.6817\n",
            "Epoch 85/100\n",
            " - 0s - loss: 5.2347 - lm_loss_loss: 3.1455 - odd_loss_loss: 0.4927 - lm_random_loss_loss: 1.5965\n",
            "Epoch 86/100\n",
            " - 0s - loss: 5.3917 - lm_loss_loss: 3.1571 - odd_loss_loss: 0.4866 - lm_random_loss_loss: 1.7479\n",
            "Epoch 87/100\n",
            " - 0s - loss: 5.3232 - lm_loss_loss: 3.1739 - odd_loss_loss: 0.5135 - lm_random_loss_loss: 1.6358\n",
            "Epoch 88/100\n",
            " - 0s - loss: 5.3812 - lm_loss_loss: 3.1613 - odd_loss_loss: 0.5322 - lm_random_loss_loss: 1.6878\n",
            "Epoch 89/100\n",
            " - 0s - loss: 5.3031 - lm_loss_loss: 3.1539 - odd_loss_loss: 0.6039 - lm_random_loss_loss: 1.5453\n",
            "Epoch 90/100\n",
            " - 0s - loss: 5.4694 - lm_loss_loss: 3.1382 - odd_loss_loss: 0.6298 - lm_random_loss_loss: 1.7014\n",
            "Epoch 91/100\n",
            " - 0s - loss: 5.4888 - lm_loss_loss: 3.1585 - odd_loss_loss: 0.6614 - lm_random_loss_loss: 1.6688\n",
            "Epoch 92/100\n",
            " - 0s - loss: 5.4296 - lm_loss_loss: 3.1568 - odd_loss_loss: 0.6186 - lm_random_loss_loss: 1.6542\n",
            "Epoch 93/100\n",
            " - 0s - loss: 5.5290 - lm_loss_loss: 3.1590 - odd_loss_loss: 0.6478 - lm_random_loss_loss: 1.7223\n",
            "Epoch 94/100\n",
            " - 0s - loss: 5.3753 - lm_loss_loss: 3.1543 - odd_loss_loss: 0.5475 - lm_random_loss_loss: 1.6735\n",
            "Epoch 95/100\n",
            " - 0s - loss: 5.4968 - lm_loss_loss: 3.1355 - odd_loss_loss: 0.7249 - lm_random_loss_loss: 1.6364\n",
            "Epoch 96/100\n",
            " - 0s - loss: 5.4683 - lm_loss_loss: 3.1438 - odd_loss_loss: 0.6652 - lm_random_loss_loss: 1.6593\n",
            "Epoch 97/100\n",
            " - 0s - loss: 5.2554 - lm_loss_loss: 3.1354 - odd_loss_loss: 0.4950 - lm_random_loss_loss: 1.6250\n",
            "Epoch 98/100\n",
            " - 0s - loss: 5.3095 - lm_loss_loss: 3.1609 - odd_loss_loss: 0.5233 - lm_random_loss_loss: 1.6254\n",
            "Epoch 99/100\n",
            " - 0s - loss: 5.3409 - lm_loss_loss: 3.1649 - odd_loss_loss: 0.5906 - lm_random_loss_loss: 1.5854\n",
            "Epoch 100/100\n",
            " - 0s - loss: 5.4446 - lm_loss_loss: 3.1681 - odd_loss_loss: 0.6084 - lm_random_loss_loss: 1.6681\n",
            "Epoch 1/100\n",
            " - 3s - loss: 3.1119 - odd_loss_loss: 0.0000e+00 - lm_random_loss_loss: 3.1119\n",
            "Epoch 2/100\n",
            " - 0s - loss: 3.3226 - odd_loss_loss: 0.0082 - lm_random_loss_loss: 3.3145\n",
            "Epoch 3/100\n",
            " - 0s - loss: 3.2810 - odd_loss_loss: 0.0118 - lm_random_loss_loss: 3.2691\n",
            "Epoch 4/100\n",
            " - 0s - loss: 3.1879 - odd_loss_loss: 0.0211 - lm_random_loss_loss: 3.1669\n",
            "Epoch 5/100\n",
            " - 0s - loss: 3.4329 - odd_loss_loss: 0.0210 - lm_random_loss_loss: 3.4120\n",
            "Epoch 6/100\n",
            " - 0s - loss: 3.2723 - odd_loss_loss: 0.0307 - lm_random_loss_loss: 3.2416\n",
            "Epoch 7/100\n",
            " - 0s - loss: 3.3572 - odd_loss_loss: 0.0447 - lm_random_loss_loss: 3.3125\n",
            "Epoch 8/100\n",
            " - 0s - loss: 3.3305 - odd_loss_loss: 0.0452 - lm_random_loss_loss: 3.2853\n",
            "Epoch 9/100\n",
            " - 0s - loss: 3.3266 - odd_loss_loss: 0.0603 - lm_random_loss_loss: 3.2663\n",
            "Epoch 10/100\n",
            " - 0s - loss: 3.2619 - odd_loss_loss: 0.0607 - lm_random_loss_loss: 3.2012\n",
            "Epoch 11/100\n",
            " - 0s - loss: 3.3144 - odd_loss_loss: 0.0592 - lm_random_loss_loss: 3.2552\n",
            "Epoch 12/100\n",
            " - 0s - loss: 3.4592 - odd_loss_loss: 0.0737 - lm_random_loss_loss: 3.3855\n",
            "Epoch 13/100\n",
            " - 0s - loss: 3.6559 - odd_loss_loss: 0.0876 - lm_random_loss_loss: 3.5683\n",
            "Epoch 14/100\n",
            " - 0s - loss: 3.4114 - odd_loss_loss: 0.0804 - lm_random_loss_loss: 3.3310\n",
            "Epoch 15/100\n",
            " - 0s - loss: 3.4480 - odd_loss_loss: 0.0880 - lm_random_loss_loss: 3.3600\n",
            "Epoch 16/100\n",
            " - 0s - loss: 3.2057 - odd_loss_loss: 0.0914 - lm_random_loss_loss: 3.1144\n",
            "Epoch 17/100\n",
            " - 0s - loss: 3.4521 - odd_loss_loss: 0.1212 - lm_random_loss_loss: 3.3310\n",
            "Epoch 18/100\n",
            " - 0s - loss: 3.3607 - odd_loss_loss: 0.1049 - lm_random_loss_loss: 3.2558\n",
            "Epoch 19/100\n",
            " - 0s - loss: 3.3087 - odd_loss_loss: 0.0956 - lm_random_loss_loss: 3.2131\n",
            "Epoch 20/100\n",
            " - 0s - loss: 3.2466 - odd_loss_loss: 0.1369 - lm_random_loss_loss: 3.1097\n",
            "Epoch 21/100\n",
            " - 0s - loss: 3.2739 - odd_loss_loss: 0.1187 - lm_random_loss_loss: 3.1552\n",
            "Epoch 22/100\n",
            " - 0s - loss: 3.2824 - odd_loss_loss: 0.1257 - lm_random_loss_loss: 3.1567\n",
            "Epoch 23/100\n",
            " - 0s - loss: 3.5715 - odd_loss_loss: 0.1782 - lm_random_loss_loss: 3.3933\n",
            "Epoch 24/100\n",
            " - 0s - loss: 3.4018 - odd_loss_loss: 0.1707 - lm_random_loss_loss: 3.2311\n",
            "Epoch 25/100\n",
            " - 0s - loss: 3.4893 - odd_loss_loss: 0.1783 - lm_random_loss_loss: 3.3110\n",
            "Epoch 26/100\n",
            " - 0s - loss: 3.2851 - odd_loss_loss: 0.1701 - lm_random_loss_loss: 3.1150\n",
            "Epoch 27/100\n",
            " - 0s - loss: 3.4032 - odd_loss_loss: 0.1499 - lm_random_loss_loss: 3.2533\n",
            "Epoch 28/100\n",
            " - 0s - loss: 3.4669 - odd_loss_loss: 0.1813 - lm_random_loss_loss: 3.2856\n",
            "Epoch 29/100\n",
            " - 0s - loss: 3.3760 - odd_loss_loss: 0.1821 - lm_random_loss_loss: 3.1940\n",
            "Epoch 30/100\n",
            " - 0s - loss: 3.5471 - odd_loss_loss: 0.1961 - lm_random_loss_loss: 3.3510\n",
            "Epoch 31/100\n",
            " - 0s - loss: 3.4230 - odd_loss_loss: 0.2209 - lm_random_loss_loss: 3.2020\n",
            "Epoch 32/100\n",
            " - 0s - loss: 3.3515 - odd_loss_loss: 0.1722 - lm_random_loss_loss: 3.1792\n",
            "Epoch 33/100\n",
            " - 0s - loss: 3.5275 - odd_loss_loss: 0.2136 - lm_random_loss_loss: 3.3138\n",
            "Epoch 34/100\n",
            " - 0s - loss: 3.4951 - odd_loss_loss: 0.1920 - lm_random_loss_loss: 3.3030\n",
            "Epoch 35/100\n",
            " - 0s - loss: 3.4394 - odd_loss_loss: 0.2441 - lm_random_loss_loss: 3.1953\n",
            "Epoch 36/100\n",
            " - 0s - loss: 3.3747 - odd_loss_loss: 0.2087 - lm_random_loss_loss: 3.1660\n",
            "Epoch 37/100\n",
            " - 0s - loss: 3.4110 - odd_loss_loss: 0.2448 - lm_random_loss_loss: 3.1662\n",
            "Epoch 38/100\n",
            " - 0s - loss: 3.5363 - odd_loss_loss: 0.2585 - lm_random_loss_loss: 3.2778\n",
            "Epoch 39/100\n",
            " - 0s - loss: 3.5238 - odd_loss_loss: 0.2849 - lm_random_loss_loss: 3.2389\n",
            "Epoch 40/100\n",
            " - 0s - loss: 3.4367 - odd_loss_loss: 0.2079 - lm_random_loss_loss: 3.2288\n",
            "Epoch 41/100\n",
            " - 0s - loss: 3.6292 - odd_loss_loss: 0.2657 - lm_random_loss_loss: 3.3634\n",
            "Epoch 42/100\n",
            " - 0s - loss: 3.4964 - odd_loss_loss: 0.3296 - lm_random_loss_loss: 3.1668\n",
            "Epoch 43/100\n",
            " - 0s - loss: 3.3823 - odd_loss_loss: 0.2442 - lm_random_loss_loss: 3.1381\n",
            "Epoch 44/100\n",
            " - 0s - loss: 3.5393 - odd_loss_loss: 0.2668 - lm_random_loss_loss: 3.2726\n",
            "Epoch 45/100\n",
            " - 0s - loss: 3.6956 - odd_loss_loss: 0.2969 - lm_random_loss_loss: 3.3987\n",
            "Epoch 46/100\n",
            " - 0s - loss: 3.4307 - odd_loss_loss: 0.2770 - lm_random_loss_loss: 3.1536\n",
            "Epoch 47/100\n",
            " - 0s - loss: 3.6677 - odd_loss_loss: 0.2771 - lm_random_loss_loss: 3.3906\n",
            "Epoch 48/100\n",
            " - 0s - loss: 3.3633 - odd_loss_loss: 0.2822 - lm_random_loss_loss: 3.0811\n",
            "Epoch 49/100\n",
            " - 0s - loss: 3.6155 - odd_loss_loss: 0.3137 - lm_random_loss_loss: 3.3018\n",
            "Epoch 50/100\n",
            " - 0s - loss: 3.5753 - odd_loss_loss: 0.3255 - lm_random_loss_loss: 3.2498\n",
            "Epoch 51/100\n",
            " - 0s - loss: 3.4581 - odd_loss_loss: 0.3326 - lm_random_loss_loss: 3.1255\n",
            "Epoch 52/100\n",
            " - 0s - loss: 3.3721 - odd_loss_loss: 0.2707 - lm_random_loss_loss: 3.1013\n",
            "Epoch 53/100\n",
            " - 0s - loss: 3.6699 - odd_loss_loss: 0.4086 - lm_random_loss_loss: 3.2612\n",
            "Epoch 54/100\n",
            " - 0s - loss: 3.6658 - odd_loss_loss: 0.3578 - lm_random_loss_loss: 3.3080\n",
            "Epoch 55/100\n",
            " - 0s - loss: 3.6142 - odd_loss_loss: 0.3204 - lm_random_loss_loss: 3.2938\n",
            "Epoch 56/100\n",
            " - 0s - loss: 3.5471 - odd_loss_loss: 0.3914 - lm_random_loss_loss: 3.1557\n",
            "Epoch 57/100\n",
            " - 0s - loss: 3.7988 - odd_loss_loss: 0.4269 - lm_random_loss_loss: 3.3719\n",
            "Epoch 58/100\n",
            " - 0s - loss: 3.6820 - odd_loss_loss: 0.4186 - lm_random_loss_loss: 3.2634\n",
            "Epoch 59/100\n",
            " - 0s - loss: 3.5807 - odd_loss_loss: 0.3884 - lm_random_loss_loss: 3.1923\n",
            "Epoch 60/100\n",
            " - 0s - loss: 3.6688 - odd_loss_loss: 0.4026 - lm_random_loss_loss: 3.2662\n",
            "Epoch 61/100\n",
            " - 0s - loss: 3.5914 - odd_loss_loss: 0.4027 - lm_random_loss_loss: 3.1886\n",
            "Epoch 62/100\n",
            " - 0s - loss: 3.7599 - odd_loss_loss: 0.4055 - lm_random_loss_loss: 3.3544\n",
            "Epoch 63/100\n",
            " - 0s - loss: 3.6507 - odd_loss_loss: 0.4205 - lm_random_loss_loss: 3.2302\n",
            "Epoch 64/100\n",
            " - 0s - loss: 3.7714 - odd_loss_loss: 0.4979 - lm_random_loss_loss: 3.2735\n",
            "Epoch 65/100\n",
            " - 0s - loss: 3.6566 - odd_loss_loss: 0.4471 - lm_random_loss_loss: 3.2095\n",
            "Epoch 66/100\n",
            " - 0s - loss: 3.6330 - odd_loss_loss: 0.4079 - lm_random_loss_loss: 3.2252\n",
            "Epoch 67/100\n",
            " - 0s - loss: 3.6756 - odd_loss_loss: 0.4974 - lm_random_loss_loss: 3.1782\n",
            "Epoch 68/100\n",
            " - 0s - loss: 3.7284 - odd_loss_loss: 0.5210 - lm_random_loss_loss: 3.2074\n",
            "Epoch 69/100\n",
            " - 0s - loss: 3.6204 - odd_loss_loss: 0.3737 - lm_random_loss_loss: 3.2467\n",
            "Epoch 70/100\n",
            " - 0s - loss: 3.7615 - odd_loss_loss: 0.4387 - lm_random_loss_loss: 3.3227\n",
            "Epoch 71/100\n",
            " - 0s - loss: 3.6069 - odd_loss_loss: 0.3910 - lm_random_loss_loss: 3.2159\n",
            "Epoch 72/100\n",
            " - 0s - loss: 3.6834 - odd_loss_loss: 0.4468 - lm_random_loss_loss: 3.2366\n",
            "Epoch 73/100\n",
            " - 0s - loss: 3.6608 - odd_loss_loss: 0.5528 - lm_random_loss_loss: 3.1080\n",
            "Epoch 74/100\n",
            " - 0s - loss: 3.6394 - odd_loss_loss: 0.4424 - lm_random_loss_loss: 3.1970\n",
            "Epoch 75/100\n",
            " - 0s - loss: 3.7359 - odd_loss_loss: 0.3944 - lm_random_loss_loss: 3.3414\n",
            "Epoch 76/100\n",
            " - 0s - loss: 3.6556 - odd_loss_loss: 0.4522 - lm_random_loss_loss: 3.2034\n",
            "Epoch 77/100\n",
            " - 0s - loss: 3.6359 - odd_loss_loss: 0.4729 - lm_random_loss_loss: 3.1631\n",
            "Epoch 78/100\n",
            " - 0s - loss: 3.6721 - odd_loss_loss: 0.5059 - lm_random_loss_loss: 3.1662\n",
            "Epoch 79/100\n",
            " - 0s - loss: 3.7799 - odd_loss_loss: 0.5745 - lm_random_loss_loss: 3.2055\n",
            "Epoch 80/100\n",
            " - 0s - loss: 3.7535 - odd_loss_loss: 0.5055 - lm_random_loss_loss: 3.2480\n",
            "Epoch 81/100\n",
            " - 0s - loss: 3.7581 - odd_loss_loss: 0.4845 - lm_random_loss_loss: 3.2736\n",
            "Epoch 82/100\n",
            " - 0s - loss: 3.6983 - odd_loss_loss: 0.4733 - lm_random_loss_loss: 3.2250\n",
            "Epoch 83/100\n",
            " - 0s - loss: 3.6631 - odd_loss_loss: 0.5097 - lm_random_loss_loss: 3.1534\n",
            "Epoch 84/100\n",
            " - 0s - loss: 3.5713 - odd_loss_loss: 0.4712 - lm_random_loss_loss: 3.1001\n",
            "Epoch 85/100\n",
            " - 0s - loss: 3.7838 - odd_loss_loss: 0.5801 - lm_random_loss_loss: 3.2037\n",
            "Epoch 86/100\n",
            " - 0s - loss: 3.7368 - odd_loss_loss: 0.5031 - lm_random_loss_loss: 3.2337\n",
            "Epoch 87/100\n",
            " - 0s - loss: 3.6358 - odd_loss_loss: 0.6342 - lm_random_loss_loss: 3.0016\n",
            "Epoch 88/100\n",
            " - 0s - loss: 3.7053 - odd_loss_loss: 0.6076 - lm_random_loss_loss: 3.0977\n",
            "Epoch 89/100\n",
            " - 0s - loss: 3.5969 - odd_loss_loss: 0.4420 - lm_random_loss_loss: 3.1549\n",
            "Epoch 90/100\n",
            " - 0s - loss: 3.6219 - odd_loss_loss: 0.4979 - lm_random_loss_loss: 3.1240\n",
            "Epoch 91/100\n",
            " - 0s - loss: 3.8644 - odd_loss_loss: 0.6260 - lm_random_loss_loss: 3.2384\n",
            "Epoch 92/100\n",
            " - 0s - loss: 3.7920 - odd_loss_loss: 0.6434 - lm_random_loss_loss: 3.1487\n",
            "Epoch 93/100\n",
            " - 0s - loss: 3.6991 - odd_loss_loss: 0.5119 - lm_random_loss_loss: 3.1872\n",
            "Epoch 94/100\n",
            " - 0s - loss: 3.6969 - odd_loss_loss: 0.5498 - lm_random_loss_loss: 3.1471\n",
            "Epoch 95/100\n",
            " - 0s - loss: 3.6131 - odd_loss_loss: 0.5613 - lm_random_loss_loss: 3.0517\n",
            "Epoch 96/100\n",
            " - 0s - loss: 3.7432 - odd_loss_loss: 0.6142 - lm_random_loss_loss: 3.1289\n",
            "Epoch 97/100\n",
            " - 0s - loss: 3.6017 - odd_loss_loss: 0.6509 - lm_random_loss_loss: 2.9508\n",
            "Epoch 98/100\n",
            " - 0s - loss: 3.8778 - odd_loss_loss: 0.6940 - lm_random_loss_loss: 3.1837\n",
            "Epoch 99/100\n",
            " - 0s - loss: 3.9173 - odd_loss_loss: 0.6691 - lm_random_loss_loss: 3.2482\n",
            "Epoch 100/100\n",
            " - 0s - loss: 3.8870 - odd_loss_loss: 0.6506 - lm_random_loss_loss: 3.2364\n",
            "[<tf.Tensor 'token_input:0' shape=(?, 8) dtype=int32>, <tf.Tensor 'segment_input:0' shape=(?, 8) dtype=int32>, <tf.Tensor 'position_input:0' shape=(?, 8) dtype=int32>, <tf.Tensor 'attention_mask_input:0' shape=(?, 1, 8, 8) dtype=float32>, <tf.Tensor 'odd_mask_input:0' shape=(?, 1) dtype=int32>]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLHiWcd6ichC",
        "colab_type": "text"
      },
      "source": [
        "### Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBdQePIsGhxj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# token, segment, pos, att_mask, odd_mask (where to extract the class from)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dc7KaNV_GjYb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "bs = 8 # must be multiple of 8\n",
        "vs = sentence_piece_encoder.vocab_size\n",
        "sl = sequence_encoder_config['max_len']\n",
        "# generate random tokens\n",
        "token = np.random.randint(0, vs, (bs, sl))\n",
        "# generate random seg_id\n",
        "segment = np.random.randint(0, 2, (bs, sl))\n",
        "# generate pos_id\n",
        "from transformer.train import generate_pos_ids\n",
        "\n",
        "pos = generate_pos_ids(bs, sl)\n",
        "# generate attn_mask\n",
        "from data.dataset import create_attention_mask\n",
        "\n",
        "# first generate a padding_mask(1 means it is not padded)\n",
        "pad_mask = np.random.randint(0, 2, (bs, sl)).astype(np.int8)\n",
        "# create the mask\n",
        "mask = create_attention_mask(pad_mask=pad_mask, is_causal=False)\n",
        "# generate target index\n",
        "target_index = np.random.randint(0, sl, (bs, 1))\n",
        "res = m.predict([token, segment, pos, mask, target_index], verbose=2)\n",
        "assert res[0].shape == (bs, sl, vs + TextEncoder.SPECIAL_COUNT)  # lm\n",
        "assert res[1].shape == (bs, 1, 2)  # odd\n",
        "assert res[2].shape == (bs, sl, vs + TextEncoder.SPECIAL_COUNT)  # random_lm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V16tO2hpTrki",
        "colab_type": "text"
      },
      "source": [
        "## Google BERT model load example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_4yp35FuZib",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "fa51c469-2d2b-4ff0-aa4f-0df8209ffaf9"
      },
      "source": [
        "# Available pretrained model checkpoints:\n",
        "#   uncased_L-12_H-768_A-12: uncased BERT base model\n",
        "#   uncased_L-24_H-1024_A-16: uncased BERT large model\n",
        "#   cased_L-12_H-768_A-12: cased BERT large model\n",
        "BERT_MODEL = 'uncased_L-12_H-768_A-12' #@param {type:\"string\"}\n",
        "download_url = 'https://storage.googleapis.com/bert_models/2018_10_18/{}.zip'.format(BERT_MODEL)\n",
        "zip_path = '{}.zip'.format(BERT_MODEL)\n",
        "! wget $download_url && unzip $zip_path\n",
        "BERT_PRETRAINED_DIR = os.path.realpath(BERT_MODEL)\n",
        "print('***** BERT pretrained directory: {} *****'.format(BERT_PRETRAINED_DIR))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-07-04 11:23:05--  https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 108.177.112.128, 2607:f8b0:4001:c12::80\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|108.177.112.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 407727028 (389M) [application/zip]\n",
            "Saving to: ‘uncased_L-12_H-768_A-12.zip.1’\n",
            "\n",
            "uncased_L-12_H-768_ 100%[===================>] 388.84M   228MB/s    in 1.7s    \n",
            "\n",
            "2019-07-04 11:23:07 (228 MB/s) - ‘uncased_L-12_H-768_A-12.zip.1’ saved [407727028/407727028]\n",
            "\n",
            "Archive:  uncased_L-12_H-768_A-12.zip\n",
            "replace uncased_L-12_H-768_A-12/bert_model.ckpt.meta? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: uncased_L-12_H-768_A-12/bert_model.ckpt.meta  \n",
            "replace uncased_L-12_H-768_A-12/bert_model.ckpt.data-00000-of-00001? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: uncased_L-12_H-768_A-12/bert_model.ckpt.data-00000-of-00001  \n",
            "replace uncased_L-12_H-768_A-12/vocab.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: uncased_L-12_H-768_A-12/vocab.txt  \n",
            "replace uncased_L-12_H-768_A-12/bert_model.ckpt.index? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: uncased_L-12_H-768_A-12/bert_model.ckpt.index  \n",
            "replace uncased_L-12_H-768_A-12/bert_config.json? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: uncased_L-12_H-768_A-12/bert_config.json  \n",
            "***** BERT pretrained directory: /content/uncased_L-12_H-768_A-12 *****\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "diq83g6subs9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "61b94656-9a6a-4b21-b95c-41e35505c720"
      },
      "source": [
        "from transformer.load import load_google_bert\n",
        "g_bert = load_google_bert(base_location=BERT_PRETRAINED_DIR+'/')\n",
        "g_bert.summary()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0704 11:25:35.638734 140107069044608 deprecation_wrapper.py:118] From bert_keras_repo/google_bert/modeling.py:92: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "segment_input (InputLayer)      (None, 512)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "position_input (InputLayer)     (None, 512)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "token_input (InputLayer)        (None, 512)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "SegmentEmbedding (Embedding)    (None, 512, 768)     1536        segment_input[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "PositionEmbedding (Embedding)   (None, 512, 768)     393216      position_input[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "TokenEmbedding (Embedding)      (None, 512, 768)     23365632    token_input[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "AddEmbeddings (Add)             (None, 512, 768)     0           SegmentEmbedding[0][0]           \n",
            "                                                                 PositionEmbedding[0][0]          \n",
            "                                                                 TokenEmbedding[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_1 (LayerNor (None, 512, 768)     1536        AddEmbeddings[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "EmbeddingDropOut (Dropout)      (None, 512, 768)     0           layer_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "layer_0/c_attn (Conv1D)         (None, 512, 2304)    1771776     EmbeddingDropOut[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "attention_mask_input (InputLaye (None, 1, 512, 512)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "layer_0/self_attention (MultiHe (None, 512, 768)     0           layer_0/c_attn[0][0]             \n",
            "                                                                 attention_mask_input[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "layer_0/c_attn_proj (Conv1D)    (None, 512, 768)     590592      layer_0/self_attention[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "layer_0/ln_1_drop (Dropout)     (None, 512, 768)     0           layer_0/c_attn_proj[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "layer_0/ln_1_add (Add)          (None, 512, 768)     0           EmbeddingDropOut[0][0]           \n",
            "                                                                 layer_0/ln_1_drop[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "layer_0/ln_1 (LayerNormalizatio (None, 512, 768)     1536        layer_0/ln_1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "layer_0/c_fc (Conv1D)           (None, 512, 3072)    2362368     layer_0/ln_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "layer_0/gelu (Gelu)             (None, 512, 3072)    0           layer_0/c_fc[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "layer_0/c_ffn_proj (Conv1D)     (None, 512, 768)     2360064     layer_0/gelu[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "layer_0/ln_2_drop (Dropout)     (None, 512, 768)     0           layer_0/c_ffn_proj[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "layer_0/ln_2_add (Add)          (None, 512, 768)     0           layer_0/ln_1[0][0]               \n",
            "                                                                 layer_0/ln_2_drop[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "layer_0/ln_2 (LayerNormalizatio (None, 512, 768)     1536        layer_0/ln_2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "layer_1/c_attn (Conv1D)         (None, 512, 2304)    1771776     layer_0/ln_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "layer_1/self_attention (MultiHe (None, 512, 768)     0           layer_1/c_attn[0][0]             \n",
            "                                                                 attention_mask_input[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "layer_1/c_attn_proj (Conv1D)    (None, 512, 768)     590592      layer_1/self_attention[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "layer_1/ln_1_drop (Dropout)     (None, 512, 768)     0           layer_1/c_attn_proj[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "layer_1/ln_1_add (Add)          (None, 512, 768)     0           layer_0/ln_2[0][0]               \n",
            "                                                                 layer_1/ln_1_drop[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "layer_1/ln_1 (LayerNormalizatio (None, 512, 768)     1536        layer_1/ln_1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "layer_1/c_fc (Conv1D)           (None, 512, 3072)    2362368     layer_1/ln_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "layer_1/gelu (Gelu)             (None, 512, 3072)    0           layer_1/c_fc[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "layer_1/c_ffn_proj (Conv1D)     (None, 512, 768)     2360064     layer_1/gelu[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "layer_1/ln_2_drop (Dropout)     (None, 512, 768)     0           layer_1/c_ffn_proj[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "layer_1/ln_2_add (Add)          (None, 512, 768)     0           layer_1/ln_1[0][0]               \n",
            "                                                                 layer_1/ln_2_drop[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "layer_1/ln_2 (LayerNormalizatio (None, 512, 768)     1536        layer_1/ln_2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "layer_2/c_attn (Conv1D)         (None, 512, 2304)    1771776     layer_1/ln_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "layer_2/self_attention (MultiHe (None, 512, 768)     0           layer_2/c_attn[0][0]             \n",
            "                                                                 attention_mask_input[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "layer_2/c_attn_proj (Conv1D)    (None, 512, 768)     590592      layer_2/self_attention[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "layer_2/ln_1_drop (Dropout)     (None, 512, 768)     0           layer_2/c_attn_proj[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "layer_2/ln_1_add (Add)          (None, 512, 768)     0           layer_1/ln_2[0][0]               \n",
            "                                                                 layer_2/ln_1_drop[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "layer_2/ln_1 (LayerNormalizatio (None, 512, 768)     1536        layer_2/ln_1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "layer_2/c_fc (Conv1D)           (None, 512, 3072)    2362368     layer_2/ln_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "layer_2/gelu (Gelu)             (None, 512, 3072)    0           layer_2/c_fc[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "layer_2/c_ffn_proj (Conv1D)     (None, 512, 768)     2360064     layer_2/gelu[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "layer_2/ln_2_drop (Dropout)     (None, 512, 768)     0           layer_2/c_ffn_proj[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "layer_2/ln_2_add (Add)          (None, 512, 768)     0           layer_2/ln_1[0][0]               \n",
            "                                                                 layer_2/ln_2_drop[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "layer_2/ln_2 (LayerNormalizatio (None, 512, 768)     1536        layer_2/ln_2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "layer_3/c_attn (Conv1D)         (None, 512, 2304)    1771776     layer_2/ln_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "layer_3/self_attention (MultiHe (None, 512, 768)     0           layer_3/c_attn[0][0]             \n",
            "                                                                 attention_mask_input[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "layer_3/c_attn_proj (Conv1D)    (None, 512, 768)     590592      layer_3/self_attention[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "layer_3/ln_1_drop (Dropout)     (None, 512, 768)     0           layer_3/c_attn_proj[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "layer_3/ln_1_add (Add)          (None, 512, 768)     0           layer_2/ln_2[0][0]               \n",
            "                                                                 layer_3/ln_1_drop[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "layer_3/ln_1 (LayerNormalizatio (None, 512, 768)     1536        layer_3/ln_1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "layer_3/c_fc (Conv1D)           (None, 512, 3072)    2362368     layer_3/ln_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "layer_3/gelu (Gelu)             (None, 512, 3072)    0           layer_3/c_fc[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "layer_3/c_ffn_proj (Conv1D)     (None, 512, 768)     2360064     layer_3/gelu[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "layer_3/ln_2_drop (Dropout)     (None, 512, 768)     0           layer_3/c_ffn_proj[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "layer_3/ln_2_add (Add)          (None, 512, 768)     0           layer_3/ln_1[0][0]               \n",
            "                                                                 layer_3/ln_2_drop[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "layer_3/ln_2 (LayerNormalizatio (None, 512, 768)     1536        layer_3/ln_2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "layer_4/c_attn (Conv1D)         (None, 512, 2304)    1771776     layer_3/ln_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "layer_4/self_attention (MultiHe (None, 512, 768)     0           layer_4/c_attn[0][0]             \n",
            "                                                                 attention_mask_input[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "layer_4/c_attn_proj (Conv1D)    (None, 512, 768)     590592      layer_4/self_attention[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "layer_4/ln_1_drop (Dropout)     (None, 512, 768)     0           layer_4/c_attn_proj[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "layer_4/ln_1_add (Add)          (None, 512, 768)     0           layer_3/ln_2[0][0]               \n",
            "                                                                 layer_4/ln_1_drop[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "layer_4/ln_1 (LayerNormalizatio (None, 512, 768)     1536        layer_4/ln_1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "layer_4/c_fc (Conv1D)           (None, 512, 3072)    2362368     layer_4/ln_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "layer_4/gelu (Gelu)             (None, 512, 3072)    0           layer_4/c_fc[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "layer_4/c_ffn_proj (Conv1D)     (None, 512, 768)     2360064     layer_4/gelu[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "layer_4/ln_2_drop (Dropout)     (None, 512, 768)     0           layer_4/c_ffn_proj[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "layer_4/ln_2_add (Add)          (None, 512, 768)     0           layer_4/ln_1[0][0]               \n",
            "                                                                 layer_4/ln_2_drop[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "layer_4/ln_2 (LayerNormalizatio (None, 512, 768)     1536        layer_4/ln_2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "layer_5/c_attn (Conv1D)         (None, 512, 2304)    1771776     layer_4/ln_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "layer_5/self_attention (MultiHe (None, 512, 768)     0           layer_5/c_attn[0][0]             \n",
            "                                                                 attention_mask_input[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "layer_5/c_attn_proj (Conv1D)    (None, 512, 768)     590592      layer_5/self_attention[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "layer_5/ln_1_drop (Dropout)     (None, 512, 768)     0           layer_5/c_attn_proj[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "layer_5/ln_1_add (Add)          (None, 512, 768)     0           layer_4/ln_2[0][0]               \n",
            "                                                                 layer_5/ln_1_drop[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "layer_5/ln_1 (LayerNormalizatio (None, 512, 768)     1536        layer_5/ln_1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "layer_5/c_fc (Conv1D)           (None, 512, 3072)    2362368     layer_5/ln_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "layer_5/gelu (Gelu)             (None, 512, 3072)    0           layer_5/c_fc[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "layer_5/c_ffn_proj (Conv1D)     (None, 512, 768)     2360064     layer_5/gelu[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "layer_5/ln_2_drop (Dropout)     (None, 512, 768)     0           layer_5/c_ffn_proj[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "layer_5/ln_2_add (Add)          (None, 512, 768)     0           layer_5/ln_1[0][0]               \n",
            "                                                                 layer_5/ln_2_drop[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "layer_5/ln_2 (LayerNormalizatio (None, 512, 768)     1536        layer_5/ln_2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "layer_6/c_attn (Conv1D)         (None, 512, 2304)    1771776     layer_5/ln_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "layer_6/self_attention (MultiHe (None, 512, 768)     0           layer_6/c_attn[0][0]             \n",
            "                                                                 attention_mask_input[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "layer_6/c_attn_proj (Conv1D)    (None, 512, 768)     590592      layer_6/self_attention[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "layer_6/ln_1_drop (Dropout)     (None, 512, 768)     0           layer_6/c_attn_proj[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "layer_6/ln_1_add (Add)          (None, 512, 768)     0           layer_5/ln_2[0][0]               \n",
            "                                                                 layer_6/ln_1_drop[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "layer_6/ln_1 (LayerNormalizatio (None, 512, 768)     1536        layer_6/ln_1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "layer_6/c_fc (Conv1D)           (None, 512, 3072)    2362368     layer_6/ln_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "layer_6/gelu (Gelu)             (None, 512, 3072)    0           layer_6/c_fc[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "layer_6/c_ffn_proj (Conv1D)     (None, 512, 768)     2360064     layer_6/gelu[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "layer_6/ln_2_drop (Dropout)     (None, 512, 768)     0           layer_6/c_ffn_proj[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "layer_6/ln_2_add (Add)          (None, 512, 768)     0           layer_6/ln_1[0][0]               \n",
            "                                                                 layer_6/ln_2_drop[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "layer_6/ln_2 (LayerNormalizatio (None, 512, 768)     1536        layer_6/ln_2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "layer_7/c_attn (Conv1D)         (None, 512, 2304)    1771776     layer_6/ln_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "layer_7/self_attention (MultiHe (None, 512, 768)     0           layer_7/c_attn[0][0]             \n",
            "                                                                 attention_mask_input[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "layer_7/c_attn_proj (Conv1D)    (None, 512, 768)     590592      layer_7/self_attention[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "layer_7/ln_1_drop (Dropout)     (None, 512, 768)     0           layer_7/c_attn_proj[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "layer_7/ln_1_add (Add)          (None, 512, 768)     0           layer_6/ln_2[0][0]               \n",
            "                                                                 layer_7/ln_1_drop[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "layer_7/ln_1 (LayerNormalizatio (None, 512, 768)     1536        layer_7/ln_1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "layer_7/c_fc (Conv1D)           (None, 512, 3072)    2362368     layer_7/ln_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "layer_7/gelu (Gelu)             (None, 512, 3072)    0           layer_7/c_fc[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "layer_7/c_ffn_proj (Conv1D)     (None, 512, 768)     2360064     layer_7/gelu[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "layer_7/ln_2_drop (Dropout)     (None, 512, 768)     0           layer_7/c_ffn_proj[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "layer_7/ln_2_add (Add)          (None, 512, 768)     0           layer_7/ln_1[0][0]               \n",
            "                                                                 layer_7/ln_2_drop[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "layer_7/ln_2 (LayerNormalizatio (None, 512, 768)     1536        layer_7/ln_2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "layer_8/c_attn (Conv1D)         (None, 512, 2304)    1771776     layer_7/ln_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "layer_8/self_attention (MultiHe (None, 512, 768)     0           layer_8/c_attn[0][0]             \n",
            "                                                                 attention_mask_input[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "layer_8/c_attn_proj (Conv1D)    (None, 512, 768)     590592      layer_8/self_attention[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "layer_8/ln_1_drop (Dropout)     (None, 512, 768)     0           layer_8/c_attn_proj[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "layer_8/ln_1_add (Add)          (None, 512, 768)     0           layer_7/ln_2[0][0]               \n",
            "                                                                 layer_8/ln_1_drop[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "layer_8/ln_1 (LayerNormalizatio (None, 512, 768)     1536        layer_8/ln_1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "layer_8/c_fc (Conv1D)           (None, 512, 3072)    2362368     layer_8/ln_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "layer_8/gelu (Gelu)             (None, 512, 3072)    0           layer_8/c_fc[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "layer_8/c_ffn_proj (Conv1D)     (None, 512, 768)     2360064     layer_8/gelu[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "layer_8/ln_2_drop (Dropout)     (None, 512, 768)     0           layer_8/c_ffn_proj[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "layer_8/ln_2_add (Add)          (None, 512, 768)     0           layer_8/ln_1[0][0]               \n",
            "                                                                 layer_8/ln_2_drop[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "layer_8/ln_2 (LayerNormalizatio (None, 512, 768)     1536        layer_8/ln_2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "layer_9/c_attn (Conv1D)         (None, 512, 2304)    1771776     layer_8/ln_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "layer_9/self_attention (MultiHe (None, 512, 768)     0           layer_9/c_attn[0][0]             \n",
            "                                                                 attention_mask_input[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "layer_9/c_attn_proj (Conv1D)    (None, 512, 768)     590592      layer_9/self_attention[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "layer_9/ln_1_drop (Dropout)     (None, 512, 768)     0           layer_9/c_attn_proj[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "layer_9/ln_1_add (Add)          (None, 512, 768)     0           layer_8/ln_2[0][0]               \n",
            "                                                                 layer_9/ln_1_drop[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "layer_9/ln_1 (LayerNormalizatio (None, 512, 768)     1536        layer_9/ln_1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "layer_9/c_fc (Conv1D)           (None, 512, 3072)    2362368     layer_9/ln_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "layer_9/gelu (Gelu)             (None, 512, 3072)    0           layer_9/c_fc[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "layer_9/c_ffn_proj (Conv1D)     (None, 512, 768)     2360064     layer_9/gelu[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "layer_9/ln_2_drop (Dropout)     (None, 512, 768)     0           layer_9/c_ffn_proj[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "layer_9/ln_2_add (Add)          (None, 512, 768)     0           layer_9/ln_1[0][0]               \n",
            "                                                                 layer_9/ln_2_drop[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "layer_9/ln_2 (LayerNormalizatio (None, 512, 768)     1536        layer_9/ln_2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "layer_10/c_attn (Conv1D)        (None, 512, 2304)    1771776     layer_9/ln_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "layer_10/self_attention (MultiH (None, 512, 768)     0           layer_10/c_attn[0][0]            \n",
            "                                                                 attention_mask_input[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "layer_10/c_attn_proj (Conv1D)   (None, 512, 768)     590592      layer_10/self_attention[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "layer_10/ln_1_drop (Dropout)    (None, 512, 768)     0           layer_10/c_attn_proj[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "layer_10/ln_1_add (Add)         (None, 512, 768)     0           layer_9/ln_2[0][0]               \n",
            "                                                                 layer_10/ln_1_drop[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "layer_10/ln_1 (LayerNormalizati (None, 512, 768)     1536        layer_10/ln_1_add[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "layer_10/c_fc (Conv1D)          (None, 512, 3072)    2362368     layer_10/ln_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "layer_10/gelu (Gelu)            (None, 512, 3072)    0           layer_10/c_fc[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "layer_10/c_ffn_proj (Conv1D)    (None, 512, 768)     2360064     layer_10/gelu[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "layer_10/ln_2_drop (Dropout)    (None, 512, 768)     0           layer_10/c_ffn_proj[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "layer_10/ln_2_add (Add)         (None, 512, 768)     0           layer_10/ln_1[0][0]              \n",
            "                                                                 layer_10/ln_2_drop[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "layer_10/ln_2 (LayerNormalizati (None, 512, 768)     1536        layer_10/ln_2_add[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "layer_11/c_attn (Conv1D)        (None, 512, 2304)    1771776     layer_10/ln_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "layer_11/self_attention (MultiH (None, 512, 768)     0           layer_11/c_attn[0][0]            \n",
            "                                                                 attention_mask_input[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "layer_11/c_attn_proj (Conv1D)   (None, 512, 768)     590592      layer_11/self_attention[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "layer_11/ln_1_drop (Dropout)    (None, 512, 768)     0           layer_11/c_attn_proj[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "layer_11/ln_1_add (Add)         (None, 512, 768)     0           layer_10/ln_2[0][0]              \n",
            "                                                                 layer_11/ln_1_drop[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "layer_11/ln_1 (LayerNormalizati (None, 512, 768)     1536        layer_11/ln_1_add[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "layer_11/c_fc (Conv1D)          (None, 512, 3072)    2362368     layer_11/ln_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "layer_11/gelu (Gelu)            (None, 512, 3072)    0           layer_11/c_fc[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "layer_11/c_ffn_proj (Conv1D)    (None, 512, 768)     2360064     layer_11/gelu[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "layer_11/ln_2_drop (Dropout)    (None, 512, 768)     0           layer_11/c_ffn_proj[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "layer_11/ln_2_add (Add)         (None, 512, 768)     0           layer_11/ln_1[0][0]              \n",
            "                                                                 layer_11/ln_2_drop[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "layer_11/ln_2 (LayerNormalizati (None, 512, 768)     1536        layer_11/ln_2_add[0][0]          \n",
            "==================================================================================================\n",
            "Total params: 108,816,384\n",
            "Trainable params: 108,816,384\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjfwtkDgRR1l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "0a60b69b-8f0f-499e-ac34-bf3520708121"
      },
      "source": [
        "print(g_bert.inputs,g_bert.outputs)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[<tf.Tensor 'token_input_1:0' shape=(?, 512) dtype=int32>, <tf.Tensor 'segment_input_1:0' shape=(?, 512) dtype=int32>, <tf.Tensor 'position_input_1:0' shape=(?, 512) dtype=int32>, <tf.Tensor 'attention_mask_input_1:0' shape=(?, 1, 512, 512) dtype=float32>] [<tf.Tensor 'layer_11/ln_2/add_1:0' shape=(?, 512, 768) dtype=float32>]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IV6WzY2awqfz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# @title Compile keras  model here\n",
        "from transformer.train import train_model\n",
        "if use_tpu:\n",
        "    assert 'COLAB_TPU_ADDR' in os.environ, 'ERROR: Not connected to a TPU runtime; Maybe you should switch hardware accelerator to TPU for TPU support'\n",
        "    import tensorflow as tf\n",
        "    tpu_address = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "    strategy = tf.contrib.tpu.TPUDistributionStrategy(\n",
        "        tf.contrib.cluster_resolver.TPUClusterResolver(tpu=tpu_address)\n",
        "    )\n",
        "    g_bert = tf.contrib.tpu.keras_to_tpu_model(\n",
        "                      g_bert, strategy=strategy)\n",
        "g_bert.compile('adam', 'mse')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qe48g-XqRkR7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "def random_data_generator():\n",
        "    while True:\n",
        "        x1 = np.random.randint(0,5000,(8,512))\n",
        "        x2 = np.random.randint(0,2,(8,512))\n",
        "        x3 = np.random.randint(0,512,(8,512))\n",
        "        x4 = np.random.rand(8,1,512,512)\n",
        "        y = np.random.rand(8,512,768)\n",
        "        yield [x1,x2,x3,x4],y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUiuchGMSeqC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "d_g = random_data_generator()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qyTV6aEdSiXF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x,y = next(d_g)\n",
        "print(g_bert.train_on_batch(x,y))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0vp62Sau8Oq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}