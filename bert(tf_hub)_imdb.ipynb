{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bert(tf hub)_imdb.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ztjfreedom/colab/blob/master/bert(tf_hub)_imdb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0a4mTk9o1Qg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Copyright 2019 Google Inc.\n",
        "\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCpvgG0vwXAZ",
        "colab_type": "text"
      },
      "source": [
        "#Predicting Movie Review Sentiment with BERT on TF Hub"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xiYrZKaHwV81",
        "colab_type": "text"
      },
      "source": [
        "If you’ve been following Natural Language Processing over the past year, you’ve probably heard of BERT: Bidirectional Encoder Representations from Transformers. It’s a neural network architecture designed by Google researchers that’s totally transformed what’s state-of-the-art for NLP tasks, like text classification, translation, summarization, and question answering.\n",
        "\n",
        "Now that BERT's been added to [TF Hub](https://www.tensorflow.org/hub) as a loadable module, it's easy(ish) to add into existing Tensorflow text pipelines. In an existing pipeline, BERT can replace text embedding layers like ELMO and GloVE. Alternatively, [finetuning](http://wiki.fast.ai/index.php/Fine_tuning) BERT can provide both an accuracy boost and faster training time in many cases.\n",
        "\n",
        "Here, we'll train a model to predict whether an IMDB movie review is positive or negative using BERT in Tensorflow with tf hub. Some code was adapted from [this colab notebook](https://colab.sandbox.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb). Let's get started!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsZvic2YxnTz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from datetime import datetime"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cp5wfXDx5SPH",
        "colab_type": "text"
      },
      "source": [
        "In addition to the standard libraries we imported above, we'll need to install BERT's python package."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jviywGyWyKsA",
        "colab_type": "code",
        "outputId": "800309f1-1954-409d-f5c1-87f667a3b31c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "!pip install bert-tensorflow"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting bert-tensorflow\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/66/7eb4e8b6ea35b7cc54c322c816f976167a43019750279a8473d355800a93/bert_tensorflow-1.0.1-py2.py3-none-any.whl (67kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 29.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from bert-tensorflow) (1.12.0)\n",
            "Installing collected packages: bert-tensorflow\n",
            "Successfully installed bert-tensorflow-1.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhbGEfwgdEtw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "764363e9-df04-458c-dd04-31736dc3448f"
      },
      "source": [
        "import bert\n",
        "from bert import run_classifier\n",
        "from bert import optimization\n",
        "from bert import tokenization"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0630 11:45:02.234014 139938041223040 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/bert/optimization.py:87: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVB3eOcjxxm1",
        "colab_type": "text"
      },
      "source": [
        "Below, we'll set an output directory location to store our model output and checkpoints. This can be a local directory, in which case you'd set OUTPUT_DIR to the name of the directory you'd like to create. If you're running this code in Google's hosted Colab, the directory won't persist after the Colab session ends.\n",
        "\n",
        "Alternatively, if you're a GCP user, you can store output in a GCP bucket. To do that, set a directory name in OUTPUT_DIR and the name of the GCP bucket in the BUCKET field.\n",
        "\n",
        "Set DO_DELETE to rewrite the OUTPUT_DIR if it exists. Otherwise, Tensorflow will load existing model checkpoints from that directory (if they exist)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "US_EAnICvP7f",
        "colab_type": "code",
        "outputId": "e8f321fe-25d4-4e94-fa9e-cd9c17d93b4c",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Set the output directory for saving model file\n",
        "# Optionally, set a GCP bucket location\n",
        "\n",
        "OUTPUT_DIR = 'bert_output'#@param {type:\"string\"}\n",
        "#@markdown Whether or not to clear/delete the directory and create a new one\n",
        "DO_DELETE = True #@param {type:\"boolean\"}\n",
        "#@markdown Set USE_BUCKET and BUCKET if you want to (optionally) store model output on GCP bucket.\n",
        "USE_BUCKET = False #@param {type:\"boolean\"}\n",
        "BUCKET = '' #@param {type:\"string\"}\n",
        "\n",
        "if USE_BUCKET:\n",
        "  OUTPUT_DIR = 'gs://{}/{}'.format(BUCKET, OUTPUT_DIR)\n",
        "  from google.colab import auth\n",
        "  auth.authenticate_user()\n",
        "\n",
        "if DO_DELETE:\n",
        "  try:\n",
        "    tf.gfile.DeleteRecursively(OUTPUT_DIR)\n",
        "  except:\n",
        "    # Doesn't matter if the directory didn't exist\n",
        "    pass\n",
        "tf.gfile.MakeDirs(OUTPUT_DIR)\n",
        "print('***** Model output directory: {} *****'.format(OUTPUT_DIR))\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "***** Model output directory: bert_output *****\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmFYvkylMwXn",
        "colab_type": "text"
      },
      "source": [
        "#Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MC_w8SRqN0fr",
        "colab_type": "text"
      },
      "source": [
        "First, let's download the dataset, hosted by Stanford. The code below, which downloads, extracts, and imports the IMDB Large Movie Review Dataset, is borrowed from [this Tensorflow tutorial](https://www.tensorflow.org/hub/tutorials/text_classification_with_tf_hub)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fom_ff20gyy6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow import keras\n",
        "import os\n",
        "import re\n",
        "\n",
        "# Load all files from a directory in a DataFrame.\n",
        "def load_directory_data(directory):\n",
        "  data = {}\n",
        "  data[\"sentence\"] = []\n",
        "  data[\"sentiment\"] = []\n",
        "  for file_path in os.listdir(directory):\n",
        "    with tf.gfile.GFile(os.path.join(directory, file_path), \"r\") as f:\n",
        "      data[\"sentence\"].append(f.read())\n",
        "      data[\"sentiment\"].append(re.match(\"\\d+_(\\d+)\\.txt\", file_path).group(1))\n",
        "  return pd.DataFrame.from_dict(data)\n",
        "\n",
        "# Merge positive and negative examples, add a polarity column and shuffle.\n",
        "def load_dataset(directory):\n",
        "  pos_df = load_directory_data(os.path.join(directory, \"pos\"))\n",
        "  neg_df = load_directory_data(os.path.join(directory, \"neg\"))\n",
        "  pos_df[\"polarity\"] = 1\n",
        "  neg_df[\"polarity\"] = 0\n",
        "  return pd.concat([pos_df, neg_df]).sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "# Download and process the dataset files.\n",
        "def download_and_load_datasets(force_download=False):\n",
        "  dataset = tf.keras.utils.get_file(\n",
        "      fname=\"aclImdb.tar.gz\", \n",
        "      origin=\"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\", \n",
        "      extract=True)\n",
        "  \n",
        "  train_df = load_dataset(os.path.join(os.path.dirname(dataset), \n",
        "                                       \"aclImdb\", \"train\"))\n",
        "  test_df = load_dataset(os.path.join(os.path.dirname(dataset), \n",
        "                                      \"aclImdb\", \"test\"))\n",
        "  \n",
        "  return train_df, test_df\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2abfwdn-g135",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "b1895c32-3721-4d6c-f83f-45fbd0d793bb"
      },
      "source": [
        "train, test = download_and_load_datasets()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
            "84131840/84125825 [==============================] - 5s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XA8WHJgzhIZf",
        "colab_type": "text"
      },
      "source": [
        "To keep training fast, we'll take a sample of 5000 train and test examples, respectively."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lw_F488eixTV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = train.sample(5000)\n",
        "test = test.sample(5000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prRQM8pDi8xI",
        "colab_type": "code",
        "outputId": "7a4ab070-ec89-47f1-9ddf-03b15b882e58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "print(train.columns)\n",
        "display(train.head())"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Index(['sentence', 'sentiment', 'polarity'], dtype='object')\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>polarity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>20976</th>\n",
              "      <td>There's nothing quite like watching giant robo...</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11227</th>\n",
              "      <td>This film has been scaring me since the first ...</td>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18159</th>\n",
              "      <td>First of all, f117 is not high tech any more a...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2713</th>\n",
              "      <td>Thank the Lord for Martin Scorsese, and his lo...</td>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18592</th>\n",
              "      <td>In 2004, I liked it. Then it became very stupi...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                sentence sentiment  polarity\n",
              "20976  There's nothing quite like watching giant robo...         3         0\n",
              "11227  This film has been scaring me since the first ...        10         1\n",
              "18159  First of all, f117 is not high tech any more a...         1         0\n",
              "2713   Thank the Lord for Martin Scorsese, and his lo...        10         1\n",
              "18592  In 2004, I liked it. Then it became very stupi...         1         0"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfRnHSz3iSXz",
        "colab_type": "text"
      },
      "source": [
        "For us, our input data is the 'sentence' column and our label is the 'polarity' column (0, 1 for negative and positive, respecitvely)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IuMOGwFui4it",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DATA_COLUMN = 'sentence'\n",
        "LABEL_COLUMN = 'polarity'\n",
        "# label_list is the list of labels, i.e. True, False or 0, 1 or 'dog', 'cat'\n",
        "label_list = [0, 1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V399W0rqNJ-Z",
        "colab_type": "text"
      },
      "source": [
        "#Data Preprocessing\n",
        "We'll need to transform our data into a format BERT understands. This involves two steps. First, we create  `InputExample`'s using the constructor provided in the BERT library.\n",
        "\n",
        "- `text_a` is the text we want to classify, which in this case, is the `Request` field in our Dataframe. \n",
        "- `text_b` is used if we're training a model to understand the relationship between sentences (i.e. is `text_b` a translation of `text_a`? Is `text_b` an answer to the question asked by `text_a`?). This doesn't apply to our task, so we can leave `text_b` blank.\n",
        "- `label` is the label for our example, i.e. True, False"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9gEt5SmM6i6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use the InputExample class from BERT's run_classifier code to create examples from the data\n",
        "train_InputExamples = train.apply(lambda x: bert.run_classifier.InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this example\n",
        "                                                                   text_a = x[DATA_COLUMN], \n",
        "                                                                   text_b = None, \n",
        "                                                                   label = x[LABEL_COLUMN]), axis = 1)\n",
        "\n",
        "test_InputExamples = test.apply(lambda x: bert.run_classifier.InputExample(guid=None, \n",
        "                                                                   text_a = x[DATA_COLUMN], \n",
        "                                                                   text_b = None, \n",
        "                                                                   label = x[LABEL_COLUMN]), axis = 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nj3bEEL1gQqD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "24329429-a53b-4297-b5cd-b3ee62a6dc1b"
      },
      "source": [
        "display(train_InputExamples.head())"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "20976    <bert.run_classifier.InputExample object at 0x...\n",
              "11227    <bert.run_classifier.InputExample object at 0x...\n",
              "18159    <bert.run_classifier.InputExample object at 0x...\n",
              "2713     <bert.run_classifier.InputExample object at 0x...\n",
              "18592    <bert.run_classifier.InputExample object at 0x...\n",
              "dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCZWZtKxObjh",
        "colab_type": "text"
      },
      "source": [
        "Next, we need to preprocess our data so that it matches the data BERT was trained on. For this, we'll need to do a couple of things (but don't worry--this is also included in the Python library):\n",
        "\n",
        "\n",
        "1. Lowercase our text (if we're using a BERT lowercase model)\n",
        "2. Tokenize it (i.e. \"sally says hi\" -> [\"sally\", \"says\", \"hi\"])\n",
        "3. Break words into WordPieces (i.e. \"calling\" -> [\"call\", \"##ing\"])\n",
        "4. Map our words to indexes using a vocab file that BERT provides\n",
        "5. Add special \"CLS\" and \"SEP\" tokens (see the [readme](https://github.com/google-research/bert))\n",
        "6. Append \"index\" and \"segment\" tokens to each input (see the [BERT paper](https://arxiv.org/pdf/1810.04805.pdf))\n",
        "\n",
        "Happily, we don't have to worry about most of these details.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMWiDtpyQSoU",
        "colab_type": "text"
      },
      "source": [
        "To start, we'll need to load a vocabulary file and lowercasing information directly from the BERT tf hub module:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IhJSe0QHNG7U",
        "colab_type": "code",
        "outputId": "5654f2a1-b174-423f-989e-34e2e2dc6ec7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# This is a path to an uncased (all lowercase) version of BERT\n",
        "BERT_MODEL_HUB = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n",
        "\n",
        "def create_tokenizer_from_hub_module():\n",
        "  \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n",
        "  with tf.Graph().as_default():\n",
        "    bert_module = hub.Module(BERT_MODEL_HUB)\n",
        "    tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
        "    with tf.Session() as sess:\n",
        "      vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\n",
        "                                            tokenization_info[\"do_lower_case\"]])\n",
        "      \n",
        "  return bert.tokenization.FullTokenizer(\n",
        "      vocab_file=vocab_file, do_lower_case=do_lower_case)\n",
        "\n",
        "tokenizer = create_tokenizer_from_hub_module()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0630 11:46:08.512880 139938041223040 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/bert/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4oFkhpZBDKm",
        "colab_type": "text"
      },
      "source": [
        "Great--we just learned that the BERT model we're using expects lowercase data (that's what stored in tokenization_info[\"do_lower_case\"]) and we also loaded BERT's vocab file. We also created a tokenizer, which breaks words into word pieces:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsBo6RCtQmwx",
        "colab_type": "code",
        "outputId": "4cf289f3-a2ce-4346-ed3e-88856c4b4afc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "tokenizer.tokenize(\"This here's an example of using the BERT tokenizer\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['this',\n",
              " 'here',\n",
              " \"'\",\n",
              " 's',\n",
              " 'an',\n",
              " 'example',\n",
              " 'of',\n",
              " 'using',\n",
              " 'the',\n",
              " 'bert',\n",
              " 'token',\n",
              " '##izer']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0OEzfFIt6GIc",
        "colab_type": "text"
      },
      "source": [
        "Using our tokenizer, we'll call `run_classifier.convert_examples_to_features` on our InputExamples to convert them into features BERT understands."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LL5W8gEGRTAf",
        "colab_type": "code",
        "outputId": "4bee3d1d-69d2-44c7-e4c5-e7a519bf706c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Change log level so that we can check the results after tokenization\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '0' \n",
        "tf.logging.set_verbosity(tf.logging.INFO)\n",
        "\n",
        "# We'll set sequences to be at most 128 tokens long.\n",
        "MAX_SEQ_LENGTH = 128\n",
        "# Convert our train and test features to InputFeatures that BERT understands.\n",
        "train_features = bert.run_classifier.convert_examples_to_features(train_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
        "test_features = bert.run_classifier.convert_examples_to_features(test_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0630 11:46:14.966231 139938041223040 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/bert/run_classifier.py:774: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n",
            "I0630 11:46:14.970876 139938041223040 run_classifier.py:774] Writing example 0 of 5000\n",
            "I0630 11:46:14.980789 139938041223040 run_classifier.py:461] *** Example ***\n",
            "I0630 11:46:14.982093 139938041223040 run_classifier.py:462] guid: None\n",
            "I0630 11:46:14.983613 139938041223040 run_classifier.py:464] tokens: [CLS] there ' s nothing quite like watching giant robots doing battle over a desert waste ##land , and robot wars does deliver . sure , the acting is lou ##sy , the dialogue is sub - par , and the characters are one - dimensional , but it has giant robots ! the special effects themselves are actually quite good for the period . they are certainly not as polished as today ' s standards , but it contains a minimum of computer graphics and instead uses miniatures , so it has aged fairly well . its short ##coming ##s are easily overlooked given the films short run ##time , and it does have a certain tongue - in - cheek humour in parts that make [SEP]\n",
            "I0630 11:46:14.984771 139938041223040 run_classifier.py:465] input_ids: 101 2045 1005 1055 2498 3243 2066 3666 5016 13507 2725 2645 2058 1037 5532 5949 3122 1010 1998 8957 5233 2515 8116 1012 2469 1010 1996 3772 2003 10223 6508 1010 1996 7982 2003 4942 1011 11968 1010 1998 1996 3494 2024 2028 1011 8789 1010 2021 2009 2038 5016 13507 999 1996 2569 3896 3209 2024 2941 3243 2204 2005 1996 2558 1012 2027 2024 5121 2025 2004 12853 2004 2651 1005 1055 4781 1010 2021 2009 3397 1037 6263 1997 3274 8389 1998 2612 3594 28615 1010 2061 2009 2038 4793 7199 2092 1012 2049 2460 18935 2015 2024 4089 17092 2445 1996 3152 2460 2448 7292 1010 1998 2009 2515 2031 1037 3056 4416 1011 1999 1011 5048 17211 1999 3033 2008 2191 102\n",
            "I0630 11:46:14.987848 139938041223040 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0630 11:46:14.988940 139938041223040 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0630 11:46:14.990291 139938041223040 run_classifier.py:468] label: 0 (id = 0)\n",
            "I0630 11:46:14.997707 139938041223040 run_classifier.py:461] *** Example ***\n",
            "I0630 11:46:14.999240 139938041223040 run_classifier.py:462] guid: None\n",
            "I0630 11:46:15.000457 139938041223040 run_classifier.py:464] tokens: [CLS] this film has been scar ##ing me since the first day i saw it . < br / > < br / > my mum had watched it when it was on the tell ##y back in ' 92 . i remember being woken up in the middle of the night by her tear ##ful ram ##bling ##s as my dad helped her up the stairs . < br / > < br / > she was saying something like \" don ' t let her get me \" or something like that . i asked what had made her so upset and she told me that she ' d been watching the woman in black . < br / > < br / > so [SEP]\n",
            "I0630 11:46:15.001973 139938041223040 run_classifier.py:465] input_ids: 101 2023 2143 2038 2042 11228 2075 2033 2144 1996 2034 2154 1045 2387 2009 1012 1026 7987 1013 1028 1026 7987 1013 1028 2026 12954 2018 3427 2009 2043 2009 2001 2006 1996 2425 2100 2067 1999 1005 6227 1012 1045 3342 2108 22795 2039 1999 1996 2690 1997 1996 2305 2011 2014 7697 3993 8223 9709 2015 2004 2026 3611 3271 2014 2039 1996 5108 1012 1026 7987 1013 1028 1026 7987 1013 1028 2016 2001 3038 2242 2066 1000 2123 1005 1056 2292 2014 2131 2033 1000 2030 2242 2066 2008 1012 1045 2356 2054 2018 2081 2014 2061 6314 1998 2016 2409 2033 2008 2016 1005 1040 2042 3666 1996 2450 1999 2304 1012 1026 7987 1013 1028 1026 7987 1013 1028 2061 102\n",
            "I0630 11:46:15.004778 139938041223040 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0630 11:46:15.005774 139938041223040 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0630 11:46:15.008379 139938041223040 run_classifier.py:468] label: 1 (id = 1)\n",
            "I0630 11:46:15.017019 139938041223040 run_classifier.py:461] *** Example ***\n",
            "I0630 11:46:15.017801 139938041223040 run_classifier.py:462] guid: None\n",
            "I0630 11:46:15.018623 139938041223040 run_classifier.py:464] tokens: [CLS] first of all , f1 ##17 is not high tech any more and it is not a fighter aircraft . < br / > < br / > secondly , the f1 ##4 ' s and f1 ##8 ' s cannot change their appearances ; they are not transformers . < br / > < br / > third ##ly , the f1 ##6 has only one m ##6 ##1 cannon , not two . < br / > < br / > last but not the least , at the end of the film , sea ##gle selected side ##wind ##er missile . but somehow when he pulled the trigger , the actual missile fired turned out to be a maverick . as i have [SEP]\n",
            "I0630 11:46:15.019917 139938041223040 run_classifier.py:465] input_ids: 101 2034 1997 2035 1010 20069 16576 2003 2025 2152 6627 2151 2062 1998 2009 2003 2025 1037 4959 2948 1012 1026 7987 1013 1028 1026 7987 1013 1028 16378 1010 1996 20069 2549 1005 1055 1998 20069 2620 1005 1055 3685 2689 2037 3922 1025 2027 2024 2025 19081 1012 1026 7987 1013 1028 1026 7987 1013 1028 2353 2135 1010 1996 20069 2575 2038 2069 2028 1049 2575 2487 8854 1010 2025 2048 1012 1026 7987 1013 1028 1026 7987 1013 1028 2197 2021 2025 1996 2560 1010 2012 1996 2203 1997 1996 2143 1010 2712 9354 3479 2217 11101 2121 7421 1012 2021 5064 2043 2002 2766 1996 9495 1010 1996 5025 7421 5045 2357 2041 2000 2022 1037 27187 1012 2004 1045 2031 102\n",
            "I0630 11:46:15.022292 139938041223040 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0630 11:46:15.027472 139938041223040 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0630 11:46:15.029314 139938041223040 run_classifier.py:468] label: 0 (id = 0)\n",
            "I0630 11:46:15.034214 139938041223040 run_classifier.py:461] *** Example ***\n",
            "I0630 11:46:15.035170 139938041223040 run_classifier.py:462] guid: None\n",
            "I0630 11:46:15.036528 139938041223040 run_classifier.py:464] tokens: [CLS] thank the lord for martin sc ##ors ##ese , and his love of the movies . < br / > < br / > this is the perfect introduction into the mind of the most talented american artist working in cinema today , and i couldn ' t recommend it more . i was en ##th ##ral ##led through the whole thing and you will be too . just relax and let him take you on a ride through his world , you ' ll love it . [SEP]\n",
            "I0630 11:46:15.038401 139938041223040 run_classifier.py:465] input_ids: 101 4067 1996 2935 2005 3235 8040 5668 6810 1010 1998 2010 2293 1997 1996 5691 1012 1026 7987 1013 1028 1026 7987 1013 1028 2023 2003 1996 3819 4955 2046 1996 2568 1997 1996 2087 10904 2137 3063 2551 1999 5988 2651 1010 1998 1045 2481 1005 1056 16755 2009 2062 1012 1045 2001 4372 2705 7941 3709 2083 1996 2878 2518 1998 2017 2097 2022 2205 1012 2074 9483 1998 2292 2032 2202 2017 2006 1037 4536 2083 2010 2088 1010 2017 1005 2222 2293 2009 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0630 11:46:15.039883 139938041223040 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0630 11:46:15.040804 139938041223040 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0630 11:46:15.042076 139938041223040 run_classifier.py:468] label: 1 (id = 1)\n",
            "I0630 11:46:15.048677 139938041223040 run_classifier.py:461] *** Example ***\n",
            "I0630 11:46:15.050004 139938041223040 run_classifier.py:462] guid: None\n",
            "I0630 11:46:15.050986 139938041223040 run_classifier.py:464] tokens: [CLS] in 2004 , i liked it . then it became very stupid . it suggests that kids are brain ##less . it insults children . cartoon network used to be great . one of the shows i liked was ham ##taro . it did manage to be interesting and imaginative in its approach to children programming . the show ( foster ' s home for imaginary friends ) is like putting 20 spoon ##s of sugar in your sp ##rite . it seems as if today ' s television producers are only interested in making money , rather than engaging the imagination ##s of children and making money ! lately , my children are tuning in to the old shows ( 60 ' s ) to [SEP]\n",
            "I0630 11:46:15.052602 139938041223040 run_classifier.py:465] input_ids: 101 1999 2432 1010 1045 4669 2009 1012 2059 2009 2150 2200 5236 1012 2009 6083 2008 4268 2024 4167 3238 1012 2009 23862 2336 1012 9476 2897 2109 2000 2022 2307 1012 2028 1997 1996 3065 1045 4669 2001 10654 28160 1012 2009 2106 6133 2000 2022 5875 1998 28575 1999 2049 3921 2000 2336 4730 1012 1996 2265 1006 6469 1005 1055 2188 2005 15344 2814 1007 2003 2066 5128 2322 15642 2015 1997 5699 1999 2115 11867 17625 1012 2009 3849 2004 2065 2651 1005 1055 2547 6443 2024 2069 4699 1999 2437 2769 1010 2738 2084 11973 1996 9647 2015 1997 2336 1998 2437 2769 999 9906 1010 2026 2336 2024 17372 1999 2000 1996 2214 3065 1006 3438 1005 1055 1007 2000 102\n",
            "I0630 11:46:15.053965 139938041223040 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0630 11:46:15.054906 139938041223040 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0630 11:46:15.056104 139938041223040 run_classifier.py:468] label: 0 (id = 0)\n",
            "I0630 11:46:39.641277 139938041223040 run_classifier.py:774] Writing example 0 of 5000\n",
            "I0630 11:46:39.649997 139938041223040 run_classifier.py:461] *** Example ***\n",
            "I0630 11:46:39.650870 139938041223040 run_classifier.py:462] guid: None\n",
            "I0630 11:46:39.652897 139938041223040 run_classifier.py:464] tokens: [CLS] ' it ' s easy to kill a monster , but it ' s hard to kill a human being . ' < br / > < br / > set in st . thomas housing project and angola prison in new orleans , \" dead man walking \" is the true story of helen pre ##jean ( susan sara ##ndon ) , a louisiana nun sister who befriended matthew ponce ##let ( sean penn ) , a murderer and a rap ##ist bound for a lethal injection machine for killing a teenage couple sister helen agrees to help the convict and to remain with him till the end ##an act never before attempted by a woman < br / > < br / > at [SEP]\n",
            "I0630 11:46:39.655426 139938041223040 run_classifier.py:465] input_ids: 101 1005 2009 1005 1055 3733 2000 3102 1037 6071 1010 2021 2009 1005 1055 2524 2000 3102 1037 2529 2108 1012 1005 1026 7987 1013 1028 1026 7987 1013 1028 2275 1999 2358 1012 2726 3847 2622 1998 13491 3827 1999 2047 5979 1010 1000 2757 2158 3788 1000 2003 1996 2995 2466 1997 6330 3653 27687 1006 6294 7354 19333 1007 1010 1037 5773 16634 2905 2040 23386 5487 21085 7485 1006 5977 9502 1007 1010 1037 13422 1998 1037 9680 2923 5391 2005 1037 12765 13341 3698 2005 4288 1037 9454 3232 2905 6330 10217 2000 2393 1996 20462 1998 2000 3961 2007 2032 6229 1996 2203 2319 2552 2196 2077 4692 2011 1037 2450 1026 7987 1013 1028 1026 7987 1013 1028 2012 102\n",
            "I0630 11:46:39.657961 139938041223040 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0630 11:46:39.660019 139938041223040 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0630 11:46:39.661602 139938041223040 run_classifier.py:468] label: 1 (id = 1)\n",
            "I0630 11:46:39.671248 139938041223040 run_classifier.py:461] *** Example ***\n",
            "I0630 11:46:39.672074 139938041223040 run_classifier.py:462] guid: None\n",
            "I0630 11:46:39.675055 139938041223040 run_classifier.py:464] tokens: [CLS] it ' s just one of those films , you ' re either love it or hate it , my girlfriend and me loved it , told my brother to rent it and he hated it , said it was too flash ##y and col ##lo ##qui ##al , then again he only usually goes to see big action movies , so probably not enough explosions left him disappointed . there were some great new talent ( i ' d never heard of the leads before anyway ) ? des brady ( the directors brother ? ) was especially good . playing a right dick at the start i thought he never would red ##eem himself but he managed to crawl out of the dark hole [SEP]\n",
            "I0630 11:46:39.676223 139938041223040 run_classifier.py:465] input_ids: 101 2009 1005 1055 2074 2028 1997 2216 3152 1010 2017 1005 2128 2593 2293 2009 2030 5223 2009 1010 2026 6513 1998 2033 3866 2009 1010 2409 2026 2567 2000 9278 2009 1998 2002 6283 2009 1010 2056 2009 2001 2205 5956 2100 1998 8902 4135 15549 2389 1010 2059 2153 2002 2069 2788 3632 2000 2156 2502 2895 5691 1010 2061 2763 2025 2438 18217 2187 2032 9364 1012 2045 2020 2070 2307 2047 5848 1006 1045 1005 1040 2196 2657 1997 1996 5260 2077 4312 1007 1029 4078 10184 1006 1996 5501 2567 1029 1007 2001 2926 2204 1012 2652 1037 2157 5980 2012 1996 2707 1045 2245 2002 2196 2052 2417 21564 2370 2021 2002 3266 2000 13529 2041 1997 1996 2601 4920 102\n",
            "I0630 11:46:39.677726 139938041223040 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0630 11:46:39.678567 139938041223040 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0630 11:46:39.679422 139938041223040 run_classifier.py:468] label: 1 (id = 1)\n",
            "I0630 11:46:39.690572 139938041223040 run_classifier.py:461] *** Example ***\n",
            "I0630 11:46:39.691560 139938041223040 run_classifier.py:462] guid: None\n",
            "I0630 11:46:39.692857 139938041223040 run_classifier.py:464] tokens: [CLS] it ' s hard to work up any enthusiasm for this awful cartoon - like epic that for some reason has become a cult classic . it certainly can ' t be because of the totally artificial look of the set designs or the limp ##id acting of an all - star cast . these days it ' s shown much too frequently on fox movie channel or amc . < br / > < br / > it pains me to report that veteran actors like walter pigeon and joan fontaine are even cast in this mud ##dled science fiction tr ##aves ##ty , none of which rings true . it ' s like watching an expensive budget being spent on a saturday afternoon kidd [SEP]\n",
            "I0630 11:46:39.693793 139938041223040 run_classifier.py:465] input_ids: 101 2009 1005 1055 2524 2000 2147 2039 2151 12024 2005 2023 9643 9476 1011 2066 8680 2008 2005 2070 3114 2038 2468 1037 8754 4438 1012 2009 5121 2064 1005 1056 2022 2138 1997 1996 6135 7976 2298 1997 1996 2275 5617 2030 1996 14401 3593 3772 1997 2019 2035 1011 2732 3459 1012 2122 2420 2009 1005 1055 3491 2172 2205 4703 2006 4419 3185 3149 2030 21962 1012 1026 7987 1013 1028 1026 7987 1013 1028 2009 20398 2033 2000 3189 2008 8003 5889 2066 4787 16516 1998 7437 25749 2024 2130 3459 1999 2023 8494 20043 2671 4349 19817 21055 3723 1010 3904 1997 2029 7635 2995 1012 2009 1005 1055 2066 3666 2019 6450 5166 2108 2985 2006 1037 5095 5027 25358 102\n",
            "I0630 11:46:39.694891 139938041223040 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0630 11:46:39.696023 139938041223040 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0630 11:46:39.697384 139938041223040 run_classifier.py:468] label: 0 (id = 0)\n",
            "I0630 11:46:39.711666 139938041223040 run_classifier.py:461] *** Example ***\n",
            "I0630 11:46:39.713643 139938041223040 run_classifier.py:462] guid: None\n",
            "I0630 11:46:39.715403 139938041223040 run_classifier.py:464] tokens: [CLS] a gang of bandits lead by the sh ##rew ##d , rugged , ruthless mon ##eter ##o ( a perfectly imposing performance by gilbert roland ) steals $ 300 , 000 worth of gold coins during a daring train robbery . but un ##trust ##worthy member ba ##hun ##da ( an amusing turn by jose torres ) makes off with the coins and hides them . unfortunately , ba ##hun ##da gets killed before he can tell mon ##eter ##o where he st ##ash ##ed the boot ##y . so mon ##eter ##o has to join forces with cunning , cocky , enigma ##tic bounty hunter the stranger ( smoothly played by the handsome george hilton ) and cage ##y , corrupt banker clayton ( a [SEP]\n",
            "I0630 11:46:39.716939 139938041223040 run_classifier.py:465] input_ids: 101 1037 6080 1997 19088 2599 2011 1996 14021 15603 2094 1010 17638 1010 18101 12256 15141 2080 1006 1037 6669 16625 2836 2011 7664 8262 1007 15539 1002 3998 1010 2199 4276 1997 2751 7824 2076 1037 15236 3345 13742 1012 2021 4895 24669 13966 2266 8670 17157 2850 1006 2019 19142 2735 2011 4560 13101 1007 3084 2125 2007 1996 7824 1998 17382 2068 1012 6854 1010 8670 17157 2850 4152 2730 2077 2002 2064 2425 12256 15141 2080 2073 2002 2358 11823 2098 1996 9573 2100 1012 2061 12256 15141 2080 2038 2000 3693 2749 2007 23626 1010 24995 1010 26757 4588 17284 4477 1996 7985 1006 15299 2209 2011 1996 8502 2577 15481 1007 1998 7980 2100 1010 13593 13448 11811 1006 1037 102\n",
            "I0630 11:46:39.718782 139938041223040 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0630 11:46:39.720379 139938041223040 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0630 11:46:39.722026 139938041223040 run_classifier.py:468] label: 1 (id = 1)\n",
            "I0630 11:46:39.730903 139938041223040 run_classifier.py:461] *** Example ***\n",
            "I0630 11:46:39.732479 139938041223040 run_classifier.py:462] guid: None\n",
            "I0630 11:46:39.734195 139938041223040 run_classifier.py:464] tokens: [CLS] if there ' s one theme of this film , it ' s that people can cope with hardship by having a good imagination . this family is poor , their father works graveyard , and their mother works double - shifts , and peter is constantly picked on for a variety of reasons , and becomes increasingly frustrated that he is often mistaken for a girl . he is just starting to approach that age of 10 or 11 where your perceptions start to change , and thinks like your appearance start to matter . the backdrop of this story is the 1967 world ' s fair and the centennial of canada . the film ' s greatest moments come during the various fantasy sequences [SEP]\n",
            "I0630 11:46:39.736003 139938041223040 run_classifier.py:465] input_ids: 101 2065 2045 1005 1055 2028 4323 1997 2023 2143 1010 2009 1005 1055 2008 2111 2064 11997 2007 26479 2011 2383 1037 2204 9647 1012 2023 2155 2003 3532 1010 2037 2269 2573 16685 1010 1998 2037 2388 2573 3313 1011 12363 1010 1998 2848 2003 7887 3856 2006 2005 1037 3528 1997 4436 1010 1998 4150 6233 10206 2008 2002 2003 2411 13534 2005 1037 2611 1012 2002 2003 2074 3225 2000 3921 2008 2287 1997 2184 2030 2340 2073 2115 23271 2707 2000 2689 1010 1998 6732 2066 2115 3311 2707 2000 3043 1012 1996 18876 1997 2023 2466 2003 1996 3476 2088 1005 1055 4189 1998 1996 15483 1997 2710 1012 1996 2143 1005 1055 4602 5312 2272 2076 1996 2536 5913 10071 102\n",
            "I0630 11:46:39.737477 139938041223040 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0630 11:46:39.739487 139938041223040 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0630 11:46:39.741025 139938041223040 run_classifier.py:468] label: 1 (id = 1)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccp5trMwRtmr",
        "colab_type": "text"
      },
      "source": [
        "#Creating a model\n",
        "\n",
        "Now that we've prepared our data, let's focus on building a model. `create_model` does just this below. First, it loads the BERT tf hub module again (this time to extract the computation graph). Next, it creates a single new layer that will be trained to adapt BERT to our sentiment task (i.e. classifying whether a movie review is positive or negative). This strategy of using a mostly trained model is called [fine-tuning](http://wiki.fast.ai/index.php/Fine_tuning)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6o2a5ZIvRcJq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model(is_predicting, input_ids, input_mask, segment_ids, labels,\n",
        "                 num_labels):\n",
        "  \"\"\"Creates a classification model.\"\"\"\n",
        "\n",
        "  bert_module = hub.Module(BERT_MODEL_HUB, trainable=True)\n",
        "    \n",
        "  bert_inputs = dict(input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids)\n",
        "\n",
        "  bert_outputs = bert_module(inputs=bert_inputs, signature=\"tokens\", as_dict=True)\n",
        "\n",
        "  # Use \"pooled_output\" for classification tasks on an entire sentence.\n",
        "  # Use \"sequence_outputs\" for token-level output.\n",
        "  output_layer = bert_outputs[\"pooled_output\"]\n",
        "\n",
        "  hidden_size = output_layer.shape[-1].value\n",
        "\n",
        "  # Create our own layer to tune for politeness data.\n",
        "  output_weights = tf.get_variable(\n",
        "      \"output_weights\", [num_labels, hidden_size],\n",
        "      initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
        "\n",
        "  output_bias = tf.get_variable(\n",
        "      \"output_bias\", [num_labels], initializer=tf.zeros_initializer())\n",
        "\n",
        "  with tf.variable_scope(\"loss\"):\n",
        "\n",
        "    # Dropout helps prevent overfitting\n",
        "    output_layer = tf.nn.dropout(output_layer, rate=0.1)\n",
        "\n",
        "    logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n",
        "    logits = tf.nn.bias_add(logits, output_bias)\n",
        "    log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
        "\n",
        "    # Convert labels into one-hot encoding\n",
        "    one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n",
        "\n",
        "    predicted_labels = tf.squeeze(tf.argmax(log_probs, axis=-1, output_type=tf.int32))\n",
        "    # If we're predicting, we want predicted labels and the probabiltiies.\n",
        "    if is_predicting:\n",
        "      return (predicted_labels, log_probs)\n",
        "\n",
        "    # If we're train/eval, compute loss between predicted and actual label\n",
        "    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n",
        "    loss = tf.reduce_mean(per_example_loss)\n",
        "    return (loss, predicted_labels, log_probs)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpE0ZIDOCQzE",
        "colab_type": "text"
      },
      "source": [
        "Next we'll wrap our model function in a `model_fn_builder` function that adapts our model to work for training, evaluation, and prediction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnH-AnOQ9KKW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model_fn_builder actually creates our model function\n",
        "# using the passed parameters for num_labels, learning_rate, etc.\n",
        "def model_fn_builder(num_labels, learning_rate, num_train_steps,\n",
        "                     num_warmup_steps):\n",
        "  \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n",
        "  def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n",
        "    \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n",
        "\n",
        "    input_ids = features[\"input_ids\"]\n",
        "    input_mask = features[\"input_mask\"]\n",
        "    segment_ids = features[\"segment_ids\"]\n",
        "    label_ids = features[\"label_ids\"]\n",
        "\n",
        "    is_predicting = (mode == tf.estimator.ModeKeys.PREDICT)\n",
        "    \n",
        "    # TRAIN and EVAL\n",
        "    if not is_predicting:\n",
        "\n",
        "      (loss, predicted_labels, log_probs) = create_model(\n",
        "        is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n",
        "\n",
        "      train_op = bert.optimization.create_optimizer(\n",
        "          loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu=False)\n",
        "\n",
        "      # Calculate evaluation metrics. \n",
        "      def metric_fn(label_ids, predicted_labels):\n",
        "        accuracy = tf.metrics.accuracy(label_ids, predicted_labels)\n",
        "        f1_score = tf.contrib.metrics.f1_score(\n",
        "            label_ids,\n",
        "            predicted_labels)\n",
        "        auc = tf.metrics.auc(\n",
        "            label_ids,\n",
        "            predicted_labels)\n",
        "        recall = tf.metrics.recall(\n",
        "            label_ids,\n",
        "            predicted_labels)\n",
        "        precision = tf.metrics.precision(\n",
        "            label_ids,\n",
        "            predicted_labels) \n",
        "        true_pos = tf.metrics.true_positives(\n",
        "            label_ids,\n",
        "            predicted_labels)\n",
        "        true_neg = tf.metrics.true_negatives(\n",
        "            label_ids,\n",
        "            predicted_labels)   \n",
        "        false_pos = tf.metrics.false_positives(\n",
        "            label_ids,\n",
        "            predicted_labels)  \n",
        "        false_neg = tf.metrics.false_negatives(\n",
        "            label_ids,\n",
        "            predicted_labels)\n",
        "        return {\n",
        "            \"eval_accuracy\": accuracy,\n",
        "            \"f1_score\": f1_score,\n",
        "            \"auc\": auc,\n",
        "            \"precision\": precision,\n",
        "            \"recall\": recall,\n",
        "            \"true_positives\": true_pos,\n",
        "            \"true_negatives\": true_neg,\n",
        "            \"false_positives\": false_pos,\n",
        "            \"false_negatives\": false_neg\n",
        "        }\n",
        "\n",
        "      eval_metrics = metric_fn(label_ids, predicted_labels)\n",
        "\n",
        "      if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "        return tf.estimator.EstimatorSpec(mode=mode,\n",
        "          loss=loss,\n",
        "          train_op=train_op)\n",
        "      else:\n",
        "          return tf.estimator.EstimatorSpec(mode=mode,\n",
        "            loss=loss,\n",
        "            eval_metric_ops=eval_metrics)\n",
        "    else:\n",
        "      (predicted_labels, log_probs) = create_model(\n",
        "        is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n",
        "\n",
        "      predictions = {\n",
        "          'probabilities': log_probs,\n",
        "          'labels': predicted_labels\n",
        "      }\n",
        "      return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
        "\n",
        "  # Return the actual model function in the closure\n",
        "  return model_fn\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjwJ4bTeWXD8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compute train and warmup steps from batch size\n",
        "# These hyperparameters are copied from this colab notebook (https://colab.sandbox.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb)\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 2e-5\n",
        "NUM_TRAIN_EPOCHS = 3.0\n",
        "# Warmup is a period of time where hte learning rate \n",
        "# is small and gradually increases--usually helps training.\n",
        "WARMUP_PROPORTION = 0.1\n",
        "# Model configs\n",
        "SAVE_CHECKPOINTS_STEPS = 500\n",
        "SAVE_SUMMARY_STEPS = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emHf9GhfWBZ_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compute # train and warmup steps from batch size\n",
        "num_train_steps = int(len(train_features) / BATCH_SIZE * NUM_TRAIN_EPOCHS)\n",
        "num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEJldMr3WYZa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Specify output directory and number of checkpoint steps to save\n",
        "run_config = tf.estimator.RunConfig(\n",
        "    model_dir=OUTPUT_DIR,\n",
        "    save_summary_steps=SAVE_SUMMARY_STEPS,\n",
        "    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_WebpS1X97v",
        "colab_type": "code",
        "outputId": "bf2e1d42-8b00-4320-a93f-6204b4b58260",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "model_fn = model_fn_builder(\n",
        "  num_labels=len(label_list),\n",
        "  learning_rate=LEARNING_RATE,\n",
        "  num_train_steps=num_train_steps,\n",
        "  num_warmup_steps=num_warmup_steps)\n",
        "\n",
        "estimator = tf.estimator.Estimator(\n",
        "  model_fn=model_fn,\n",
        "  config=run_config,\n",
        "  params={\"batch_size\": BATCH_SIZE})\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I0630 11:55:32.913325 139938041223040 estimator.py:209] Using config: {'_model_dir': 'bert_output', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 500, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f457034ff98>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOO3RfG1DYLo",
        "colab_type": "text"
      },
      "source": [
        "Next we create an input builder function that takes our training feature set (`train_features`) and produces a generator. This is a pretty standard design pattern for working with Tensorflow [Estimators](https://www.tensorflow.org/guide/estimators)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Pv2bAlOX_-K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create an input function for training. drop_remainder = True for using TPUs.\n",
        "train_input_fn = bert.run_classifier.input_fn_builder(\n",
        "    features=train_features,\n",
        "    seq_length=MAX_SEQ_LENGTH,\n",
        "    is_training=True,\n",
        "    drop_remainder=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6Nukby2EB6-",
        "colab_type": "text"
      },
      "source": [
        "Now we train our model! For me, using a Colab notebook running on Google's GPUs, my training time was about 14 minutes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nucD4gluYJmK",
        "colab_type": "code",
        "outputId": "e87c1ff0-1817-41f3-90fc-025ec02ef9b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "source": [
        "print(f'Beginning Training!')\n",
        "current_time = datetime.now()\n",
        "estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n",
        "print(\"Training took time \", datetime.now() - current_time)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Beginning Training!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0630 11:58:03.748739 139938041223040 estimator.py:1145] Calling model_fn.\n",
            "I0630 11:58:06.796174 139938041223040 saver.py:1499] Saver not created because there are no variables in the graph to restore\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
            "I0630 11:58:15.531408 139938041223040 estimator.py:1147] Done calling model_fn.\n",
            "I0630 11:58:15.534143 139938041223040 basic_session_run_hooks.py:541] Create CheckpointSaverHook.\n",
            "I0630 11:58:16.855484 139938041223040 monitored_session.py:240] Graph was finalized.\n",
            "W0630 11:58:16.866664 139938041223040 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "I0630 11:58:16.874634 139938041223040 saver.py:1280] Restoring parameters from bert_output/model.ckpt-0\n",
            "W0630 11:58:18.626861 139938041223040 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1066: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file utilities to get mtimes.\n",
            "I0630 11:58:19.270207 139938041223040 session_manager.py:500] Running local_init_op.\n",
            "I0630 11:58:19.493640 139938041223040 session_manager.py:502] Done running local_init_op.\n",
            "I0630 11:58:29.257827 139938041223040 basic_session_run_hooks.py:606] Saving checkpoints for 0 into bert_output/model.ckpt.\n",
            "I0630 11:58:45.824826 139938041223040 basic_session_run_hooks.py:262] loss = 0.6543295, step = 0\n",
            "I0630 12:00:26.396340 139938041223040 basic_session_run_hooks.py:692] global_step/sec: 0.994311\n",
            "I0630 12:00:26.397970 139938041223040 basic_session_run_hooks.py:260] loss = 0.2980404, step = 100 (100.573 sec)\n",
            "I0630 12:01:56.334575 139938041223040 basic_session_run_hooks.py:692] global_step/sec: 1.11187\n",
            "I0630 12:01:56.336206 139938041223040 basic_session_run_hooks.py:260] loss = 0.3248061, step = 200 (89.938 sec)\n",
            "I0630 12:03:28.181520 139938041223040 basic_session_run_hooks.py:692] global_step/sec: 1.08877\n",
            "I0630 12:03:28.182774 139938041223040 basic_session_run_hooks.py:260] loss = 0.08040221, step = 300 (91.847 sec)\n",
            "I0630 12:05:01.545289 139938041223040 basic_session_run_hooks.py:692] global_step/sec: 1.07108\n",
            "I0630 12:05:01.546849 139938041223040 basic_session_run_hooks.py:260] loss = 0.009424648, step = 400 (93.364 sec)\n",
            "I0630 12:06:04.985057 139938041223040 basic_session_run_hooks.py:606] Saving checkpoints for 468 into bert_output/model.ckpt.\n",
            "I0630 12:06:14.414470 139938041223040 estimator.py:368] Loss for final step: 0.15657091.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training took time  0:08:13.297790\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmbLTVniARy3",
        "colab_type": "text"
      },
      "source": [
        "Now let's use our test data to see how well our model did:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JIhejfpyJ8Bx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_input_fn = run_classifier.input_fn_builder(\n",
        "    features=test_features,\n",
        "    seq_length=MAX_SEQ_LENGTH,\n",
        "    is_training=False,\n",
        "    drop_remainder=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PPVEXhNjYXC-",
        "colab_type": "code",
        "outputId": "6d7d7a57-f63e-4e13-e715-c13647a34e3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        }
      },
      "source": [
        "estimator.evaluate(input_fn=test_input_fn, steps=None)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I0630 12:11:34.147834 139938041223040 estimator.py:1145] Calling model_fn.\n",
            "I0630 12:11:37.231179 139938041223040 saver.py:1499] Saver not created because there are no variables in the graph to restore\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
            "I0630 12:11:45.852903 139938041223040 estimator.py:1147] Done calling model_fn.\n",
            "I0630 12:11:45.874767 139938041223040 evaluation.py:255] Starting evaluation at 2019-06-30T12:11:45Z\n",
            "I0630 12:11:47.747067 139938041223040 monitored_session.py:240] Graph was finalized.\n",
            "I0630 12:11:47.758321 139938041223040 saver.py:1280] Restoring parameters from bert_output/model.ckpt-468\n",
            "I0630 12:11:50.131825 139938041223040 session_manager.py:500] Running local_init_op.\n",
            "I0630 12:11:50.367362 139938041223040 session_manager.py:502] Done running local_init_op.\n",
            "I0630 12:12:39.106802 139938041223040 evaluation.py:275] Finished evaluation at 2019-06-30-12:12:39\n",
            "I0630 12:12:39.108069 139938041223040 estimator.py:2039] Saving dict for global step 468: auc = 0.8686192, eval_accuracy = 0.8686, f1_score = 0.8688884, false_negatives = 314.0, false_positives = 343.0, global_step = 468, loss = 0.4865039, precision = 0.86388886, recall = 0.8739462, true_negatives = 2166.0, true_positives = 2177.0\n",
            "I0630 12:12:41.357228 139938041223040 estimator.py:2099] Saving 'checkpoint_path' summary for global step 468: bert_output/model.ckpt-468\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'auc': 0.8686192,\n",
              " 'eval_accuracy': 0.8686,\n",
              " 'f1_score': 0.8688884,\n",
              " 'false_negatives': 314.0,\n",
              " 'false_positives': 343.0,\n",
              " 'global_step': 468,\n",
              " 'loss': 0.4865039,\n",
              " 'precision': 0.86388886,\n",
              " 'recall': 0.8739462,\n",
              " 'true_negatives': 2166.0,\n",
              " 'true_positives': 2177.0}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ueKsULteiz1B",
        "colab_type": "text"
      },
      "source": [
        "Now let's write code to make predictions on new sentences:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OsrbTD2EJTVl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getPrediction(in_sentences):\n",
        "  labels = [\"Negative\", \"Positive\"]\n",
        "  input_examples = [run_classifier.InputExample(guid=\"\", text_a = x, text_b = None, label = 0) for x in in_sentences] # here, \"\" is just a dummy label\n",
        "  input_features = run_classifier.convert_examples_to_features(input_examples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
        "  predict_input_fn = run_classifier.input_fn_builder(features=input_features, seq_length=MAX_SEQ_LENGTH, is_training=False, drop_remainder=False)\n",
        "  predictions = estimator.predict(predict_input_fn)\n",
        "  return [(sentence, prediction['probabilities'], labels[prediction['labels']]) for sentence, prediction in zip(in_sentences, predictions)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-thbodgih_VJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred_sentences = [\n",
        "  \"That movie was absolutely awful\",\n",
        "  \"The acting was a bit lacking\",\n",
        "  \"The film was creative and surprising\",\n",
        "  \"Absolutely fantastic!\"\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QrZmvZySKQTm",
        "colab_type": "code",
        "outputId": "3a96e2b3-593f-429c-af3a-5a7b86ff4d39",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 649
        }
      },
      "source": [
        "predictions = getPrediction(pred_sentences)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I0630 12:13:27.830938 139938041223040 run_classifier.py:774] Writing example 0 of 4\n",
            "I0630 12:13:27.835525 139938041223040 run_classifier.py:461] *** Example ***\n",
            "I0630 12:13:27.840805 139938041223040 run_classifier.py:462] guid: \n",
            "I0630 12:13:27.843917 139938041223040 run_classifier.py:464] tokens: [CLS] that movie was absolutely awful [SEP]\n",
            "I0630 12:13:27.844985 139938041223040 run_classifier.py:465] input_ids: 101 2008 3185 2001 7078 9643 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0630 12:13:27.848368 139938041223040 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0630 12:13:27.851624 139938041223040 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0630 12:13:27.853755 139938041223040 run_classifier.py:468] label: 0 (id = 0)\n",
            "I0630 12:13:27.855652 139938041223040 run_classifier.py:461] *** Example ***\n",
            "I0630 12:13:27.857133 139938041223040 run_classifier.py:462] guid: \n",
            "I0630 12:13:27.859825 139938041223040 run_classifier.py:464] tokens: [CLS] the acting was a bit lacking [SEP]\n",
            "I0630 12:13:27.861331 139938041223040 run_classifier.py:465] input_ids: 101 1996 3772 2001 1037 2978 11158 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0630 12:13:27.863517 139938041223040 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0630 12:13:27.865753 139938041223040 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0630 12:13:27.867665 139938041223040 run_classifier.py:468] label: 0 (id = 0)\n",
            "I0630 12:13:27.876975 139938041223040 run_classifier.py:461] *** Example ***\n",
            "I0630 12:13:27.877985 139938041223040 run_classifier.py:462] guid: \n",
            "I0630 12:13:27.880495 139938041223040 run_classifier.py:464] tokens: [CLS] the film was creative and surprising [SEP]\n",
            "I0630 12:13:27.881191 139938041223040 run_classifier.py:465] input_ids: 101 1996 2143 2001 5541 1998 11341 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0630 12:13:27.881845 139938041223040 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0630 12:13:27.882446 139938041223040 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0630 12:13:27.884419 139938041223040 run_classifier.py:468] label: 0 (id = 0)\n",
            "I0630 12:13:27.885781 139938041223040 run_classifier.py:461] *** Example ***\n",
            "I0630 12:13:27.886830 139938041223040 run_classifier.py:462] guid: \n",
            "I0630 12:13:27.887809 139938041223040 run_classifier.py:464] tokens: [CLS] absolutely fantastic ! [SEP]\n",
            "I0630 12:13:27.889536 139938041223040 run_classifier.py:465] input_ids: 101 7078 10392 999 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0630 12:13:27.890355 139938041223040 run_classifier.py:466] input_mask: 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0630 12:13:27.890981 139938041223040 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0630 12:13:27.891524 139938041223040 run_classifier.py:468] label: 0 (id = 0)\n",
            "I0630 12:13:27.914184 139938041223040 estimator.py:1145] Calling model_fn.\n",
            "I0630 12:13:30.297474 139938041223040 saver.py:1499] Saver not created because there are no variables in the graph to restore\n",
            "I0630 12:13:30.484184 139938041223040 estimator.py:1147] Done calling model_fn.\n",
            "I0630 12:13:30.949558 139938041223040 monitored_session.py:240] Graph was finalized.\n",
            "I0630 12:13:30.961242 139938041223040 saver.py:1280] Restoring parameters from bert_output/model.ckpt-468\n",
            "I0630 12:13:31.745724 139938041223040 session_manager.py:500] Running local_init_op.\n",
            "I0630 12:13:31.824527 139938041223040 session_manager.py:502] Done running local_init_op.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXkRiEBUqN3n",
        "colab_type": "text"
      },
      "source": [
        "Voila! We have a sentiment classifier!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERkTE8-7oQLZ",
        "colab_type": "code",
        "outputId": "9c815ab3-0332-4540-874e-4286fa98bb59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "predictions"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('That movie was absolutely awful',\n",
              "  array([-2.8898641e-03, -5.8480024e+00], dtype=float32),\n",
              "  'Negative'),\n",
              " ('The acting was a bit lacking',\n",
              "  array([-0.00880431, -4.7369137 ], dtype=float32),\n",
              "  'Negative'),\n",
              " ('The film was creative and surprising',\n",
              "  array([-5.261323e+00, -5.201972e-03], dtype=float32),\n",
              "  'Positive'),\n",
              " ('Absolutely fantastic!',\n",
              "  array([-4.703031  , -0.00910913], dtype=float32),\n",
              "  'Positive')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    }
  ]
}